<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="神经网络研究-Transformer架构, Amnematchinensis">
    <meta name="description" content="神经网络研究-Transformer架构，主要介绍Transformer架构的关键技术、架构、深度学习框架下的模型代码分析、大语言模型分类和参考文献等。">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>神经网络研究-Transformer架构 | Amnematchinensis</title>
    <link rel="icon" type="image/png" href="/favicon.png">
    


    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/css/matery.css">
<link rel="stylesheet" type="text/css" href="/css/my.css">
<link rel="stylesheet" type="text/css" href="/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/css/post.css">




    



    <script src="/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="Amnematchinensis" type="application/atom+xml">
<link rel="stylesheet" href="/css/prism.css" type="text/css"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Amnematchinensis</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/contact" class="waves-effect waves-light">
      
      <i class="fas fa-comments" style="zoom: 0.6;"></i>
      
      <span>留言板</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Amnematchinensis</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/contact" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-comments"></i>
			
			留言板
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			友情链接
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/VarAmnematchinensis/" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/VarAmnematchinensis/" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/8.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">神经网络研究-Transformer架构</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/AI/">
                                <span class="chip bg-color">AI</span>
                            </a>
                        
                            <a href="/tags/%E7%AE%97%E6%B3%95/">
                                <span class="chip bg-color">算法</span>
                            </a>
                        
                            <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">
                                <span class="chip bg-color">神经网络</span>
                            </a>
                        
                            <a href="/tags/Transformer/">
                                <span class="chip bg-color">Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/AI/" class="post-category">
                                AI
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-09-30
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-13
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    27 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <h2 id="神经网络研究-Transformer架构"><a href="#神经网络研究-Transformer架构" class="headerlink" title="神经网络研究-Transformer架构"></a>神经网络研究-Transformer架构</h2><p>介绍：是第一个完全基于自注意力机制的处理序列转换模型的神经网络架构，是大模型当前的主流架构。</p>
<h3 id="典型神经网络优缺点"><a href="#典型神经网络优缺点" class="headerlink" title="典型神经网络优缺点"></a>典型神经网络优缺点</h3><p>RNN的缺点：无法并行计算，长语句输入计算效率低；梯度消失&amp;爆炸&amp;链式信息损失，导致难以实现长期记忆<br>CNN的缺点：输入长度有限.优点是可并计算，计算效率高<br>Transformer相比RNN和CNN的特点：<br>和RNN相比，可并计算，计算效率高；有固有的全局视野，可捕获长距离依赖。本质是层内没有序列限制。<br>和CNN相比，感受长度不受限制，可灵活处理位置信息。本质是可以完整输入的感受野。</p>
<h3 id="关键技术"><a href="#关键技术" class="headerlink" title="关键技术"></a>关键技术</h3><ol>
<li><p>Self Attention(自注意力)：<br> 意义：核心组件。<br> 用途：通过计算每个词和其他词之间的相似度来建立他们之间的关系，并根据这些关系加权计算每个词的表示。优点是能捕获序列中任意两个位置之间的关系，在序列建模任务中表现良好。使用自注意力的模型可以捕获长距离的依赖关系，提高并行计算效率。<br> 过程：由输入表示、权重计算和加权求和三部分组成。输入表示是指输入序列中的每个词通过词嵌入(Embedding)转换为向量表示。每个词有三种向量组成(查询、键、值向量)，对于每个词，查询向量和键向量通过线性变换得到，然后通过相似度函数（点积或缩放点积）计算查询向量和键向量的相似度，将相似度归一化得到注意力权重，进而得到每个词和其他词的相关性权重。再使用这些权重对值向量加权求和，获得每个词的上下文表示。<br> 单头注意力机制计算公式如下：<br>$$<br>Attention(Q, K, V) &#x3D; softmax(\frac{QK^T}{\sqrt{d_k}})V<br>$$<br>$Q$是query(查询向量)，来自用户输入,计算公式$Q &#x3D; ZW_{Q}$,$Z$是词嵌入矩阵$E$和位置编码矩阵$P$的和，都是$T<em>d_{model}维度$。$K$是key(键向量)，来自词库,计算公式$Q &#x3D; ZW_{K}$。$V$是value(值向量),计算公式$Q &#x3D; ZW_{V}$, Attention score高，value占比越大。这其中$Q$、$K$、$V$的维度分别是$T</em>d_k$、$T<em>d_k$、$T</em>d_v$,$d_k$是键的维度，用于缩放点积，防止梯度消失，特别是维度比较大的时候。通常$d_k&#x3D;d_v&#x3D;d_{model&#x2F;h}$,其中$h$是head的个数。<br>多头注意力机制计算公式如下：<br>$$<br>MultiHead(Q, K, V) &#x3D; Concat(head_1,…, head_h)W^{O}<br>$$$$<br>where head_i &#x3D; Attention(QW^{Q}<em>{i}, KW^{K}</em>{i}, VW^{VkV}_{i})<br>$$</p>
</li>
<li><p>Multi-Head Attention(多头注意力层)：<br> 过程：将输入数据划分为多个head,每个head独立关注输入的不同表示子空间，每个子空间独立计算注意力，从而使用模型并行捕获输入数据中的不同特征和模式信息，综合各项信息来更全面的理解文本。具体来说，就是通过初始权重随机分布，通过梯度下降优化过程，去推动每个head去适应其权重来学习不同的特征，进而减少总体损失。<br> 优势：允许模型关注序列中不同部分的信息；多个注意力头可以并行计算。</p>
</li>
<li><p>Positional Encoding(位置编码)<br> 用途：为输入序列中的每个词添加位置信息，进而解决attention缺失词的位置信息的问题，进一步使用序列的顺序信息，在输入表示中添加位置信息编码来注入绝对或者相对位置编码，位置编码可以通过学习或者直接固定获得。<br> 典型的位置编码方法：基于正弦和余弦函数的固定位置编码<br>$$<br>PE_{(pos, 2i)} &#x3D; sin(pos&#x2F;10000^{2i&#x2F;d_{model}})<br>$$$$<br>PE_{(pos, 2i+1)} &#x3D; cos(pos&#x2F;10000^{2i&#x2F;d_{model}})<br>$$<br>其中，$pos$是词位置，$i$是维度索引。</p>
</li>
<li><p>Feed Forward Neural Network(前馈层)：通常由两个全连接层，中间使用Relu函数作为激活函数。<br>$$<br>FFN(x) &#x3D; max(0, xW_1 + b_1)W_2 + b_2<br>$$<br>Feed Forward Neural Network(前馈层)目的：对序列中所有位置的表示进行变换，使用同一个多层感知机(MLP)。对同一个layer的不同position，其所作的线性变换是一致的。增加网络结构的非线性，增加参数量，提升模型复杂度和深度，使模型可以处理更复杂的任务。</p>
</li>
<li><p>Residual Connectionz(残差连接)+Layer Normalization(归一层)：用于稳定训练过程。</p>
</li>
</ol>
<h3 id="Transformer架构"><a href="#Transformer架构" class="headerlink" title="Transformer架构"></a>Transformer架构</h3><h4 id="Inputs-输入-Output-输出"><a href="#Inputs-输入-Output-输出" class="headerlink" title="Inputs(输入)&#x2F;Output(输出)"></a>Inputs(输入)&#x2F;Output(输出)</h4><p>Inputs(输入)&#x2F;Output(输出，在训练过程中作为输入结果导入):在翻译第一个词时需要填充开始标签<code>&lt;BOS&gt;</code></p>
<h4 id="Tokenize-分词"><a href="#Tokenize-分词" class="headerlink" title="Tokenize(分词)"></a>Tokenize(分词)</h4><p>Tokenize(分词):是Transformer的第一关，是作为模型的输入。<br><strong>含义</strong>：Token可理解为词元，英文token可视为单词的“拼图”，一个token大约对应于0.75个单词或者3-4个字母，或者对应1-1.8个汉字。Tokenize的过程就是根据词汇表，将文本映射到词ID序列，作为词嵌入的输入。Tokenizer是只分割词元的算法方法。Tokenization是指分割后的单词。</p>
<p><strong>Tokenize的处理流程</strong>：Normalization(文本清洗、数字处理)-&gt;Pre-Tokenization(基于规则初步分割)-&gt;Model(使用分词算法做字词拆分)-&gt;Post-Tokenization(特殊处理和标记)</p>
<ol>
<li>Normalization(文本清洗、数字处理):包括文本清洗、标准化写法、安全和规范化操作。其中文本清洗一般是指去除无用字符、额外空白等，只保留对分词和模型训练有意义的内容。标准化写法是指统一大小写设置，将数字统一格式，确保文本采用统一的字符编码等。安全和规范化操作一般时过滤掉危险内容。</li>
<li>Pre-Tokenization(基于规则初步分割)：先基于简单规则对文本做初步分割，将文本初步拆分为更小的单元，如句子或者词语等。</li>
<li>Model(使用分词算法做字词拆分)：利用分词模型算法对文本做处理，生成词汇表(Vocabulary),利用词汇表(Vocabulary),将文本拆分为Token.例如训练好的大语言模型文件中的tokenizer.json&#x2F;tokenizer_config.json&#x2F;vocab.json文件。</li>
<li>Post-Tokenization(特殊处理和标记):包括序列填充和截断、特殊token信息添加、构建注意力掩码等。保证输入序列长度一致，并在序列的适当位置添加特殊token(例如<CLS>&#x2F;<SEP>),区分实际token和填充token.</li>
</ol>
<p><strong>核心参数</strong>：词汇表(Vocabulary).词汇表(Vocabulary)大小影响着模型的泛化能力和计算效率。大词汇表可以提高模型覆盖不同词汇和表达的能力，但也会影响模型处理的速度。</p>
<p><strong>分词算法</strong>：Work-based(基于单词)、Character-based(基于字符)、Subword-based(子词分词)等</p>
<ol>
<li>Work-based(基于单词)：按照单词进行分词，利用空格和标点符号做分割。好处就是简单直观，缺点就是词汇表爆炸、Out-ot-Vocabulary问题严重，同时不能学习到词缀之间的关系。</li>
<li>Character-based(基于字符)：以char为最小粒度，按照单字符做分词。好处是词汇表比较小(26个英文字符+标点符号)，缺点就是丢失语义信息，每个token的信息密度过低，导致序列过长，解码效率低。</li>
<li>Subword-based(子词分词)：按照词的subword进行分词。对低频词保留完整词或者拆分为字符，对高频词拆分为更细粒度的子词。好处就是能平衡词汇表的大小和语义表达能力，词表大小适中，解码效率高，也能学习到词缀之间的关系。典型算法：BPE(使用最广泛的分词算法)、WordPrice、Unigram<br> <strong>Byte Pair Encoding(BPE)</strong>：来自论文<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1508.07909">Neural Machine Translation of Rare Words with Subword Units</a>，当前LLM的主流方法之一。</li>
</ol>
<p>过程如下：构建初始词表-&gt;识别最频繁出现的标记对-&gt;合并最频繁出现的token对-&gt;重复步骤2和3</p>
<ol>
<li>构建初始词表：先按照字符粒度先分词，将文本中的每个字符先视为一个token,形成初始词表；</li>
<li>识别最频繁出现的标记对：扫描语料库，找到出现频率最高的token对,也就是字符或子词；</li>
<li>合并最频繁出现的token对：将2中频率最高的token对合并为一个新的token后，将新token添加到词表中，并删除被token完成覆盖的token,做词汇表的更新，并将token对的合并加入到合并规则列表中。</li>
<li>重复步骤2和3：迭代执行2和3步骤，直到达到指定词汇量或者token对不住频繁出现为止。</li>
</ol>
<p><strong>评估标准</strong>：词汇表覆盖率、token数量、OOV率.<br>词汇表覆盖率：用于检验时候可以处理多种语言和专业术语。<br>token数量：token过长会导致计算浪费，过短导致信息丢失。<br>OOV率：即Over of Vocabulary Rate.训练中未出现而测试时出现的单词概率。</p>
<h4 id="Embedding-词嵌入"><a href="#Embedding-词嵌入" class="headerlink" title="Embedding(词嵌入)"></a>Embedding(词嵌入)</h4><p>含义：将离散的输入ID序列中的每个词转换为向量表示(一般为列向量)，通过查表为每个字符赋予语义信息。<br>词嵌入维度：需要嵌入的一句句子中词汇的总数（sequence）* 嵌入向量的维度(d_model)<br>Embedding矩阵参数确定：先初始化，后训练迭代</p>
<h4 id="Positional-Encoding-位置编码"><a href="#Positional-Encoding-位置编码" class="headerlink" title="Positional Encoding(位置编码)"></a>Positional Encoding(位置编码)</h4><p>Positional Encoding(位置编码)：和输入的词向量维度相同<br><strong>位置编码设计</strong>：唯一性：每个位置的编码是唯一的，这确保模型能够区分序列中的不同位置。周期性：能够根据相位捕获位置关系.正交性：偶数位置和奇数位置的编码是正交，增加编码区分度和信息丰富度。<br><strong>典型方法</strong>：Absolute Positional Encoding(绝对位置编码, APE)、Relative Positional Encoding(相对位置编码,RPE)、Rotary Positional Embedding(旋转位置编码, RoPE)</p>
<p><strong>Absolute Positional Encoding(绝对位置编码, APE)</strong>:为序列中每个位置分配唯一的固定或可学习的向量，直接表征决定位置索引。分为Sinusoidal编码和Learnable编码。适用于短序列、对位置敏感的任务。缺点是难以反应序列字符之间的相对位置关系，表示不了比预训练文本长度更长的位置向量。</p>
<ol>
<li>Sinusoidal编码：Sinusoidal三角式绝对位置编码，也是Transformer论文中使用的方法，利用不同频率的正弦、余弦函数生成位置编码，偶数维度用正弦，奇数维度用余弦。可无需训练，就具有周期性外推能力，但是无法直接表达相对的位置关系，适用于短文本翻译，需要固定位置感知的序列任务，在处理长序列性能时会出现下降，需要进一步微调和扩展。</li>
<li>Learnable编码：Learnable可学习绝对位置编码，随机初始化位置编码矩阵，作为可训练参数，模型只能感知每个词向量所处的绝对位置，无法感知词向量之间的相对位置。适用早期如BERT的预训练模型，不具备长度外推性。</li>
</ol>
<p><strong>Relative Positional Encoding(相对位置编码,RPE)</strong>：建模序列中任意两个位置之间的相对距离而非绝对索引，增强模型对局部结构的感知能力。论文：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.02155">Self-Attention with Relative Position Representations</a>.</p>
<p><strong>Rotary Positional Embedding(旋转位置编码, RoPE)</strong>:将位置信息编码为旋转矩阵，用于查询向量和键向量的注意力计算中，隐式融合绝对位置和相对位置的双重特性，通过旋转位置编码将一个向量旋转到某一个角度，为其赋予位置信息。好处就是可支持超长序列的推理，可通过波长设计自动衰减远距离依赖，借助旋转操作实现相对位置偏移的数学等价。适用于需要处理超长上下文的长文本生成，也可审批不同模态的序列长度下的多模态任务。出自论文：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2104.09864">Roformer: Enhanced Transformer With Rotray Position Embedding</a></p>
<h4 id="Encoder-Block-编码器模块"><a href="#Encoder-Block-编码器模块" class="headerlink" title="Encoder Block(编码器模块)"></a>Encoder Block(编码器模块)</h4><p>用途：将源语言句子编码为一系列向量，理解和提取输入文本中的相关信息，捕获序列中各个元素的上下文信息，其输出的是输入文本的连续表示，通常称为嵌入(Embedding)。可以视为一个阅读者。<br>过程：输入带位置信息的Embedding矩阵，内部循环N次，输出key、value传递给Decoder Block.<br>结构：主要由两个子层组成。<br>Multi-Head Self-Attention(多头自注意力层):计算输入序列中每个词和其他词的相关性。<br>Feed Forward Neural Network(前馈层)：对每个词都独立非线性变换。<br>组成：</p>
<ol>
<li>Multi-Head Self-Attention(多头自注意力层)+Residual Connectionz(残差连接)+Layer Normalization(归一层)。</li>
<li>Feed Forward Neural Network(前馈层)+Residual Connectionz(残差连接)+Layer Normalization(归一层)。</li>
<li>Residual Connectionz(残差连接)&amp;Layer Normalization(相加+归一层)。</li>
</ol>
<p>Residual Connectionz(残差连接)的目的：每次循环能充分挖掘特征，用于将以前循环挖掘结果一并处理。<br>Layer Normalization的目的:按照样本跨所有特征进行标准化，即对hidden的维度去做归一化，针对单样本的不同特征做操作，对特定层的每个输入，将消除对批数据的依赖。在归一化层中，特定层中的所有神经元在给定输入的所有特征中有效地具有相同的分布。稳定神经网络学习过程，减少训练时间，提高模型最终性能。</p>
<h4 id="Decoder-Block-解码器模块"><a href="#Decoder-Block-解码器模块" class="headerlink" title="Decoder Block(解码器模块)"></a>Decoder Block(解码器模块)</h4><p>用途：使用向量生成目标语言的翻译，根据从编码器接收到的上下文信息，使用自注意力机制，自回归生生成与上下文相关的连贯输出序列，保证输出时序性。可类比一个口述者。<br>过程：输入带位置信息的Embedding矩阵，内部循环N次，输出信息传递给Linear(线性层).<br>结构：主要由三个子层组成。<br>Masked Multi-Head Self-Attention(遮蔽多头自注意力层)：计算输出序列中每个词和前面词的相关性（使用掩码防止未来信息泄露）<br>Cross Attention(交叉注意力)：或者也称为Encoder-Deconder Attention(编码器-解码器注意力机制)：计算输入序列和输出序列的相关性。<br>Feed Forward Neural Network(前馈层)：对每个词都独立非线性变换。</p>
<ol>
<li>Residual Connectionz(残差连接)&amp;Layer Normalization(相加+归一层)+ Masked Multi-Head Self-Attention(遮蔽多头自注意力层):<br> Masked Multi-Head Self-Attention(遮蔽多头自注意力层):确保解码器生成当前输出时，不会受到未来输出的影响。<br> 原理：对注意力分数矩阵应用一个遮蔽(一个上三角矩阵，其中未来位置的元素设置为负无穷)，在计算softmax前有效将这些位置注意力分数降到零附近，从而实现在生成序列的每一步，模型只能注意当前位置之前的词和标记。</li>
<li>Residual Connectionz(残差连接)&amp;Layer Normalization(相加+归一层)+Multi-Head Attention(多头注意力层):<br> Cross Attention(交叉注意力)：输入：编码器Encoder的编码信息矩阵计算得出的key、value矩阵和上一个解码器Decoder block的输出计算得到的query<br> 目的：用于融合两个不同序列或信息源的特征<br> 原理：解码器使用当前状态作为查询，与编码器的输出（key和value）进行交互，通过注意力机制确定编码器输出中的哪些部分是重要的，并据此生成下一个输出元素。</li>
</ol>
<h4 id="Feed-Forward-前馈层"><a href="#Feed-Forward-前馈层" class="headerlink" title="Feed Forward(前馈层)"></a>Feed Forward(前馈层)</h4><p>Residual Connectionz(残差连接)&amp;Layer Normalization(相加+归一层)+Feed Forward(前馈层)：</p>
<h4 id="Linear-线性层-Softmax-Softmax层"><a href="#Linear-线性层-Softmax-Softmax层" class="headerlink" title="Linear(线性层)&amp;Softmax(Softmax层)"></a>Linear(线性层)&amp;Softmax(Softmax层)</h4><p>Linear(线性层)&amp;Softmax(Softmax层):<br>目的：解码器输出转换为能和词汇表对应的更高维空间。<br>原理：将解密器输出矩阵映射到[100,10000]中，每个维度值代表相应单词作为序列下一个单词的未归一化分数，再利用Softmax做归一，从而得到10000个词的概率。概率越高，表示该单词成为下一个输出单词的可能性越大。获得概率词表。</p>
<h3 id="基于深度学习框架的Transformer模型"><a href="#基于深度学习框架的Transformer模型" class="headerlink" title="基于深度学习框架的Transformer模型"></a>基于深度学习框架的Transformer模型</h3><h4 id="基于PyTorch的Transformer模型"><a href="#基于PyTorch的Transformer模型" class="headerlink" title="基于PyTorch的Transformer模型"></a>基于PyTorch的Transformer模型</h4><h5 id="必要库和模块"><a href="#必要库和模块" class="headerlink" title="必要库和模块"></a>必要库和模块</h5><p>库和模块名：torch&#x2F;torch.nn&#x2F;torch.optim&#x2F;math&#x2F;copy<br>用途：构建Transformer模型架构、管理数据和训练过程。</p>
<h5 id="模型基本模块"><a href="#模型基本模块" class="headerlink" title="模型基本模块"></a>模型基本模块</h5><p>组成：位置编码、多头注意力、前馈神经网络</p>
<h6 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h6><p>用途：注入输入序列中词嵌入矩阵的位置信息。<br>构建方法：正弦和余弦函数，代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line">import torch as torch</span><br><span class="line">import math as math</span><br><span class="line">class PositionalEncoding(nn.Module):</span><br><span class="line">    def __init__(self, d_model, max_seq_length):</span><br><span class="line">        super(PositionalEncoding, self).__init__()</span><br><span class="line">        # 用零矩阵初始化位置矩阵</span><br><span class="line">        pe = torch.zeros(max_seq_length, d_model)</span><br><span class="line">        # 生成从0到max_seq_length-1的浮点数序列，并转换为二维张量</span><br><span class="line">        position = torch.arange(0, max_seq_length, dtype = torch.float).unsqueeze(1)</span><br><span class="line">        # 生成频率项，math.log(10000.0) / d_model表示固定常数，用于控制频率项衰减速度</span><br><span class="line">        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))</span><br><span class="line">        # 按照正弦公式，在偶数位置使用正弦函数</span><br><span class="line">        pe[:, 0::2] = torch.sin(position * div_term)</span><br><span class="line">        # 按照余弦公式，在奇数位置使用余弦函数</span><br><span class="line">        pe[:, 1::2] = torch.cos(position * div_term)</span><br><span class="line">        # 讲位置编码pe注册为模型缓存区，随模型保存和加载，不参与梯度更新，位置编码一般在初始化时计算好并在训练中保持不变</span><br><span class="line">        self.register_buffer(&#x27;pe&#x27;, pe.unsqueeze(0))</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        # 用于将位置编码信息添加到词嵌入矩阵中</span><br><span class="line">        return x + self.pe[:, :x.size(1)]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h6 id="多头注意力"><a href="#多头注意力" class="headerlink" title="多头注意力"></a>多头注意力</h6><p>用途：计算序列中每个位置之间的关系，并捕获输入序列不同的特征和模式。<br>构建方法：$Q$是query(查询向量)，$K$是key(键向量)，$V$是value(值向量)，将他们拆分为多个注意力head, 对每个head处理注意力，并在最后将结果concat,再linear处理。<br>代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line">import torch as torch</span><br><span class="line">import math as math</span><br><span class="line"></span><br><span class="line">class MultiHeadAttention(nn.Module):</span><br><span class="line">    def __init__(self, d_model, num_heads):</span><br><span class="line">        super(MultiHeadAttention, self).__init__()</span><br><span class="line">        # d_model是词嵌入向量的维度，必须确保d_model能被num_heads整除</span><br><span class="line">        assert d_model % num_heads == 0</span><br><span class="line"></span><br><span class="line">        # 设置基本参数 d_model为模型维度，num_heads为注意力head数， d_k为每个head的维度</span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        self.d_k = d_model // num_heads</span><br><span class="line"></span><br><span class="line">        # 定义线性变换层，设置初始化的查询q/键k/值v和输出o的线性变换</span><br><span class="line">        self.W_q = nn.Linear(d_model, d_model)</span><br><span class="line">        self.W_k = nn.Linear(d_model, d_model)</span><br><span class="line">        self.W_v = nn.Linear(d_model, d_model)</span><br><span class="line">        self.W_o = nn.Linear(d_model, d_model)</span><br><span class="line"></span><br><span class="line">        # 按照注意力公式，计算缩放后的点积注意力</span><br><span class="line">    def scaled_dot_product_attention(self, Q, K, V, mask = None):</span><br><span class="line">        # 计算注意力分数, math.sqrt(self.d_k)用于缩放</span><br><span class="line">        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)</span><br><span class="line">        # 应用掩码，这是用于解码器中的掩码多头注意力机制中</span><br><span class="line">        if mask is not None:</span><br><span class="line">            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)</span><br><span class="line">        # 按照注意力公式，使用softmax做归一化处理，计算注意力权重</span><br><span class="line">        attn_probs = torch.softmax(attn_scores, dim=-1)</span><br><span class="line">        # 按照注意力公式，将注意力权重和值向量做加权求和</span><br><span class="line">        output = torch.matmul(attn_probs, V)</span><br><span class="line"></span><br><span class="line">        # 将输入的词嵌入矩阵分割为多head</span><br><span class="line">    def split_heads(self, x):</span><br><span class="line">        batch_size, seq_lengh, d_model = x.size()</span><br><span class="line">        return x.view(batch_size, seq_lengh, self.num_heads, self.d_k).transpose(1, 2)</span><br><span class="line"></span><br><span class="line">        # 将输出多head合并为原始形状</span><br><span class="line">    def combine_heads(self, x):</span><br><span class="line">        batch_size, _, seq_legth, d_k = x.size()</span><br><span class="line">        # contiguous确保张量在内存中连续，view重塑张量外形</span><br><span class="line">        return x.transpose(1, 2).contiguous().view(batch_size, seq_legth, self.d_model)</span><br><span class="line"></span><br><span class="line">        # 前向传播</span><br><span class="line">    def forward(self, Q, K, V, mask = None):</span><br><span class="line">        # 对Q/K/V做线性变换后，再按照多head注意力机制分割为多head</span><br><span class="line">        Q = self.split_heads(self.W_q(Q))</span><br><span class="line">        K = self.split_heads(self.W_k(K))</span><br><span class="line">        V = self.split_heads(self.W_v(V))</span><br><span class="line"></span><br><span class="line">        # 编码器入口，对Q/K/V计算多头注意力，根据mask情况，计算掩码后的多头注意力机制</span><br><span class="line">        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)</span><br><span class="line"></span><br><span class="line">        # 更加公式concat合并多头并线性输出</span><br><span class="line">        output = self.W_o(self.combine_heads(attn_output))</span><br><span class="line">        return output</span><br></pre></td></tr></table></figure>

<h6 id="前馈神经网络-Feed-Forward"><a href="#前馈神经网络-Feed-Forward" class="headerlink" title="前馈神经网络(Feed Forward)"></a>前馈神经网络(Feed Forward)</h6><p>用途：对每个位置的特征做非线性变换。<br>构建方法：共三层，第一层为线性变换，用于将输入特征升维，第二层为非线性激活函数，用于捕获更复杂的特征关系，常用ReLU函数，第三层为线性变换，用于降维到原始维度。<br>代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line"></span><br><span class="line">class PositionwiseFeedForward(nn.Module):</span><br><span class="line">    def __init__(self, d_model, d_ff):</span><br><span class="line">        super(PositionwiseFeedForward, self).__init__()</span><br><span class="line">        # 设置第一层全连接层</span><br><span class="line">        self.fc1 = nn.Linear(d_model, d_ff)</span><br><span class="line">        # 设置第二层非线性激活函数层</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">        # 设置第三层层全连接层</span><br><span class="line">        self.fc2 = nn.Linear(d_ff, d_model)</span><br><span class="line">        </span><br><span class="line">    def forward(self, x):</span><br><span class="line">        # FNN网络计算</span><br><span class="line">        return self.fc2(self.relu(self.fc1(x)))</span><br></pre></td></tr></table></figure>






<h5 id="模型构成模块"><a href="#模型构成模块" class="headerlink" title="模型构成模块"></a>模型构成模块</h5><p>组成：编码器(Encoder)和解码器(Decoder)</p>
<h6 id="编码器-Encoder"><a href="#编码器-Encoder" class="headerlink" title="编码器(Encoder)"></a>编码器(Encoder)</h6><p>构成：多头注意力机制、前馈神经网络、残差连接和归一化<br>代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">class Encoderlayer(nn.Module):</span><br><span class="line">    def __init__(self, d_model, d_ff, num_heads, d_ff, dropout):</span><br><span class="line">        super(Encoderlayer, self).__init__()</span><br><span class="line">        # 设置多头注意力机制</span><br><span class="line">        self.self_attn = MultiHeadAttention(d_model, num_heads)</span><br><span class="line">        # 设置前馈神经网络</span><br><span class="line">        self.feed_forward = PositionwiseFeedForward(d_model, d_ff)</span><br><span class="line">        # 设置归一化层</span><br><span class="line">        self.norm1 = nn.LayerNorm(d_model)</span><br><span class="line">        self.norm2 = nn.LayerNorm(d_model)</span><br><span class="line">        # 设置Dropout正则化，防止过拟合</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    def forward(self, x, mask = None):</span><br><span class="line">        # 根据输入序列，运行多头注意力机制</span><br><span class="line">        attn_output = self.self_attn(x, x, x, mask)</span><br><span class="line">        # 先做利用dropout做残差连接，再做归一化。</span><br><span class="line">        x = self.norm1(x + self.dropout(attn_output))</span><br><span class="line">        # 运行前馈神经网络</span><br><span class="line">        ff_output = self.feed_forward(x)</span><br><span class="line">        # 先做利用dropout做残差连接，再做归一化。</span><br><span class="line">        x = self.norm2(x + self.dropout(ff_output))</span><br><span class="line">        return</span><br></pre></td></tr></table></figure>
<h6 id="解码器-Decoder"><a href="#解码器-Decoder" class="headerlink" title="解码器(Decoder)"></a>解码器(Decoder)</h6><p>构成：掩码多头注意力机制、交叉注意力机制、前馈神经网络、残差连接和归一化<br>代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">class Decoderlayer(nn.Module):</span><br><span class="line">    def __init__(self, d_model, num_heads, d_ff, dropout):</span><br><span class="line">        super(Decoderlayer, self).__init__()</span><br><span class="line">        # 设置多头注意力机制</span><br><span class="line">        self.self_attn = MultiHeadAttention(d_model, num_heads)</span><br><span class="line">        # 设置交叉注意力机制</span><br><span class="line">        self.cross_attn = MultiHeadAttention(d_model, num_heads)</span><br><span class="line">        # 设置前馈神经网络</span><br><span class="line">        self.feed_forward = PositionwiseFeedForward(d_model, d_ff)</span><br><span class="line">        # 设置归一化层</span><br><span class="line">        self.norm1 = nn.LayerNorm(d_model)</span><br><span class="line">        self.norm2 = nn.LayerNorm(d_model)</span><br><span class="line">        self.norm3 = nn.LayerNorm(d_model)</span><br><span class="line">        # 设置Dropout正则化，防止过拟合</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    def forward(self, x, enc_output, src_mask, tgt_mask):</span><br><span class="line">        # 根据输入序列，运行掩码多头注意力机制</span><br><span class="line">        attn_output = self.self_attn(x, x, x, tgt_mask)</span><br><span class="line">        # 先做利用dropout做残差连接，再做归一化。</span><br><span class="line">        x = self.norm1(x + self.dropout(attn_output))</span><br><span class="line">        # 获取编码器的输出K/V，和解码器的q，计算交叉注意力机制</span><br><span class="line">        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)</span><br><span class="line">        # 先做利用dropout做残差连接，再做归一化。</span><br><span class="line">        x = self.norm2(x + self.dropout(attn_output))</span><br><span class="line">        # 运行前馈神经网络</span><br><span class="line">        ff_output = self.feed_forward(x)</span><br><span class="line">        # 先做利用dropout做残差连接，再做归一化。</span><br><span class="line">        x = self.norm3(x + self.dropout(ff_output))</span><br><span class="line">        return x</span><br></pre></td></tr></table></figure>

<h5 id="完整Transformer模型代码如下："><a href="#完整Transformer模型代码如下：" class="headerlink" title="完整Transformer模型代码如下："></a>完整Transformer模型代码如下：</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">其中模型初始化参数含义如下：</span><br><span class="line">src_vocab_size,  # 源语言词汇表大小（如英文单词数）</span><br><span class="line">tgt_vocab_size,  # 目标语言词汇表大小（如中文单词数）</span><br><span class="line">d_model = 512,  # 模型维度（每个词向量的长度）</span><br><span class="line">num_heads = 8,  # 多头注意力的头数</span><br><span class="line">num_layers = 6,  # 编码器/解码器的堆叠层数</span><br><span class="line">d_ff = 2048,  # 前馈网络隐藏层维度</span><br><span class="line">max_seq_length = 100,  # 最大序列长度（用于位置编码）</span><br><span class="line">dropout = 0.1  # Dropout概率</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">class Transformer(nn.Module):</span><br><span class="line">    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):</span><br><span class="line">        super(Transformer, self).__init__()</span><br><span class="line">        # 将来源语句词汇映射到d_model维空间的嵌入层，构成来源语句序列编码器词嵌入矩阵</span><br><span class="line">        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)</span><br><span class="line">        # 将目标语句词汇映射到d_model维空间的嵌入层，构成目标语句序列解码器词嵌入矩阵</span><br><span class="line">        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)</span><br><span class="line">        # 设置初始化位置编码矩阵</span><br><span class="line">        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)</span><br><span class="line"></span><br><span class="line">        # 根据num_layers编码器/解码器的堆叠层数，每层作为编码器/解码器对象，构建编码器/解码器列表</span><br><span class="line">        self.encoder_layers = nn.ModuleList(</span><br><span class="line">            [EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])</span><br><span class="line">        self.decoder_layers = nn.ModuleList(</span><br><span class="line">            [DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])</span><br><span class="line"></span><br><span class="line">        # 初始化最后的全连接层，将解码器的输出转换为目标词汇表大小的维度</span><br><span class="line">        self.fc = nn.Linear(d_model, tgt_vocab_size)</span><br><span class="line">        # Dropout</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    # 生成掩码函数，用于屏蔽无效位置和未来信息</span><br><span class="line">    def generate_mask(self, src, tgt):</span><br><span class="line">        # 源掩码：屏蔽填充符（假设填充符索引为0）</span><br><span class="line">        # 形状：(batch_size, 1, 1, seq_length)</span><br><span class="line">        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)</span><br><span class="line"></span><br><span class="line">        # 目标掩码：屏蔽填充符和未来信息</span><br><span class="line">        # 形状：(batch_size, 1, seq_length, 1)</span><br><span class="line">        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)</span><br><span class="line">        seq_length = tgt.size(1)</span><br><span class="line">        # 生成上三角矩阵掩码，防止解码时看到未来信息</span><br><span class="line">        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()</span><br><span class="line">        # 合并填充掩码和未来信息掩码</span><br><span class="line">        tgt_mask = tgt_mask &amp; nopeak_mask</span><br><span class="line">        return src_mask, tgt_mask</span><br><span class="line"></span><br><span class="line">        # 主要执行逻辑，前向传播，依次通过编码、解码器，最后通过全连接层输出</span><br><span class="line">    def forward(self, src, tgt):</span><br><span class="line">        # 生成源掩码和目标掩码</span><br><span class="line">        src_mask, tgt_mask = self.generate_mask(src, tgt)</span><br><span class="line"></span><br><span class="line">        # 编码器部分，先做词嵌入、再做位置编码，最后通过enc_layer做编码层处理</span><br><span class="line">        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))</span><br><span class="line">        enc_output = src_embedded</span><br><span class="line">        for enc_layer in self.encoder_layers:</span><br><span class="line">            enc_output = enc_layer(enc_output, src_mask)</span><br><span class="line"></span><br><span class="line">        # 解码器部分，先做词嵌入、再做位置编码，最后通过dec_layer做解码层处理</span><br><span class="line">        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))</span><br><span class="line">        dec_output = tgt_embedded</span><br><span class="line">        for dec_layer in self.decoder_layers:</span><br><span class="line">            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)</span><br><span class="line"></span><br><span class="line">        # 全连接层将解码器的输出转换为目标词汇表大小的维度，得到最终的输出</span><br><span class="line">        output = self.fc(dec_output)</span><br><span class="line">        return output</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h6 id="不同深度学习框架下的Transformer模块"><a href="#不同深度学习框架下的Transformer模块" class="headerlink" title="不同深度学习框架下的Transformer模块"></a>不同深度学习框架下的Transformer模块</h6><table>
<thead>
<tr>
<th>深度学习框架</th>
<th>函数</th>
<th>支持平台</th>
</tr>
</thead>
<tbody><tr>
<td>Pytorch</td>
<td><a target="_blank" rel="noopener" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Transformer.html">torch.nn.Transformer()</a></td>
<td>CPU&#x2F;GPU&#x2F;TPU&#x2F;Ascend&#x2F;AppleMetal</td>
</tr>
<tr>
<td>MindSpore</td>
<td><a target="_blank" rel="noopener" href="https://www.mindspore.cn/docs/zh-CN/r2.7.0/api_python/nn/mindspore.nn.Transformer.html">mindspore.nn.Transformer()</a></td>
<td>GPU&#x2F;CPU&#x2F;Ascend</td>
</tr>
<tr>
<td>Paddle</td>
<td><a target="_blank" rel="noopener" href="https://www.paddlepaddle.org.cn/documentation/api/paddle/nn/Transformer_cn.html#paddle.nn.Transformer">paddle.nn.Transformer()</a></td>
<td>CPU&#x2F;GPU&#x2F;A加速芯片(华为&#x2F;海光&#x2F;昆仑芯&#x2F;寒武纪等)</td>
</tr>
</tbody></table>
<p>可以进一步研究不同的深度学习框架对Transformer架构的实现方法之间的差别，和同一个深度学习框架在不同硬件平台上的实现差别和工作效率。</p>
<h3 id="大语言模型分类-Transformer变体"><a href="#大语言模型分类-Transformer变体" class="headerlink" title="大语言模型分类(Transformer变体)"></a>大语言模型分类(Transformer变体)</h3><p>影响模型参数量的影响因素：Transformer层数、隐层维度、词向量维度、词典大小、输入token数<br>按照编码器和解密器分类：</p>
<p><strong>只有编码器(encoder-only)</strong>：自编码模型(Auto-encoder model),即先通过某种方式破坏句子，希望模型将被破坏的部分还原。需要从输入序列中提取有意义的上下文信息。<br>BERT(Bidirectional Encoder Representation from Transformers)：训练时先基于大量未标注的语料库进行自监督学习（预训练），然后基于标注良好的数据集进行微调，其核心在自监督训练策略。包括Mask LM(掩码语言模型，Mask Language Model,随机将输入序列中的一些词汇遮挡或者随机替换为其他词，让模型预测被遮挡的词)和NSP(随机从语料库中抽取两个句子判断是否连续)。是双向模型，训练时需要利用上下文信息，各位复杂。BERT是自编码模型，需要经过预训练和微调两个阶段学习文本表示，只能完成特定的NLP任务。</p>
<p><strong>只有解码器(decoder-only)</strong>：自回归模型(Auto-regressive model),给出上文，预测下文。允许编码器聚焦于输入序列的相关部分，同时生成输出序列的每个词。<br>GPT(Generative Pretained Transformer):也使用Pre-training + Fine-tuning模式.是单向模型，不需要利用上下文信息，只能利用上文。GPT是基于自回归的模型，能够进行文本生成，可以利用Prompt应用到NLP任务中。</p>
<p><strong>编码解码器混合(encoder decoder hybrid)</strong>：理解输入的内容NLU,又能处理并生成内容NLG，特别是处理输入和输出序列之间存在复杂映射关系的任务，以及捕捉两个序列中元素之间关系至关重要的任务。<br>T5(Tranfer Text-to-Text Transformer):所有NLP任务都可以转化为Text-to-Text任务。</p>
<h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><p>[1] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">Attention is All You Need</a><br>[2] <a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1pu411o7BE/?spm_id_from=333.1391.0.0&vd_source=0a909d15c459f7c0fd724c236e540d79">Transformer论文逐段精读【论文精读】</a><br>[3] <a target="_blank" rel="noopener" href="https://www.runoob.com/pytorch/Transformer-model.html">菜鸟教程</a><br>[4]<a target="_blank" rel="noopener" href="https://github.com/tensorflow/tensor2tensor">Tensor2Tensor</a></p>

                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">青山生柳</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://varamnematchinensis.github.io/2025/09/30/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%A0%94%E7%A9%B6-Transformer%E6%9E%B6%E6%9E%84/">https://varamnematchinensis.github.io/2025/09/30/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%A0%94%E7%A9%B6-Transformer%E6%9E%B6%E6%9E%84/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/about" target="_blank">青山生柳</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/AI/">
                                    <span class="chip bg-color">AI</span>
                                </a>
                            
                                <a href="/tags/%E7%AE%97%E6%B3%95/">
                                    <span class="chip bg-color">算法</span>
                                </a>
                            
                                <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">
                                    <span class="chip bg-color">神经网络</span>
                                </a>
                            
                                <a href="/tags/Transformer/">
                                    <span class="chip bg-color">Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    
	
	
		<div class="card" data-aos="fade-up">
    <div id="utteranc-container" class="card-content">
		<script src="https://utteranc.es/client.js"
				repo="VarAmnematchinensis/commit-utterance"
				issue-term="pathname"
				theme="github-light"
				crossorigin="anonymous"
				async>
		</script>
    </div>
</div>
	

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/2025/10/01/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83(Tuning)%E5%AE%9E%E8%B7%B5/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/15.jpg" class="responsive-img" alt="大模型微调(Tuning)实践">
                        
                        <span class="card-title">大模型微调(Tuning)实践</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            大模型微调(Tuning)实践，主要介绍大模型微调(Tuning)、微调分类、当前主要的参数高效微调方法(LoRA等)、微调数据准备、大模型能力评估指标、训练开发流程和微调开发流程和参考文献等。
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-01
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/AI/" class="post-category">
                                    AI
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/AI/">
                        <span class="chip bg-color">AI</span>
                    </a>
                    
                    <a href="/tags/%E7%AE%97%E6%B3%95/">
                        <span class="chip bg-color">算法</span>
                    </a>
                    
                    <a href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83/">
                        <span class="chip bg-color">大模型微调</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2025/09/18/AI%E5%BA%94%E7%94%A8%E5%AE%9E%E8%B7%B5-%E6%99%BA%E8%83%BDBI(%E6%99%BA%E8%83%BD%E9%97%AE%E6%95%B0%E4%B8%8E%E6%99%BA%E8%83%BD%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90)/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/1.jpg" class="responsive-img" alt="AI应用实践-智能BI(智能问数与智能数据分析)">
                        
                        <span class="card-title">AI应用实践-智能BI(智能问数与智能数据分析)</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            AI应用实践-智能BI(智能问数与智能数据分析)，主要介绍智能BI，包括商业智能(BI)定义、历史发展、智能BI核心技术、系统架构设计、关键技术、产品设计要点、设计实施过程、优秀实践产品、产品实践案例、当前技术挑战、长远设想和个人思考。
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E5%BA%94%E7%94%A8/" class="post-category">
                                    应用
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/AI/">
                        <span class="chip bg-color">AI</span>
                    </a>
                    
                    <a href="/tags/%E7%AE%97%E6%B3%95/">
                        <span class="chip bg-color">算法</span>
                    </a>
                    
                    <a href="/tags/%E5%BA%94%E7%94%A8/">
                        <span class="chip bg-color">应用</span>
                    </a>
                    
                    <a href="/tags/BI/">
                        <span class="chip bg-color">BI</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>


<script src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
    });
</script>



    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/libs/aplayer/APlayer.min.js"></script>
<script src="/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2025</span>
            
            <a href="/about" target="_blank">青山生柳</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">118.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2025";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/VarAmnematchinensis/" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>















    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fas fa-rss"></i>
    </a>

</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    

    
    
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

<script async>window.onload=function(){var a=document.createElement('script'),b=document.getElementsByTagName('script')[0];a.type='text/javascript',a.async=!0,a.src='/sw-register.js?v='+Date.now(),b.parentNode.insertBefore(a,b)};</script></body></html>