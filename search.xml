<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>大模型微调(Tuning)实践</title>
      <link href="/2025/10/01/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83(Tuning)%E5%AE%9E%E8%B7%B5/"/>
      <url>/2025/10/01/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83(Tuning)%E5%AE%9E%E8%B7%B5/</url>
      
        <content type="html"><![CDATA[<h2 id="大模型微调-Tuning"><a href="#大模型微调-Tuning" class="headerlink" title="大模型微调(Tuning)"></a>大模型微调(Tuning)</h2><h3 id="大模型微调-Tuning-1"><a href="#大模型微调-Tuning-1" class="headerlink" title="大模型微调(Tuning)"></a>大模型微调(Tuning)</h3><p><strong>定义</strong>：在预训练模型基础上，调整LLM的参数以适应特定领域特定任务的过程。微调量取决于任务复杂度和数据集大小。<br><strong>本质</strong>：迁移学习。利用从源数据集学习到的新知识迁移到目标数据集上。对于预训练模型而言，微调就是将预训练模型迁移到新目标数据集上。<br><strong>价值</strong>：低成本效益和特定领域高适应性+防止过拟合，提升数据训练效果.</p><ol><li>低成本效益和特定领域高适应性：对比利用Prompt engineering(提示词工程)来引导模型的输出行为，微调SFT可以更高效地优化模型的输出表现，可以更直接执行任务，减少对复杂提示词引导的依赖。更重要的是，通过微调技术，针对特定领域可以在小模型上实现接近大模型的性能效果，降低推理的计算成本和延迟过程，更好的适应特定领域的专业术语和内容结构，提高模型在这些领域的使用效果，提升特定领域任务的表现。</li><li>防止过拟合，提升数据训练效果：预训练的过程可能会导致模型过拟合无监督学习的任务，导致在特定任务上效果不好，特别是数据稀缺而且缺乏标签数据的特定领域。微调可以使模型聚焦于这些特定领域的任务数据，在减少过拟合风险的同时，针对有限的特定数据做针对性的高效训练，使得在有限数据的情况下也能获得好的模型性能。</li></ol><h3 id="训练数据下的微调分类"><a href="#训练数据下的微调分类" class="headerlink" title="训练数据下的微调分类"></a>训练数据下的微调分类</h3><p>微调分类:增量预训练+SFT监督微调+RLHF人类反馈下的强化学习<br>SFT监督微调方法分类：指令微调(IFT)+思维链(COT)</p><ol><li><p>指令微调(IFT):<br>来源：<a href="https://arxiv.org/abs/2308.10792v2">Instruction Tuning for Large Language Models: A Survey</a><br>定义：使用监督学习的方法，在&lt;人类指令、已知输入、期望输出&gt;构成的数据集上，训练LLM的过程。<br>场景：对话、信息提取、代码生成等。<br>难度：适中<br>方法：数据集准备+微调<br>数据集准备：按照&lt;instruction人类指令、input已知输入、output期望输出&gt;模板人为构造数据集，或者借助LLM来扩展基于少量手写样例的数据集，通过给LLM指令，通过多样化的输出形成指令数据集。<br>微调：基于准备好的数据集，按照监督学习的方式训练模型，根据给定指令和输入到LLM中，根据预测输出和期望输出的loss计算，反向优化训练LLM模型。</p></li><li><p>思维链(COT):<br>来源：<a href="https://arxiv.org/abs/2310.04959">Towards Better Chain-of-Thought Prompting Strategies: A Survey</a><br>定义：可以视为指令微调的一种，更强调在指令中明确的要求回答要给出“推理过程”，并在期望输出内容中有推理过程。<br>场景：推理类任务<br>难度：适中<br>方法：COT数据集准备+微调<br>数据集准备：按照&lt;Question问题描述+按照循序渐进的方式逐步思考并回答问题、Answer推理过程(逐步分解问题)+最后答案&gt;模板人为构造数据集。<br>微调：和指令微调(IFT)的微调方法类似，注意其中在逻辑推理内容。</p></li></ol><h3 id="训练方法下的微调分类"><a href="#训练方法下的微调分类" class="headerlink" title="训练方法下的微调分类"></a>训练方法下的微调分类</h3><p>训练方法下的微调分类：全微调+部分微调(也称为参数高效微调)</p><ol><li><p>全微调(Full Fine-Tuning, FFT):<br>定义：对预训练模型全体进行微调，模型的所有参数和模型层全部被更新和优化，已达成目标任务。<br>应用场景：适用于目标任务和预训练模型之间存在差异的情况，或者需要模型具体较高灵活度和自适应的情况。<br>缺点：计算资源和时间要求较高，训练成本高;灾难性遗忘，即只微调好特定领域的表现，导致其他表现好的领域能力反而变差。</p></li><li><p>部分微调(即参数高效微调，Parameter-Efficient Fine-Tuning, PEFT):<br>定义：在保存预训练模型底层参数不变的情况下，只更新模型的少数模型层和参数，以实现在保留预训练模型通用知识的情况下，适应特定目标任务的目的。<br>缺点：相比全微调计算资源和时间消耗较少，但是性能会比全微调有所减低，无法达到全微调在特定领域上的性能水平，好处就是训练成本低节省存储成本。<br>技术方法：LORA&#x2F;P-Tuning&#x2F;Prompt Tuning&#x2F;Prefix-Tuning</p></li></ol><h3 id="新兴微调技术"><a href="#新兴微调技术" class="headerlink" title="新兴微调技术"></a>新兴微调技术</h3><p><a href="https://www.datacamp.com/blog/reinforcement-fine-tuning">强化微调(Reinforcement Fine-Tuning, RFT)</a><br>定义：利用奖励驱动的训练循环来完善LLM的知识的计算</p><h3 id="当前主流的参数高效微调方法"><a href="#当前主流的参数高效微调方法" class="headerlink" title="当前主流的参数高效微调方法"></a>当前主流的参数高效微调方法</h3><h4 id="微调综述"><a href="#微调综述" class="headerlink" title="微调综述"></a>微调综述</h4><p><a href="https://arxiv.org/abs/2303.15647">Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning</a><br>综述论文，系统地概述参数高效微调方法，涵盖了 2019 年初至 2024 年中期发表的 50 多篇论文，并评估这些参数高效微调方法的表现和性能。</p><h4 id="In-Context-Learning-上下文学习"><a href="#In-Context-Learning-上下文学习" class="headerlink" title="In-Context Learning(上下文学习)"></a>In-Context Learning(上下文学习)</h4><p><strong>介绍</strong>：轻量级无参数更新方法。在不修改模型和其权重的条件下，利用在输入的Prompt中添加和任务相关的上下文背景信息和样例，使模型更好地生成预期结果。<br><strong>特点</strong>：不需要更新模型参数，成本低，用示例做驱动灵活度高。通过和Prompt Engineering的精细化优化结合，可以获得更好的输出效果，达成10%-15%的性能提升，其作为监督微调的补充，利用Prompt精细化设计，在不依赖格外训练的情况下，实现在模型在特定任务上的更好工具使用能力。<br><strong>优点</strong>：可以在不修改模型和其参数的情况下，用于快速尝试和验证；在有限带标签数据用于监督微调的情况下，提升模型在特定任务上的表现。<br><strong>缺点</strong>：由于没有修改模型权重参数，其在特定任务上的表现不然全微调和参数高效微调。<br><strong>基本构成要素</strong>：Role(角色，必选)+Instruction(指令，必选)+Backgroud Information(背景信息，可选) + Example(示例,可选) + Specific Requirement(具体要求,可选) + Text Input(输入文本, 必选) + 工具列表(可选)</p><ol><li>Role(角色，必选):明确LLM在系统中的角色身份，聚集核心职责。</li><li>Instruction(指令，必选):准确清晰描述用户意图和任务要求，并指定输出格式的要求。</li><li>Backgroud Information(背景信息，可选)：提供帮助模型理解用户意图的上下文背景信息，包括历史信息、系统设置和用户偏好等内容。</li><li>Example(示例,可选)：在token数量限制下，few-shot少量高质量、覆盖绝大多数情况和潜在复杂场景下的工具提示调用示例，内容应该包括完整输入、推理过程思维链(CoT)、工具调用结果呈现等。</li><li>Specific Requirement(具体要求,可选)：确定模型对任务说明的详细要求，包括对输出格式的具体要求，帮助模型理解指令要求。</li><li>Text Input(输入文本, 必选)：模型处理的实际问题，一般是用户的直接文本输入。</li><li>工具列表(可选)：可以使用OpenAI格式规范，清晰明确提供模型可调用的工具内容信息，工具应包括其名称、参数和功能说明等。</li></ol><p><strong>相关论文</strong>：<br>[1] <a href="https://www.arxiv.org/abs/2510.04618">Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models</a><br>[2] <a href="https://arxiv.org/abs/2509.25140">ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory</a></p><h4 id="Prefix-Tuning"><a href="#Prefix-Tuning" class="headerlink" title="Prefix Tuning"></a>Prefix Tuning</h4><p><strong>来源</strong>：<a href="https://arxiv.org/abs/2101.00190">Prefix-Tuning: Optimizing Continuous Prompts for Generation</a><br><strong>方法</strong>：和Prompt Tuning和P-Tuning相比，Prefix Tuning将Prompt视为可学习的prefix(前缀)，在输入数据增加prefix(前缀)做微调，即在输入token前构造和任务相关的virtual tokens作为prefix, 训练时值更新prefix部分的参数，每个下游任务都单独训练一套prefix token。<br><strong>作用</strong>：prefix(前缀)其实就是引导模型提取输入中的特定信息，进而更好地生成结果。针对不同下游任务训练的prefix(前缀)可保存起来，当用户在切换不同下游任务时，可以通过加载不同prefix(前缀)参数，实现模型功能快速切换。<br><strong>缺点</strong>：前缀token会占用序列长度，造成一定的额外开销，而且效果存在上限，模型表现不一定会随着prefix(前缀)延长而提高，有用模型输入token有限，增加prefix(前缀)后反而有可能降低了原来prompt的表现；prefix Tuning的线性插值比较复杂。</p><h4 id="Prompt-Tuning"><a href="#Prompt-Tuning" class="headerlink" title="Prompt Tuning"></a>Prompt Tuning</h4><p><strong>来源</strong>：<a href="https://arxiv.org/abs/2104.08691">The Power of Scale for Parameter-Efficient Prompt Tuning</a><br><strong>方法</strong>：最简单的参数高效性微调方法。固定LLM全部参数，侧重于制作可指导预训练模型的输入提示和模型，在训练数据前加入一小节Prompt,只训练Prompt的词嵌入层(Embedding模块)，仅更新部分embedding参数来实现低成本微调大模型。其中Prompt可分为soft Prompt(Prompt Embedding采用随机初始化方式)和hard Prompt(Prompt Embedding采用人力实践准备做词嵌入初始化，编写Hard Prompt的过程又称Prompt Engineering)。<br><strong>特点</strong>：任务不通用和规模不通用；和Prefix Tuning相比，参与训练的参数量和更新参数量更小，更节省显存资源.<br><strong>相关论文</strong>：<br>[1] <a href="https://arxiv.org/abs/2208.02532">Prompt Tuning for Generative Multimodal Pretrained Models</a><br>[2] <a href="https://arxiv.org/abs/2109.04332">PPT: Pre-trained Prompt Tuning for Few-shot Learning</a></p><h4 id="P-tuning"><a href="#P-tuning" class="headerlink" title="P-tuning"></a>P-tuning</h4><p><strong>来源</strong>：<a href="https://arxiv.org/abs/2103.10385">GPT Understands, Too</a><br><strong>方法</strong>：在Prompt Tuning基础上，对Prompt部分进一步编码计算，将Prompt转换为可学习的Embedding层，并利用MLP(多层感知机,multilayer perceptron)和LSTM(长短期记忆网络,Long short-term memory)方式对Prompt Embedding做处理，其中Prompt只能是soft Prompt形式。<br><strong>特点</strong>：通过引入Prompt encoder来建模virtual tokens的相互依赖会收敛更快，效果更好。将目标任务的离散文本映射为可连续的优化向量，解决离散文本难定向优化的问题。少量高质量语料训练新增加的参数，其训练消耗小，训练效率更高。冻结原来的参数，利用前缀Prefix来适配不同任务，规避了模型遗忘的问题，同时由于可以在前缀Prefix中训练多个简单任务，其适配性更好，减少部署所需要的资源。</p><h4 id="P-tuning-V2"><a href="#P-tuning-V2" class="headerlink" title="P-tuning V2"></a>P-tuning V2</h4><p><strong>来源</strong>：<a href="https://arxiv.org/abs/2110.07602">P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks</a><br><strong>方法</strong>：在transformer的每一层都加入Prompt token作为输入，引入多任务学习，针对不同任务采用不同的提示长度。<br><strong>目标</strong>：让Prompt Tuning方法能在不同规模的预训练模型、不同目标任务结果上实现和Fine-Tuning一样的结果。<br><strong>特点</strong>：解决Prompt Tuning无法再小模型上有效提升的问题，改进对复杂硬标记任务的效果。Prompt长度是P-tuning V2最关键参数，不同类型难度的自然语言理解任务可能会因为不同的长度Prompt配置实现不同的效果。根据论文中的实验反馈，简单的分类任务需要较短的Prompt(少于10 Token),而复杂困难的任务(如序列标注等)，倾向于使用更长的Prompt(超过100)。</p><h4 id="Adapter-Tuning"><a href="#Adapter-Tuning" class="headerlink" title="Adapter Tuning"></a>Adapter Tuning</h4><p><strong>方法</strong>：在transformer架构中multi-head self-attention(多头注意力机制)和fully conneted layers(全连接层)后添加Adapter层进行微调。Adapter层由fully conneted layers(全连接层,将高维输入映射为低维表示) + Nonlinear activation(非线性激活函数) + fully conneted layers(全连接层，将低维输入映射回高维空间)三层构成，降低训练使更新的参数量。微调过程中，模型其他参数冻结，只更新Adapter层的参数。<br><strong>缺点</strong>: 添加Adapter层后，导致模型变深，导致训练和推理速度变慢</p><h4 id="LoRA-Low-Rank-Adaptation"><a href="#LoRA-Low-Rank-Adaptation" class="headerlink" title="LoRA(Low-Rank Adaptation)"></a>LoRA(Low-Rank Adaptation)</h4><p><strong>来源</strong>：<a href="https://arxiv.org/abs/2106.09685">LoRA: Low-Rank Adaptation of Large Language Models</a><br><strong>作者</strong>: 2021年微软提出的轻量化微调算法<br><strong>方法</strong>：对预训练模型而言，适用目标任务的过程，就是变动模型低维参数的过程，通过低秩分解的方法来模拟参数的改变量，用极小的参数量来实现LLM的间接训练。具体来说，在训练时，对输入分别和原始权重的两个低秩矩阵进行计算，得到最终结果，训练完成后将两个低秩矩阵和原始模型中的权重进行合并，合并后的模型和原始模型无区别，从而避免推理期间Prompt方法带来的额外计算量问题。<br><strong>效果</strong>：根据论文，添加LoRA结构的transformer模型，在仅更新少量参数的情况下，就可以实现微调精度近似全参微调的效果。<br><strong>数学公式如下</strong>：<br>$$<br>h &#x3D; Wx + \Delta W \approx Wx + BAx &#x3D; Wx + \frac{\alpha}{r}BAx<br>$$<br>$x$表示输入，$h$表示模型输出，$W$表示pretrained weights权重矩阵，$\Delta W$表示增量权重矩阵，都是d维方阵，$A$是$r<em>d$维矩阵，$B$是$d</em>r$维矩阵，r为$\Delta W$的秩。$\alpha$表示缩放因子，作为超参数表示LoRA从特定任务学习中获得的特征缩放沉淀。可以先随机设置$r$和$\Delta &#x3D; r$，保存$\Delta$不变的条件下，利用调整$r$来实现参数微调的效果。<br><strong>数学原理</strong>：线性代数中的<a href="https://zh.wikipedia.org/wiki/%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3">奇异值分解(SVD)</a>,奇异值分解(SVD)分为紧凑和截断两种形式，可以视为对矩阵数据的无损和有损压缩，借助奇异值分解(SVD)实现对矩阵数据在Frobenius norm(<a href="https://zh.wikipedia.org/wiki/%E7%9F%A9%E9%99%A3%E7%AF%84%E6%95%B8">F-范数</a>)下的近似，从而利用低秩低维矩阵来表达原矩阵。<br><strong>LoRA适配位置选择</strong>：理论上对于具有低秩性质同时参数密集的模型层都可以利用LoRA来处理，添加LoRA适配模块，典型如多头注意力层和FFN层。需要更加不同的显存硬件资源，设置不同的参数训练方案。<br><strong>特点</strong>：可以切换到不同任务上，设计简单效果好,同时可训练参数的层次更为多样化。<br><strong>优点</strong>：参数规模降低，权重维度下降，训练快，减少了模型调参对硬件显存的要求。适配度高，更灵活，可以针对不同任务训练不同的LoRA,在最大化共享预训练权重的基础上，保存不同任务下的新增权重，在使用少量内存的情况下，实现对不同任务的灵活适配。在部署过程中可将训练矩阵和冻结权重合并处理，尽量减少模型遗忘的问题。</p><h3 id="微调数据准备"><a href="#微调数据准备" class="headerlink" title="微调数据准备"></a>微调数据准备</h3><p><strong>微调数据要求</strong>：高质量、真实性、针对性、多样性和广覆盖、数量级</p><ol><li>高质量：高质量的Prompt以及高质量reponse可以很好的让模型学会遵守指令。garbage in garbage out。对复杂逻辑和模糊意图的BadCase，可以补充思维链CoT的数据，通过引导模型做推理和决策，帮助模型学习怎样分解处理问题。</li><li>真实性：优先使用真实用户交互产生的BadCase标准和补充数据，真实数据可以很好捕获到实际应用场景中的各种变化和复杂性，从而使微调后的模型可以更好适应真实场景。</li><li>针对性：不同prompt对模型的重要性和必要性不一样，针对用户反馈差的数据人工修改大模型回复，更改为更优质的数据加入到微调数据中。</li><li>多样性和广覆盖：prompt和response的类型多样、场景多样、避免重复，尽量覆盖到全场景确保数据的多样性，覆盖各种长尾和边缘问题情况，避免过拟合导致的模型效果不佳，缺乏泛化能力。</li><li>数量级：最低要准备100条数据，最好达到5000条为最优，补充数据可以采用少量多次的策略，每次补充针对性强的数据做小规模迭代训练，观察效果后再尝试。<br>总之微调数据的准备，要重复挖掘各个维度，分析数据的分布特点，合理划分训练、测试和验证集。</li></ol><p>注意：</p><ol><li>基础模型能力在特定目标任务处理上能力越弱，其更需要数据集来补充加强其能力的提升，对数据集的数量和覆盖度的要求就越高。</li><li>复杂逻辑推理、多步多工具调用、精细化参数组合使得任务复杂度越高，其对数据的需求量也越大。</li></ol><p><strong>微调数据构造</strong>：获取和合成<br>数据获取方式：格式化NLP任务数据、从日常对话中获取数据、数据合成<br>数据合成方式：批量生成(Batch Generation)、脚本合成(Script Synthesis)、数据增强(Data Augmentation)<br><strong>批量生成</strong>(Batch Generation):<br>Self-Instruct(出自论文<a href="https://arxiv.org/abs/2212.10560">Self-Instruct: Aligning Language Models with Self-Generated Instructions</a>)、Evol-Instruct(出自论文<a href="https://arxiv.org/abs/2304.12244">WizardLM: Empowering large pre-trained language models to follow complex instructions</a>)。利用大模型根据自身能力和少量的样例数据，自动生成海量指令响应对数据，高效扩展数据库。</p><ol><li>Self-Instruct步骤：指令生成-&gt;分类任务识别-&gt;实例生成-&gt;过滤和后处理</li><li>Evol-Instruct步骤：指令生成-&gt;响应指令-&gt;进化消除-&gt;数据集拆分<br>指令生成：使用LLM生成多样化而且负责的指令，包括深度进化(包括添加约束、深化、具体化，增加推理步骤以及输出负责化)和广度进化(增加主题和技能覆盖度和数据集多样性)。<br>响应指令：使用LLM为进化后的指令生成相应的响应。<br>进化消除：过滤进化后失败的指令。<br>数据集拆分：测试数据集中的数据质量分布要和训练数据集中大致一致，内容不能重复，避免测试不准确。<br><strong>脚本合成(Script Synthesis)</strong>:适用于具有特定格式和规则下的数据，特别是控制参数组合和特定场景时。先定制灵活的数据Prompt模板，在利用python等脚本自动化合成数据。例如先定义工具调用的prompt模板，利用Python随机组合不同下的用户输入、参数值和工具名称，自动化生成结构化语料。<br><strong>数据增强(Data Augmentation)</strong>:是对现成语料的扩展增强，例如将输入内容做同义词替换、结构变换、删除无关信息或口语化表达等，尽量增加数据集的多样性和泛化能力，涉及工具调用部分，可以对参数值做随机组合，来生成更多的变体。低成本扩充语料，尽可能模拟用户输入复杂性。</li></ol><h3 id="大模型能力评估评测指标"><a href="#大模型能力评估评测指标" class="headerlink" title="大模型能力评估评测指标"></a>大模型能力评估评测指标</h3><p>目的：评估模型在未见过数据上的泛化能力和预测能力.<br>问题分类：封闭式问题和开放式问题.</p><ol><li>封闭式问题：<br>方法：自动化评测。根据模型输出和标准答案，使用脚本完成模型评测。<br>封闭式评测数据集：</li></ol><table><thead><tr><th>数据集名</th><th>数据集内容</th></tr></thead><tbody><tr><td><a href="https://github.com/hendrycks/test">MMLU</a></td><td>出自论文<a href="https://arxiv.org/abs/2009.03300">Measuring Massive Multitask Language Understanding</a>。大规模多任务语言理解基准数据集，用于评估大型语言模型（LLM）在广泛知识领域和复杂任务中的综合能力。</td></tr><tr><td><a href="https://cevalbenchmark.com/index_zh.html">C-Eval</a></td><td>出自论文<a href="https://arxiv.org/abs/2305.08322">C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models</a>。全面的中文基础模型评估套件.C-Eval包含四个难度级别（初中、高中、大学和专业）的多项选择题，涵盖了从人文到科学与工程的52个不同学科</td></tr></tbody></table><ol start="2"><li>开放式问题：<br>方法：人工评测和大模型评测<br>人工评测：人工打分是金标准，关键在于如何指定评价规则.<br>评分标准：明确评估维度，角色扮演场景评估看重大模型人设遵循(言行是否符合角色设定身份、语气和特点等)和回答质量(回答是否和上下文对话相符，内容丰富有建设性)，在打分时要尽量屏蔽掉预测结果的来源，以防止人为偏见的引入。<br>计分标准：GSB打分制和绝对分值制.<br><em><strong>GSB打分制(Good Same Bad)</strong></em>:用于评判对同一个评估集的两份预测结果之间的好坏，可以直接对比两种模型或参数组之间的好坏。Good代表结果A比B好，Same代表回答质量相近，Bad代表A不如B.<br><em><strong>绝对分值制</strong></em>:按照一定评分标准直接对大模型的输出结果做评分，用于横向比较多个模型的结果。<br>典型微调数据集：</li></ol><table><thead><tr><th align="left">数据集名</th><th>描述</th><th align="left">中英文</th></tr></thead><tbody><tr><td align="left"><a href="https://modelscope.cn/datasets/AI-ModelScope/alpaca-gpt4-data-zh">alpaca-gpt4-data-zh</a></td><td>该数据集为GPT-4生成的中文数据集，用于LLM的指令精调和强化学习等</td><td align="left">中文</td></tr><tr><td align="left"><a href="https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/Auto-CoT">QingyiSi&#x2F;Alpaca-CoT</a></td><td>指令微调数据集集合（Alpaca-CoT）</td><td align="left">中英文</td></tr></tbody></table><p><strong>大模型评测</strong>：使用大模型打开可辅助降低人工成本。为保证评估结果有效，自动打分结果也要人工复查。<br>开放式评测数据集：</p><table><thead><tr><th>数据集名</th><th>数据集内容</th></tr></thead><tbody><tr><td><a href="https://github.com/hendrycks/test">MT-Bench</a></td><td>出自论文<a href="https://arxiv.org/abs/2306.05685">Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</a>。MT-Bench是一个专门用于评估大模型能力的测试框架，它涵盖了写作、角色扮演、推理、数学、编码、人文、提取以及STEM（科学、技术、工程、数学）等8个不同领域的问题。这些问题旨在全面考察大模型在各个方面的表现。</td></tr><tr><td><a href="https://github.com/THUDM/AlignBench">AlignBench</a></td><td>出自论文<a href="https://arxiv.org/abs/2311.18743">AlignBench: Benchmarking Chinese Alignment of Large Language Models</a>。AlignBench 是第一个多维度全面评估中文大模型对齐水平的评测基准。此仓库包含了 AlignBench 的介绍信息、数据和代码。</td></tr></tbody></table><p>常用任务的评价指标：</p><ol><li>分类型任务：准确率(accuracy &#x3D; (TP + TN)&#x2F;(TP + FP + TN + FN))、召回率(recall &#x3D; TP&#x2F;(FP + FN))、精确率(precision &#x3D; TP&#x2F;(TP + FP))、F1分数(F1 Score &#x3D; 2 * (precision * recall)&#x2F;(precision + recall))、ROC曲线和AUC等。</li><li>生成类任务：Rouge和Bleu, 其中Rouge(Recall-Orientaed Understudy for Gisting Evaluation)利用自动生成的内容和一组人工参考内容做对比计算，得到分值，借以衡量自动生成的内容和参考内容之间的“相似度”。分为单个词(Rouge-1)、两个连续词(Rouge-2)、最长公共子序列(Rouge-L)三种类型。Bleu(Bilingual Evaluation Understudy)，主要用于机器翻译任务，计算模型输出和参考内容直接的n-gram(通常为1-4gram)的精确度，并对过短的输出进行惩罚，范围在[0,1]之间，一般越高越好。</li></ol><h3 id="大模型训练开发基本流程"><a href="#大模型训练开发基本流程" class="headerlink" title="大模型训练开发基本流程"></a>大模型训练开发基本流程</h3><p>大模型训练开发基本流程：pre-training(预训练)+post_training(后训练)</p><ol><li><strong>pre-training(预训练)</strong>：使用数据训练基础大模型，再通过增量预训练来为模型注入特定领域知识。<br>1.1 <strong>预训练(Pre-Training)</strong>: 利用大量数据、算力资源，利用无监督训练方法获得基础大模型，该模型虽然有强大的文本生成能力，但存在不适应特定任务的能力，需要微调。<br>1.2 <strong>增量预训练(Continued Pre-Training</strong>:在通用的基础大模型基础上进行二次训练，利用特定领域的语料数据进行增量预训练，注入领域知识后形成垂域大模型。</li><li><strong>post_training(后训练)</strong>：利用监督微调、偏好对齐、上下文学习、特定任务微调，使训练好的模型更好的适应特定领域的任务，并符合人类表达行为和习惯。<br>2.1 <strong>监督微调(Supervised Fine-Tuning, SFT)</strong>:在高效率指令-响应对数据集(Instruction-Response)上对基础大模型做微调后，使模型适应特定任务。例如开源模型经过问答任务微调后的chat模型版本。Qwen 1.5-14B-Chat-GQPT-Int4(Qwen代表模型系列，1.5代表模型版本，14B表示模型参数量，Chat代表模型经过问答任务微调，GQPT代表量化格式)<br>2.2 <strong>偏好对齐(Reinforcement Learning from Human Feedback, RLHF)</strong>:基于人类反馈的强化学习。利用人类反馈优化模型输出的生成质量，促使其生成的回答内容符合人类价值观，对齐人类偏好。一般方法是利用训练好的奖励模型(Reward Model, RM)代替人类手动打分，利用强化学习(RL)框架做大规模自动优化。<br>2.3 <strong>上下文学习(In-Context Learning, ICL)</strong>:利用在Prompt中添加few-shot(少量示例)引导模型完成任务，不需修改模型参数，好处是可以快速适应新任务，但是其性能受限于Prompt的质量和模型本身上下文窗口大小。<br>2.4 <strong>特定任务微调(Task-sepcific Fine-tuning)</strong>:聚集阶段，针对特定的目标任务，利用该任务少量标注数据对模型做进一步微调，以实现最大化模型在该任务上的性能。将通用模型适配到具体应用场景中，使成为该领域的专家。该阶段依赖微调数据集的构建，可以先根据不同的泛化情况构建不同泛化的数据集，再针对表示不佳的BadCase内容纠错补缺，精细化补充少量的高质量数据集，再让模型在此数据集上迭代训练。</li></ol><h3 id="大模型微调基本流程"><a href="#大模型微调基本流程" class="headerlink" title="大模型微调基本流程"></a>大模型微调基本流程</h3><p>大模型微调流程(以现成模型为基础)<br><strong>流程</strong>：数据集准备-&gt;模型选择-&gt;微调策略选择-&gt;超参数设置-&gt;初始化模型参数-&gt;微调训练-&gt;模型评估和调优-&gt;模型性能测试-&gt;模型部署和应用</p><ol><li>数据集准备：收集并准备和目标任务相关的数据集。在保证数据集质量和标准准确性的前提下，做必要的数据清洗和数据预处理。</li><li>模型选择：根据目标任务和数据集特点，选择合适的大模型</li><li>微调策略选择：根据任务需求和可用资源情况，选择适当的微调策略，确定微调层级和范围。</li><li>超参数设置: 确定微调过程中的超参数，如学习率等。这些超参数会影响微调的性能和收敛速度。</li><li>初始化模型参数: 根据微调策略和模型选择，初始化微调模型参数和权重。全微调的所有参数都需要初始化，部分微调的要选择一些模型层的参数做初始化。</li><li>微调训练: 运用训练数据集和微调策略，对模型开始训练，根据设置的超参数和优化算法，逐渐调整模型参数以最小化损失函数。</li><li>模型评估和调优: 使用验证数据集对模型做性能评估，并根据评估结果调整超参数和微调策略。</li><li>模型性能测试: 微调完成后，使用验证数据集对最终微调模型做性能评估，获取最终性能指标。</li><li>模型部署和应用: 将微调后的模型部署到实际应用中，并根据实际情况做优化和调整，满足实际需要。</li></ol><p><strong>注意事项</strong>：</p><ol><li>超过10B的大模型在训练过程中需要特别关注训练不稳定和不收敛的问题，遇到这类问题时，可以考虑在训练崩溃时调整学习率，跳过异常的batch。对于词嵌入层的梯度爆炸或者消失的问题，可以使用BF16来避免精度溢出导致的异常。</li><li>使用全量微调模型适配自身领域，需要重点关注模型参数存储、前向传播和梯度更新。模型结构决定模型参数，模型的前向传播产生的中间激活值参数会影响训练所需的显存大小，模型从梯度参数更新下的优化器也需要对应的参数量，这些因素都会导致需要在模型训练前，借助对显存大小的估计来确定显卡硬件资源。</li></ol><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><p>[1] <a href="https://arxiv.org/abs/2212.10560">Self-Instruct: Aligning Language Models with Self-Generated Instructions</a><br>[2] <a href="https://arxiv.org/abs/2001.08361">Scaling Laws for Neural Language Models</a><br>[3] <a href="https://arxiv.org/abs/2402.17152">Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations</a><br>[4] <a href="https://arxiv.org/abs/2309.12307">LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models</a><br>[5] <a href="https://arxiv.org/abs/2310.05209">Scaling Laws of RoPE-based Extrapolation</a><br>[6] <a href="https://github.com/huggingface/text-generation-inference">Text Generation Inference</a><br>[7] <a href="https://arxiv.org/abs/2203.02155">Training language models to follow instructions with human feedback</a><br>[8] <a href="https://arxiv.org/abs/2305.18290">Direct Preference Optimization: Your Language Model is Secretly a Reward Model</a><br>[9] <a href="https://arxiv.org/abs/2310.12036">A General Theoretical Paradigm to Understand Learning from Human Preferences</a><br>[10] <a href="https://arxiv.org/abs/2403.07691">ORPO: Monolithic Preference Optimization without Reference Model</a><br>[11] <a href="https://arxiv.org/abs/2402.03300">DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models</a><br>[12] <a href="https://arxiv.org/abs/2504.02495">Inference-Time Scaling for Generalist Reward Modeling</a><br>[13] <a href="https://arxiv.org/abs/2505.17017">Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO</a><br>[14] <a href="https://arxiv.org/abs/2401.10020">Self-Rewarding Language Models</a><br>[15] <a href="https://arxiv.org/abs/2505.03335">Absolute Zero: Reinforced Self-play Reasoning with Zero Data</a><br>[16] <a href="https://arxiv.org/abs/2403.05822">TrafficGPT: Breaking the Token Barrier for Efficient Long Traffic Analysis and Generation</a><br>[17] <a href="https://arxiv.org/abs/2505.20866">Respond to Change with Constancy: Instruction-tuning with LLM for Non-I.I.D. Network Traffic Classification</a><br>[18] <a href="https://time.geekbang.org/course/intro/100768101">大模型微调实践课</a><br>[19] <a href="https://arxiv.org/abs/2307.09288">Llama 2: Open Foundation and Fine-Tuned Chat Models</a><br>[20] <a href="https://arxiv.org/abs/2303.18223">A Survey of Large Language Models</a><br>[21] <a href="https://github.com/morecry/CharacterEval?tab=readme-ov-file">CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation</a><br>[22] <a href="https://zhuanlan.zhihu.com/p/6646407396">大模型 LoRA 微调原理详解</a><br>[23] <a href="https://udlbook.github.io/udlbook/">理解深度学习</a><br>[24] <a href="https://prompt-engineering.xiniushu.com/">面向开发者的 Prompt 工程</a></p>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
            <tag> 算法 </tag>
            
            <tag> 大模型微调 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>神经网络研究-Transformer架构</title>
      <link href="/2025/09/30/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%A0%94%E7%A9%B6-Transformer%E6%9E%B6%E6%9E%84/"/>
      <url>/2025/09/30/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%A0%94%E7%A9%B6-Transformer%E6%9E%B6%E6%9E%84/</url>
      
        <content type="html"><![CDATA[<h2 id="神经网络研究-Transformer架构"><a href="#神经网络研究-Transformer架构" class="headerlink" title="神经网络研究-Transformer架构"></a>神经网络研究-Transformer架构</h2><p>介绍：是第一个完全基于自注意力机制的处理序列转换模型的神经网络架构，是大模型当前的主流架构。</p><h3 id="典型神经网络优缺点"><a href="#典型神经网络优缺点" class="headerlink" title="典型神经网络优缺点"></a>典型神经网络优缺点</h3><p>RNN的缺点：无法并行计算，长语句输入计算效率低；梯度消失&amp;爆炸&amp;链式信息损失，导致难以实现长期记忆<br>CNN的缺点：输入长度有限.优点是可并计算，计算效率高<br>Transformer相比RNN和CNN的特点：<br>和RNN相比，可并计算，计算效率高；有固有的全局视野，可捕获长距离依赖。本质是层内没有序列限制。<br>和CNN相比，感受长度不受限制，可灵活处理位置信息。本质是可以完整输入的感受野。</p><h3 id="关键技术"><a href="#关键技术" class="headerlink" title="关键技术"></a>关键技术</h3><ol><li><p>Self Attention(自注意力)：<br> 意义：核心组件。<br> 用途：通过计算每个词和其他词之间的相似度来建立他们之间的关系，并根据这些关系加权计算每个词的表示。优点是能捕获序列中任意两个位置之间的关系，在序列建模任务中表现良好。使用自注意力的模型可以捕获长距离的依赖关系，提高并行计算效率。<br> 过程：由输入表示、权重计算和加权求和三部分组成。输入表示是指输入序列中的每个词通过词嵌入(Embedding)转换为向量表示。每个词有三种向量组成(查询、键、值向量)，对于每个词，查询向量和键向量通过线性变换得到，然后通过相似度函数（点积或缩放点积）计算查询向量和键向量的相似度，将相似度归一化得到注意力权重，进而得到每个词和其他词的相关性权重。再使用这些权重对值向量加权求和，获得每个词的上下文表示。<br> 单头注意力机制计算公式如下：<br>$$<br>Attention(Q, K, V) &#x3D; softmax(\frac{QK^T}{\sqrt{d_k}})V<br>$$<br>$Q$是query(查询向量)，来自用户输入,计算公式$Q &#x3D; ZW_{Q}$,$Z$是词嵌入矩阵$E$和位置编码矩阵$P$的和，都是$T<em>d_{model}维度$。$K$是key(键向量)，来自词库,计算公式$Q &#x3D; ZW_{K}$。$V$是value(值向量),计算公式$Q &#x3D; ZW_{V}$, Attention score高，value占比越大。这其中$Q$、$K$、$V$的维度分别是$T</em>d_k$、$T<em>d_k$、$T</em>d_v$,$d_k$是键的维度，用于缩放点积，防止梯度消失，特别是维度比较大的时候。通常$d_k&#x3D;d_v&#x3D;d_{model&#x2F;h}$,其中$h$是head的个数。<br>多头注意力机制计算公式如下：<br>$$<br>MultiHead(Q, K, V) &#x3D; Concat(head_1,…, head_h)W^{O}<br>$$$$<br>where head_i &#x3D; Attention(QW^{Q}<em>{i}, KW^{K}</em>{i}, VW^{VkV}_{i})<br>$$</p></li><li><p>Multi-Head Attention(多头注意力层)：<br> 过程：将输入数据划分为多个head,每个head独立关注输入的不同表示子空间，每个子空间独立计算注意力，从而使用模型并行捕获输入数据中的不同特征和模式信息，综合各项信息来更全面的理解文本。具体来说，就是通过初始权重随机分布，通过梯度下降优化过程，去推动每个head去适应其权重来学习不同的特征，进而减少总体损失。<br> 优势：允许模型关注序列中不同部分的信息；多个注意力头可以并行计算。</p></li><li><p>Positional Encoding(位置编码)<br> 用途：为输入序列中的每个词添加位置信息，进而解决attention缺失词的位置信息的问题，进一步使用序列的顺序信息，在输入表示中添加位置信息编码来注入绝对或者相对位置编码，位置编码可以通过学习或者直接固定获得。<br> 典型的位置编码方法：基于正弦和余弦函数的固定位置编码<br>$$<br>PE_{(pos, 2i)} &#x3D; sin(pos&#x2F;10000^{2i&#x2F;d_{model}})<br>$$$$<br>PE_{(pos, 2i+1)} &#x3D; cos(pos&#x2F;10000^{2i&#x2F;d_{model}})<br>$$<br>其中，$pos$是词位置，$i$是维度索引。</p></li><li><p>Feed Forward Neural Network(前馈层)：通常由两个全连接层，中间使用Relu函数作为激活函数。<br>$$<br>FFN(x) &#x3D; max(0, xW_1 + b_1)W_2 + b_2<br>$$<br>Feed Forward Neural Network(前馈层)目的：对序列中所有位置的表示进行变换，使用同一个多层感知机(MLP)。对同一个layer的不同position，其所作的线性变换是一致的。增加网络结构的非线性，增加参数量，提升模型复杂度和深度，使模型可以处理更复杂的任务。</p></li><li><p>Residual Connectionz(残差连接)+Layer Normalization(归一层)：用于稳定训练过程。</p></li></ol><h3 id="Transformer架构"><a href="#Transformer架构" class="headerlink" title="Transformer架构"></a>Transformer架构</h3><h4 id="Inputs-输入-Output-输出"><a href="#Inputs-输入-Output-输出" class="headerlink" title="Inputs(输入)&#x2F;Output(输出)"></a>Inputs(输入)&#x2F;Output(输出)</h4><p>Inputs(输入)&#x2F;Output(输出，在训练过程中作为输入结果导入):在翻译第一个词时需要填充开始标签<code>&lt;BOS&gt;</code></p><h4 id="Tokenize-分词"><a href="#Tokenize-分词" class="headerlink" title="Tokenize(分词)"></a>Tokenize(分词)</h4><p>Tokenize(分词):是Transformer的第一关，是作为模型的输入。<br><strong>含义</strong>：Token可理解为词元，英文token可视为单词的“拼图”，一个token大约对应于0.75个单词或者3-4个字母，或者对应1-1.8个汉字。Tokenize的过程就是根据词汇表，将文本映射到词ID序列，作为词嵌入的输入。Tokenizer是只分割词元的算法方法。Tokenization是指分割后的单词。</p><p><strong>Tokenize的处理流程</strong>：Normalization(文本清洗、数字处理)-&gt;Pre-Tokenization(基于规则初步分割)-&gt;Model(使用分词算法做字词拆分)-&gt;Post-Tokenization(特殊处理和标记)</p><ol><li>Normalization(文本清洗、数字处理):包括文本清洗、标准化写法、安全和规范化操作。其中文本清洗一般是指去除无用字符、额外空白等，只保留对分词和模型训练有意义的内容。标准化写法是指统一大小写设置，将数字统一格式，确保文本采用统一的字符编码等。安全和规范化操作一般时过滤掉危险内容。</li><li>Pre-Tokenization(基于规则初步分割)：先基于简单规则对文本做初步分割，将文本初步拆分为更小的单元，如句子或者词语等。</li><li>Model(使用分词算法做字词拆分)：利用分词模型算法对文本做处理，生成词汇表(Vocabulary),利用词汇表(Vocabulary),将文本拆分为Token.例如训练好的大语言模型文件中的tokenizer.json&#x2F;tokenizer_config.json&#x2F;vocab.json文件。</li><li>Post-Tokenization(特殊处理和标记):包括序列填充和截断、特殊token信息添加、构建注意力掩码等。保证输入序列长度一致，并在序列的适当位置添加特殊token(例如<CLS>&#x2F;<SEP>),区分实际token和填充token.</li></ol><p><strong>核心参数</strong>：词汇表(Vocabulary).词汇表(Vocabulary)大小影响着模型的泛化能力和计算效率。大词汇表可以提高模型覆盖不同词汇和表达的能力，但也会影响模型处理的速度。</p><p><strong>分词算法</strong>：Work-based(基于单词)、Character-based(基于字符)、Subword-based(子词分词)等</p><ol><li>Work-based(基于单词)：按照单词进行分词，利用空格和标点符号做分割。好处就是简单直观，缺点就是词汇表爆炸、Out-ot-Vocabulary问题严重，同时不能学习到词缀之间的关系。</li><li>Character-based(基于字符)：以char为最小粒度，按照单字符做分词。好处是词汇表比较小(26个英文字符+标点符号)，缺点就是丢失语义信息，每个token的信息密度过低，导致序列过长，解码效率低。</li><li>Subword-based(子词分词)：按照词的subword进行分词。对低频词保留完整词或者拆分为字符，对高频词拆分为更细粒度的子词。好处就是能平衡词汇表的大小和语义表达能力，词表大小适中，解码效率高，也能学习到词缀之间的关系。典型算法：BPE(使用最广泛的分词算法)、WordPrice、Unigram<br> <strong>Byte Pair Encoding(BPE)</strong>：来自论文<a href="https://arxiv.org/abs/1508.07909">Neural Machine Translation of Rare Words with Subword Units</a>，当前LLM的主流方法之一。</li></ol><p>过程如下：构建初始词表-&gt;识别最频繁出现的标记对-&gt;合并最频繁出现的token对-&gt;重复步骤2和3</p><ol><li>构建初始词表：先按照字符粒度先分词，将文本中的每个字符先视为一个token,形成初始词表；</li><li>识别最频繁出现的标记对：扫描语料库，找到出现频率最高的token对,也就是字符或子词；</li><li>合并最频繁出现的token对：将2中频率最高的token对合并为一个新的token后，将新token添加到词表中，并删除被token完成覆盖的token,做词汇表的更新，并将token对的合并加入到合并规则列表中。</li><li>重复步骤2和3：迭代执行2和3步骤，直到达到指定词汇量或者token对不住频繁出现为止。</li></ol><p><strong>评估标准</strong>：词汇表覆盖率、token数量、OOV率.<br>词汇表覆盖率：用于检验时候可以处理多种语言和专业术语。<br>token数量：token过长会导致计算浪费，过短导致信息丢失。<br>OOV率：即Over of Vocabulary Rate.训练中未出现而测试时出现的单词概率。</p><h4 id="Embedding-词嵌入"><a href="#Embedding-词嵌入" class="headerlink" title="Embedding(词嵌入)"></a>Embedding(词嵌入)</h4><p>含义：将离散的输入ID序列中的每个词转换为向量表示(一般为列向量)，通过查表为每个字符赋予语义信息。<br>词嵌入维度：需要嵌入的一句句子中词汇的总数（sequence）* 嵌入向量的维度(d_model)<br>Embedding矩阵参数确定：先初始化，后训练迭代</p><h4 id="Positional-Encoding-位置编码"><a href="#Positional-Encoding-位置编码" class="headerlink" title="Positional Encoding(位置编码)"></a>Positional Encoding(位置编码)</h4><p>Positional Encoding(位置编码)：和输入的词向量维度相同<br><strong>位置编码设计</strong>：唯一性：每个位置的编码是唯一的，这确保模型能够区分序列中的不同位置。周期性：能够根据相位捕获位置关系.正交性：偶数位置和奇数位置的编码是正交，增加编码区分度和信息丰富度。<br><strong>典型方法</strong>：Absolute Positional Encoding(绝对位置编码, APE)、Relative Positional Encoding(相对位置编码,RPE)、Rotary Positional Embedding(旋转位置编码, RoPE)</p><p><strong>Absolute Positional Encoding(绝对位置编码, APE)</strong>:为序列中每个位置分配唯一的固定或可学习的向量，直接表征决定位置索引。分为Sinusoidal编码和Learnable编码。适用于短序列、对位置敏感的任务。缺点是难以反应序列字符之间的相对位置关系，表示不了比预训练文本长度更长的位置向量。</p><ol><li>Sinusoidal编码：Sinusoidal三角式绝对位置编码，也是Transformer论文中使用的方法，利用不同频率的正弦、余弦函数生成位置编码，偶数维度用正弦，奇数维度用余弦。可无需训练，就具有周期性外推能力，但是无法直接表达相对的位置关系，适用于短文本翻译，需要固定位置感知的序列任务，在处理长序列性能时会出现下降，需要进一步微调和扩展。</li><li>Learnable编码：Learnable可学习绝对位置编码，随机初始化位置编码矩阵，作为可训练参数，模型只能感知每个词向量所处的绝对位置，无法感知词向量之间的相对位置。适用早期如BERT的预训练模型，不具备长度外推性。</li></ol><p><strong>Relative Positional Encoding(相对位置编码,RPE)</strong>：建模序列中任意两个位置之间的相对距离而非绝对索引，增强模型对局部结构的感知能力。论文：<a href="https://arxiv.org/abs/1803.02155">Self-Attention with Relative Position Representations</a>.</p><p><strong>Rotary Positional Embedding(旋转位置编码, RoPE)</strong>:将位置信息编码为旋转矩阵，用于查询向量和键向量的注意力计算中，隐式融合绝对位置和相对位置的双重特性，通过旋转位置编码将一个向量旋转到某一个角度，为其赋予位置信息。好处就是可支持超长序列的推理，可通过波长设计自动衰减远距离依赖，借助旋转操作实现相对位置偏移的数学等价。适用于需要处理超长上下文的长文本生成，也可审批不同模态的序列长度下的多模态任务。出自论文：<a href="https://arxiv.org/abs/2104.09864">Roformer: Enhanced Transformer With Rotray Position Embedding</a></p><h4 id="Encoder-Block-编码器模块"><a href="#Encoder-Block-编码器模块" class="headerlink" title="Encoder Block(编码器模块)"></a>Encoder Block(编码器模块)</h4><p>用途：将源语言句子编码为一系列向量，理解和提取输入文本中的相关信息，捕获序列中各个元素的上下文信息，其输出的是输入文本的连续表示，通常称为嵌入(Embedding)。可以视为一个阅读者。<br>过程：输入带位置信息的Embedding矩阵，内部循环N次，输出key、value传递给Decoder Block.<br>结构：主要由两个子层组成。<br>Multi-Head Self-Attention(多头自注意力层):计算输入序列中每个词和其他词的相关性。<br>Feed Forward Neural Network(前馈层)：对每个词都独立非线性变换。<br>组成：</p><ol><li>Multi-Head Self-Attention(多头自注意力层)+Residual Connectionz(残差连接)+Layer Normalization(归一层)。</li><li>Feed Forward Neural Network(前馈层)+Residual Connectionz(残差连接)+Layer Normalization(归一层)。</li><li>Residual Connectionz(残差连接)&amp;Layer Normalization(相加+归一层)。</li></ol><p>Residual Connectionz(残差连接)的目的：每次循环能充分挖掘特征，用于将以前循环挖掘结果一并处理。<br>Layer Normalization的目的:按照样本跨所有特征进行标准化，即对hidden的维度去做归一化，针对单样本的不同特征做操作，对特定层的每个输入，将消除对批数据的依赖。在归一化层中，特定层中的所有神经元在给定输入的所有特征中有效地具有相同的分布。稳定神经网络学习过程，减少训练时间，提高模型最终性能。</p><h4 id="Decoder-Block-解码器模块"><a href="#Decoder-Block-解码器模块" class="headerlink" title="Decoder Block(解码器模块)"></a>Decoder Block(解码器模块)</h4><p>用途：使用向量生成目标语言的翻译，根据从编码器接收到的上下文信息，使用自注意力机制，自回归生生成与上下文相关的连贯输出序列，保证输出时序性。可类比一个口述者。<br>过程：输入带位置信息的Embedding矩阵，内部循环N次，输出信息传递给Linear(线性层).<br>结构：主要由三个子层组成。<br>Masked Multi-Head Self-Attention(遮蔽多头自注意力层)：计算输出序列中每个词和前面词的相关性（使用掩码防止未来信息泄露）<br>Cross Attention(交叉注意力)：或者也称为Encoder-Deconder Attention(编码器-解码器注意力机制)：计算输入序列和输出序列的相关性。<br>Feed Forward Neural Network(前馈层)：对每个词都独立非线性变换。</p><ol><li>Residual Connectionz(残差连接)&amp;Layer Normalization(相加+归一层)+ Masked Multi-Head Self-Attention(遮蔽多头自注意力层):<br> Masked Multi-Head Self-Attention(遮蔽多头自注意力层):确保解码器生成当前输出时，不会受到未来输出的影响。<br> 原理：对注意力分数矩阵应用一个遮蔽(一个上三角矩阵，其中未来位置的元素设置为负无穷)，在计算softmax前有效将这些位置注意力分数降到零附近，从而实现在生成序列的每一步，模型只能注意当前位置之前的词和标记。</li><li>Residual Connectionz(残差连接)&amp;Layer Normalization(相加+归一层)+Multi-Head Attention(多头注意力层):<br> Cross Attention(交叉注意力)：输入：编码器Encoder的编码信息矩阵计算得出的key、value矩阵和上一个解码器Decoder block的输出计算得到的query<br> 目的：用于融合两个不同序列或信息源的特征<br> 原理：解码器使用当前状态作为查询，与编码器的输出（key和value）进行交互，通过注意力机制确定编码器输出中的哪些部分是重要的，并据此生成下一个输出元素。</li></ol><h4 id="Feed-Forward-前馈层"><a href="#Feed-Forward-前馈层" class="headerlink" title="Feed Forward(前馈层)"></a>Feed Forward(前馈层)</h4><p>Residual Connectionz(残差连接)&amp;Layer Normalization(相加+归一层)+Feed Forward(前馈层)：</p><h4 id="Linear-线性层-Softmax-Softmax层"><a href="#Linear-线性层-Softmax-Softmax层" class="headerlink" title="Linear(线性层)&amp;Softmax(Softmax层)"></a>Linear(线性层)&amp;Softmax(Softmax层)</h4><p>Linear(线性层)&amp;Softmax(Softmax层):<br>目的：解码器输出转换为能和词汇表对应的更高维空间。<br>原理：将解密器输出矩阵映射到[100,10000]中，每个维度值代表相应单词作为序列下一个单词的未归一化分数，再利用Softmax做归一，从而得到10000个词的概率。概率越高，表示该单词成为下一个输出单词的可能性越大。获得概率词表。</p><h3 id="基于深度学习框架的Transformer模型"><a href="#基于深度学习框架的Transformer模型" class="headerlink" title="基于深度学习框架的Transformer模型"></a>基于深度学习框架的Transformer模型</h3><h4 id="基于PyTorch的Transformer模型"><a href="#基于PyTorch的Transformer模型" class="headerlink" title="基于PyTorch的Transformer模型"></a>基于PyTorch的Transformer模型</h4><h5 id="必要库和模块"><a href="#必要库和模块" class="headerlink" title="必要库和模块"></a>必要库和模块</h5><p>库和模块名：torch&#x2F;torch.nn&#x2F;torch.optim&#x2F;math&#x2F;copy<br>用途：构建Transformer模型架构、管理数据和训练过程。</p><h5 id="模型基本模块"><a href="#模型基本模块" class="headerlink" title="模型基本模块"></a>模型基本模块</h5><p>组成：位置编码、多头注意力、前馈神经网络</p><h6 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h6><p>用途：注入输入序列中词嵌入矩阵的位置信息。<br>构建方法：正弦和余弦函数，代码如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line">import torch as torch</span><br><span class="line">import math as math</span><br><span class="line">class PositionalEncoding(nn.Module):</span><br><span class="line">    def __init__(self, d_model, max_seq_length):</span><br><span class="line">        super(PositionalEncoding, self).__init__()</span><br><span class="line">        # 用零矩阵初始化位置矩阵</span><br><span class="line">        pe = torch.zeros(max_seq_length, d_model)</span><br><span class="line">        # 生成从0到max_seq_length-1的浮点数序列，并转换为二维张量</span><br><span class="line">        position = torch.arange(0, max_seq_length, dtype = torch.float).unsqueeze(1)</span><br><span class="line">        # 生成频率项，math.log(10000.0) / d_model表示固定常数，用于控制频率项衰减速度</span><br><span class="line">        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))</span><br><span class="line">        # 按照正弦公式，在偶数位置使用正弦函数</span><br><span class="line">        pe[:, 0::2] = torch.sin(position * div_term)</span><br><span class="line">        # 按照余弦公式，在奇数位置使用余弦函数</span><br><span class="line">        pe[:, 1::2] = torch.cos(position * div_term)</span><br><span class="line">        # 讲位置编码pe注册为模型缓存区，随模型保存和加载，不参与梯度更新，位置编码一般在初始化时计算好并在训练中保持不变</span><br><span class="line">        self.register_buffer(&#x27;pe&#x27;, pe.unsqueeze(0))</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        # 用于将位置编码信息添加到词嵌入矩阵中</span><br><span class="line">        return x + self.pe[:, :x.size(1)]</span><br><span class="line"></span><br></pre></td></tr></table></figure><h6 id="多头注意力"><a href="#多头注意力" class="headerlink" title="多头注意力"></a>多头注意力</h6><p>用途：计算序列中每个位置之间的关系，并捕获输入序列不同的特征和模式。<br>构建方法：$Q$是query(查询向量)，$K$是key(键向量)，$V$是value(值向量)，将他们拆分为多个注意力head, 对每个head处理注意力，并在最后将结果concat,再linear处理。<br>代码如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line">import torch as torch</span><br><span class="line">import math as math</span><br><span class="line"></span><br><span class="line">class MultiHeadAttention(nn.Module):</span><br><span class="line">    def __init__(self, d_model, num_heads):</span><br><span class="line">        super(MultiHeadAttention, self).__init__()</span><br><span class="line">        # d_model是词嵌入向量的维度，必须确保d_model能被num_heads整除</span><br><span class="line">        assert d_model % num_heads == 0</span><br><span class="line"></span><br><span class="line">        # 设置基本参数 d_model为模型维度，num_heads为注意力head数， d_k为每个head的维度</span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        self.d_k = d_model // num_heads</span><br><span class="line"></span><br><span class="line">        # 定义线性变换层，设置初始化的查询q/键k/值v和输出o的线性变换</span><br><span class="line">        self.W_q = nn.Linear(d_model, d_model)</span><br><span class="line">        self.W_k = nn.Linear(d_model, d_model)</span><br><span class="line">        self.W_v = nn.Linear(d_model, d_model)</span><br><span class="line">        self.W_o = nn.Linear(d_model, d_model)</span><br><span class="line"></span><br><span class="line">        # 按照注意力公式，计算缩放后的点积注意力</span><br><span class="line">    def scaled_dot_product_attention(self, Q, K, V, mask = None):</span><br><span class="line">        # 计算注意力分数, math.sqrt(self.d_k)用于缩放</span><br><span class="line">        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)</span><br><span class="line">        # 应用掩码，这是用于解码器中的掩码多头注意力机制中</span><br><span class="line">        if mask is not None:</span><br><span class="line">            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)</span><br><span class="line">        # 按照注意力公式，使用softmax做归一化处理，计算注意力权重</span><br><span class="line">        attn_probs = torch.softmax(attn_scores, dim=-1)</span><br><span class="line">        # 按照注意力公式，将注意力权重和值向量做加权求和</span><br><span class="line">        output = torch.matmul(attn_probs, V)</span><br><span class="line"></span><br><span class="line">        # 将输入的词嵌入矩阵分割为多head</span><br><span class="line">    def split_heads(self, x):</span><br><span class="line">        batch_size, seq_lengh, d_model = x.size()</span><br><span class="line">        return x.view(batch_size, seq_lengh, self.num_heads, self.d_k).transpose(1, 2)</span><br><span class="line"></span><br><span class="line">        # 将输出多head合并为原始形状</span><br><span class="line">    def combine_heads(self, x):</span><br><span class="line">        batch_size, _, seq_legth, d_k = x.size()</span><br><span class="line">        # contiguous确保张量在内存中连续，view重塑张量外形</span><br><span class="line">        return x.transpose(1, 2).contiguous().view(batch_size, seq_legth, self.d_model)</span><br><span class="line"></span><br><span class="line">        # 前向传播</span><br><span class="line">    def forward(self, Q, K, V, mask = None):</span><br><span class="line">        # 对Q/K/V做线性变换后，再按照多head注意力机制分割为多head</span><br><span class="line">        Q = self.split_heads(self.W_q(Q))</span><br><span class="line">        K = self.split_heads(self.W_k(K))</span><br><span class="line">        V = self.split_heads(self.W_v(V))</span><br><span class="line"></span><br><span class="line">        # 编码器入口，对Q/K/V计算多头注意力，根据mask情况，计算掩码后的多头注意力机制</span><br><span class="line">        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)</span><br><span class="line"></span><br><span class="line">        # 更加公式concat合并多头并线性输出</span><br><span class="line">        output = self.W_o(self.combine_heads(attn_output))</span><br><span class="line">        return output</span><br></pre></td></tr></table></figure><h6 id="前馈神经网络-Feed-Forward"><a href="#前馈神经网络-Feed-Forward" class="headerlink" title="前馈神经网络(Feed Forward)"></a>前馈神经网络(Feed Forward)</h6><p>用途：对每个位置的特征做非线性变换。<br>构建方法：共三层，第一层为线性变换，用于将输入特征升维，第二层为非线性激活函数，用于捕获更复杂的特征关系，常用ReLU函数，第三层为线性变换，用于降维到原始维度。<br>代码如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line"></span><br><span class="line">class PositionwiseFeedForward(nn.Module):</span><br><span class="line">    def __init__(self, d_model, d_ff):</span><br><span class="line">        super(PositionwiseFeedForward, self).__init__()</span><br><span class="line">        # 设置第一层全连接层</span><br><span class="line">        self.fc1 = nn.Linear(d_model, d_ff)</span><br><span class="line">        # 设置第二层非线性激活函数层</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">        # 设置第三层层全连接层</span><br><span class="line">        self.fc2 = nn.Linear(d_ff, d_model)</span><br><span class="line">        </span><br><span class="line">    def forward(self, x):</span><br><span class="line">        # FNN网络计算</span><br><span class="line">        return self.fc2(self.relu(self.fc1(x)))</span><br></pre></td></tr></table></figure><h5 id="模型构成模块"><a href="#模型构成模块" class="headerlink" title="模型构成模块"></a>模型构成模块</h5><p>组成：编码器(Encoder)和解码器(Decoder)</p><h6 id="编码器-Encoder"><a href="#编码器-Encoder" class="headerlink" title="编码器(Encoder)"></a>编码器(Encoder)</h6><p>构成：多头注意力机制、前馈神经网络、残差连接和归一化<br>代码如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">class Encoderlayer(nn.Module):</span><br><span class="line">    def __init__(self, d_model, d_ff, num_heads, d_ff, dropout):</span><br><span class="line">        super(Encoderlayer, self).__init__()</span><br><span class="line">        # 设置多头注意力机制</span><br><span class="line">        self.self_attn = MultiHeadAttention(d_model, num_heads)</span><br><span class="line">        # 设置前馈神经网络</span><br><span class="line">        self.feed_forward = PositionwiseFeedForward(d_model, d_ff)</span><br><span class="line">        # 设置归一化层</span><br><span class="line">        self.norm1 = nn.LayerNorm(d_model)</span><br><span class="line">        self.norm2 = nn.LayerNorm(d_model)</span><br><span class="line">        # 设置Dropout正则化，防止过拟合</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    def forward(self, x, mask = None):</span><br><span class="line">        # 根据输入序列，运行多头注意力机制</span><br><span class="line">        attn_output = self.self_attn(x, x, x, mask)</span><br><span class="line">        # 先做利用dropout做残差连接，再做归一化。</span><br><span class="line">        x = self.norm1(x + self.dropout(attn_output))</span><br><span class="line">        # 运行前馈神经网络</span><br><span class="line">        ff_output = self.feed_forward(x)</span><br><span class="line">        # 先做利用dropout做残差连接，再做归一化。</span><br><span class="line">        x = self.norm2(x + self.dropout(ff_output))</span><br><span class="line">        return</span><br></pre></td></tr></table></figure><h6 id="解码器-Decoder"><a href="#解码器-Decoder" class="headerlink" title="解码器(Decoder)"></a>解码器(Decoder)</h6><p>构成：掩码多头注意力机制、交叉注意力机制、前馈神经网络、残差连接和归一化<br>代码如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">class Decoderlayer(nn.Module):</span><br><span class="line">    def __init__(self, d_model, num_heads, d_ff, dropout):</span><br><span class="line">        super(Decoderlayer, self).__init__()</span><br><span class="line">        # 设置多头注意力机制</span><br><span class="line">        self.self_attn = MultiHeadAttention(d_model, num_heads)</span><br><span class="line">        # 设置交叉注意力机制</span><br><span class="line">        self.cross_attn = MultiHeadAttention(d_model, num_heads)</span><br><span class="line">        # 设置前馈神经网络</span><br><span class="line">        self.feed_forward = PositionwiseFeedForward(d_model, d_ff)</span><br><span class="line">        # 设置归一化层</span><br><span class="line">        self.norm1 = nn.LayerNorm(d_model)</span><br><span class="line">        self.norm2 = nn.LayerNorm(d_model)</span><br><span class="line">        self.norm3 = nn.LayerNorm(d_model)</span><br><span class="line">        # 设置Dropout正则化，防止过拟合</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    def forward(self, x, enc_output, src_mask, tgt_mask):</span><br><span class="line">        # 根据输入序列，运行掩码多头注意力机制</span><br><span class="line">        attn_output = self.self_attn(x, x, x, tgt_mask)</span><br><span class="line">        # 先做利用dropout做残差连接，再做归一化。</span><br><span class="line">        x = self.norm1(x + self.dropout(attn_output))</span><br><span class="line">        # 获取编码器的输出K/V，和解码器的q，计算交叉注意力机制</span><br><span class="line">        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)</span><br><span class="line">        # 先做利用dropout做残差连接，再做归一化。</span><br><span class="line">        x = self.norm2(x + self.dropout(attn_output))</span><br><span class="line">        # 运行前馈神经网络</span><br><span class="line">        ff_output = self.feed_forward(x)</span><br><span class="line">        # 先做利用dropout做残差连接，再做归一化。</span><br><span class="line">        x = self.norm3(x + self.dropout(ff_output))</span><br><span class="line">        return x</span><br></pre></td></tr></table></figure><h5 id="完整Transformer模型代码如下："><a href="#完整Transformer模型代码如下：" class="headerlink" title="完整Transformer模型代码如下："></a>完整Transformer模型代码如下：</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">其中模型初始化参数含义如下：</span><br><span class="line">src_vocab_size,  # 源语言词汇表大小（如英文单词数）</span><br><span class="line">tgt_vocab_size,  # 目标语言词汇表大小（如中文单词数）</span><br><span class="line">d_model = 512,  # 模型维度（每个词向量的长度）</span><br><span class="line">num_heads = 8,  # 多头注意力的头数</span><br><span class="line">num_layers = 6,  # 编码器/解码器的堆叠层数</span><br><span class="line">d_ff = 2048,  # 前馈网络隐藏层维度</span><br><span class="line">max_seq_length = 100,  # 最大序列长度（用于位置编码）</span><br><span class="line">dropout = 0.1  # Dropout概率</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">class Transformer(nn.Module):</span><br><span class="line">    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):</span><br><span class="line">        super(Transformer, self).__init__()</span><br><span class="line">        # 将来源语句词汇映射到d_model维空间的嵌入层，构成来源语句序列编码器词嵌入矩阵</span><br><span class="line">        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)</span><br><span class="line">        # 将目标语句词汇映射到d_model维空间的嵌入层，构成目标语句序列解码器词嵌入矩阵</span><br><span class="line">        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)</span><br><span class="line">        # 设置初始化位置编码矩阵</span><br><span class="line">        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)</span><br><span class="line"></span><br><span class="line">        # 根据num_layers编码器/解码器的堆叠层数，每层作为编码器/解码器对象，构建编码器/解码器列表</span><br><span class="line">        self.encoder_layers = nn.ModuleList(</span><br><span class="line">            [EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])</span><br><span class="line">        self.decoder_layers = nn.ModuleList(</span><br><span class="line">            [DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])</span><br><span class="line"></span><br><span class="line">        # 初始化最后的全连接层，将解码器的输出转换为目标词汇表大小的维度</span><br><span class="line">        self.fc = nn.Linear(d_model, tgt_vocab_size)</span><br><span class="line">        # Dropout</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    # 生成掩码函数，用于屏蔽无效位置和未来信息</span><br><span class="line">    def generate_mask(self, src, tgt):</span><br><span class="line">        # 源掩码：屏蔽填充符（假设填充符索引为0）</span><br><span class="line">        # 形状：(batch_size, 1, 1, seq_length)</span><br><span class="line">        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)</span><br><span class="line"></span><br><span class="line">        # 目标掩码：屏蔽填充符和未来信息</span><br><span class="line">        # 形状：(batch_size, 1, seq_length, 1)</span><br><span class="line">        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)</span><br><span class="line">        seq_length = tgt.size(1)</span><br><span class="line">        # 生成上三角矩阵掩码，防止解码时看到未来信息</span><br><span class="line">        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()</span><br><span class="line">        # 合并填充掩码和未来信息掩码</span><br><span class="line">        tgt_mask = tgt_mask &amp; nopeak_mask</span><br><span class="line">        return src_mask, tgt_mask</span><br><span class="line"></span><br><span class="line">        # 主要执行逻辑，前向传播，依次通过编码、解码器，最后通过全连接层输出</span><br><span class="line">    def forward(self, src, tgt):</span><br><span class="line">        # 生成源掩码和目标掩码</span><br><span class="line">        src_mask, tgt_mask = self.generate_mask(src, tgt)</span><br><span class="line"></span><br><span class="line">        # 编码器部分，先做词嵌入、再做位置编码，最后通过enc_layer做编码层处理</span><br><span class="line">        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))</span><br><span class="line">        enc_output = src_embedded</span><br><span class="line">        for enc_layer in self.encoder_layers:</span><br><span class="line">            enc_output = enc_layer(enc_output, src_mask)</span><br><span class="line"></span><br><span class="line">        # 解码器部分，先做词嵌入、再做位置编码，最后通过dec_layer做解码层处理</span><br><span class="line">        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))</span><br><span class="line">        dec_output = tgt_embedded</span><br><span class="line">        for dec_layer in self.decoder_layers:</span><br><span class="line">            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)</span><br><span class="line"></span><br><span class="line">        # 全连接层将解码器的输出转换为目标词汇表大小的维度，得到最终的输出</span><br><span class="line">        output = self.fc(dec_output)</span><br><span class="line">        return output</span><br><span class="line"></span><br></pre></td></tr></table></figure><h6 id="不同深度学习框架下的Transformer模块"><a href="#不同深度学习框架下的Transformer模块" class="headerlink" title="不同深度学习框架下的Transformer模块"></a>不同深度学习框架下的Transformer模块</h6><table><thead><tr><th>深度学习框架</th><th>函数</th><th>支持平台</th></tr></thead><tbody><tr><td>Pytorch</td><td><a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Transformer.html">torch.nn.Transformer()</a></td><td>CPU&#x2F;GPU&#x2F;TPU&#x2F;Ascend&#x2F;AppleMetal</td></tr><tr><td>MindSpore</td><td><a href="https://www.mindspore.cn/docs/zh-CN/r2.7.0/api_python/nn/mindspore.nn.Transformer.html">mindspore.nn.Transformer()</a></td><td>GPU&#x2F;CPU&#x2F;Ascend</td></tr><tr><td>Paddle</td><td><a href="https://www.paddlepaddle.org.cn/documentation/api/paddle/nn/Transformer_cn.html#paddle.nn.Transformer">paddle.nn.Transformer()</a></td><td>CPU&#x2F;GPU&#x2F;A加速芯片(华为&#x2F;海光&#x2F;昆仑芯&#x2F;寒武纪等)</td></tr></tbody></table><p>可以进一步研究不同的深度学习框架对Transformer架构的实现方法之间的差别，和同一个深度学习框架在不同硬件平台上的实现差别和工作效率。</p><h3 id="大语言模型分类-Transformer变体"><a href="#大语言模型分类-Transformer变体" class="headerlink" title="大语言模型分类(Transformer变体)"></a>大语言模型分类(Transformer变体)</h3><p>影响模型参数量的影响因素：Transformer层数、隐层维度、词向量维度、词典大小、输入token数<br>按照编码器和解密器分类：</p><p><strong>只有编码器(encoder-only)</strong>：自编码模型(Auto-encoder model),即先通过某种方式破坏句子，希望模型将被破坏的部分还原。需要从输入序列中提取有意义的上下文信息。<br>BERT(Bidirectional Encoder Representation from Transformers)：训练时先基于大量未标注的语料库进行自监督学习（预训练），然后基于标注良好的数据集进行微调，其核心在自监督训练策略。包括Mask LM(掩码语言模型，Mask Language Model,随机将输入序列中的一些词汇遮挡或者随机替换为其他词，让模型预测被遮挡的词)和NSP(随机从语料库中抽取两个句子判断是否连续)。是双向模型，训练时需要利用上下文信息，各位复杂。BERT是自编码模型，需要经过预训练和微调两个阶段学习文本表示，只能完成特定的NLP任务。</p><p><strong>只有解码器(decoder-only)</strong>：自回归模型(Auto-regressive model),给出上文，预测下文。允许编码器聚焦于输入序列的相关部分，同时生成输出序列的每个词。<br>GPT(Generative Pretained Transformer):也使用Pre-training + Fine-tuning模式.是单向模型，不需要利用上下文信息，只能利用上文。GPT是基于自回归的模型，能够进行文本生成，可以利用Prompt应用到NLP任务中。</p><p><strong>编码解码器混合(encoder decoder hybrid)</strong>：理解输入的内容NLU,又能处理并生成内容NLG，特别是处理输入和输出序列之间存在复杂映射关系的任务，以及捕捉两个序列中元素之间关系至关重要的任务。<br>T5(Tranfer Text-to-Text Transformer):所有NLP任务都可以转化为Text-to-Text任务。</p><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><p>[1] <a href="https://arxiv.org/abs/1706.03762">Attention is All You Need</a><br>[2] <a href="https://www.bilibili.com/video/BV1pu411o7BE/?spm_id_from=333.1391.0.0&vd_source=0a909d15c459f7c0fd724c236e540d79">Transformer论文逐段精读【论文精读】</a><br>[3] <a href="https://www.runoob.com/pytorch/Transformer-model.html">菜鸟教程</a><br>[4]<a href="https://github.com/tensorflow/tensor2tensor">Tensor2Tensor</a></p>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
            <tag> 算法 </tag>
            
            <tag> 神经网络 </tag>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AI应用实践-智能BI(智能问数与智能数据分析)</title>
      <link href="/2025/09/18/AI%E5%BA%94%E7%94%A8%E5%AE%9E%E8%B7%B5-%E6%99%BA%E8%83%BDBI(%E6%99%BA%E8%83%BD%E9%97%AE%E6%95%B0%E4%B8%8E%E6%99%BA%E8%83%BD%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90)/"/>
      <url>/2025/09/18/AI%E5%BA%94%E7%94%A8%E5%AE%9E%E8%B7%B5-%E6%99%BA%E8%83%BDBI(%E6%99%BA%E8%83%BD%E9%97%AE%E6%95%B0%E4%B8%8E%E6%99%BA%E8%83%BD%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90)/</url>
      
        <content type="html"><![CDATA[<h1 id="AI应用实践-智能BI-智能问数与智能数据分析"><a href="#AI应用实践-智能BI-智能问数与智能数据分析" class="headerlink" title="AI应用实践-智能BI(智能问数与智能数据分析)"></a>AI应用实践-智能BI(智能问数与智能数据分析)</h1><h2 id="商业智能-BI"><a href="#商业智能-BI" class="headerlink" title="商业智能(BI)"></a>商业智能(BI)</h2><p><a href="https://www.ibm.com/cn-zh/topics/business-intelligence">商业智能(BI)</a><br>　　定义：根据IBM的定义，商业智能(BI)是一套用于收集、管理和分析组织数据以生成为业务战略和运营提供洞察信息的技术流程。<br>　　商业智能与业务分析的关系：商业智能 (BI) 属于描述性信息，它可基于当前业务数据基础来实现更明智的业务决策。因此，业务分析 (BA) 是 BI 的一个子集，它可提供规范性和前瞻性的分析。它是 BI 基础设施的总括，其中包括用于识别和存储决策所需数据的工具。</p><h2 id="BI工具历史"><a href="#BI工具历史" class="headerlink" title="BI工具历史"></a>BI工具历史</h2><p>历史：固定报表BI-&gt;自助式BI-&gt;智能BI<br><strong>固定报表式BI</strong><br>　　定义：通过手工编写复杂的SQL代码，固定代码定期调度查询执行，查询数据呈现作为固定报表使用。<br>　　典型使用对象：数据工程师、数据开发者。<br>　　数据刷新频率：每月、每周。<br>　　主要交互方式：即席查询。<br>　　技术发展背景：信息化。<br>　　典型工具：国内有金蝶、用友，国外有Oracle、SAP、Hyperion(海波龙)、IBM Cognos Analytisc早期等。<br>　　主要问题：按需要开发，定制化交付，对代码编写要求比较高，编写复杂而且由于代码固定修改代价比较高，对开发设计人员的要求比较高，响应周期长、交互体验差、适应性比较差。</p><p><strong>自助分析式BI</strong><br>　　定义：通过“拖、拉、拽”等方式拼接生成SQL语句，再针对拼接后SQL语句调测验证后，调度查询数据形成前台可视化数据或报表呈现。<br>　　典型使用对象：业务数据分析师、数据开发者。<br>　　数据刷新频率：每天、每小时。<br>　　主要交互方式：拖拉拽控件形成可视化图表。<br>　　技术发展背景：数字化。<br>　　典型工具：国内有<a href="https://www.finebi.com/">帆软BI</a>、<a href="https://www.yonghongtech.com/bd/">永洪BI</a>，国外有商业软件<a href="https://www.microsoft.com/zh-cn/power-platform/products/power-bi">PowerBI</a>、<a href="https://www.tableau.com/zh-cn">Tableau</a>、<a href="https://www.qlik.com/us/products/qlikview">QlikView</a>、<a href="https://www.thoughtspot.com/">ThoughtSpot</a>，开源软件<a href="https://superset.org.cn/docs/intro/">Apache Superset</a>、<a href="https://metabase.net.cn/docs/latest/">Metabase</a>等。<br>　　主要问题：操作繁琐复杂、需要明确各种数据表之间的关系，自助化交付、还需要一定的SQL编写和验证能力，导致操作效率比较低，操作门槛高的问题。</p><p><strong>关于商业智能 (BI)的观点</strong><br>　　Traditional BI Can’t Keep Up.Static dashboards, request backlogs, stale data—this outdated model slows decisions and stalls innovation.In a real-time world, insights should flow live into workflows so every team can act with confidence, not lag behind.(传统商业智能 (BI) 已经跟不上时代了。静态的仪表板、积压的数据请求、过时的数据——这种落后的模式拖慢了决策速度，阻碍了创新。在实时的世界里，洞察应该实时地融入工作流程中，让每个团队都能充满信心地采取行动，而不是落后于人) 　　——ThoughtSpot</p><p><strong>智能BI(智能问数与智能数据分析)</strong><br>　　定义：借助Text2SQL(NL2SQL, Natural Language to SQL)等技术，代替使用结构化查询语言SQL的图形化界面(GUI)交流数据，实现用自然语言的交互方式获取、理解并分析数据，降低技术要求、提升取数效率和节约成本、改善用户体验，进一步深度挖掘数据潜在价值。<br>　　典型使用对象：所有人（市场、销售、业务和技术人员）。<br>　　数据刷新频率：分钟级、秒级。<br>　　主要交互方式：自然语言交流。<br>　　技术发展背景：智能化。<br>　　意义：借助不断迭代升级自适应各种场景的LLM，用自然语言和数据进行多轮并且上下文关联式的交流，将会成为以后普遍的数据访问形式、数据挖掘方式和数据库管理方式，也将作为产品设计中不可或缺的关键功能设计。</p><h2 id="核心技术"><a href="#核心技术" class="headerlink" title="核心技术"></a>核心技术</h2><p>核心技术：大语言模型学习和微调 + 语义标注和解析 + SQL转换(数据库技术)<br>　　语义标注和解析(自然语言处理，NLP) ：通过用户业务数据标注，利用向量数据库和知识图谱，实现Prompt优化。<br>　　大语言模型学习和微调(大语言模型)：当语料标注数据无法满足要求时，利用构建的Text2SQL(NL2SQL)问答对标注集，对大模型进行微调。</p><h2 id="系统架构设计"><a href="#系统架构设计" class="headerlink" title="系统架构设计"></a>系统架构设计</h2><h3 id="最简架构设计"><a href="#最简架构设计" class="headerlink" title="最简架构设计"></a>最简架构设计</h3><p>用户自然语言请求-&gt;大模型处理转换SQL-&gt;数据库执行SQL-&gt;获取数据库相关数据-&gt;大模型依赖数据和用户问题解答用户。<br>大模型处理转换SQL技术难点：SQL可用性问题, 大模型需要知道需要查询哪些表和字段，知道解决用户问题需要什么样的查询语句，理解用户意图并能转换为数据库信息。</p><h3 id="最简使用过程"><a href="#最简使用过程" class="headerlink" title="最简使用过程"></a>最简使用过程</h3><p>创建模型-&gt;对话转SQL-&gt;数据查询执行-&gt;数据分析可视化</p><h3 id="复杂架构设计"><a href="#复杂架构设计" class="headerlink" title="复杂架构设计"></a>复杂架构设计</h3><p>复杂架构设计：<br>　　<strong>硬件组件</strong>：专用算力组件（英伟达GPU&#x2F;华为NPU等计算设备）、通用算力组件（x86&#x2F;Arm等设备），用途是构建系统承载软件执行的硬件设备。结合使用要求，这个组件也可以是云化组件，即可以使用一种云平台上的不同硬件设备，也可以是对接多种云计算平台。<br>　　<strong>数据组件</strong>：也可以看成是知识库的一部分。技术上由数据库软件和数据适配器组成，逻辑上对外以数据集和逻辑模型呈现。数据库软件可以是任何结构化或者非结构化的数据库软件，典型如PostgreSql&#x2F;Oracle&#x2F;Mysql等。数据适配器的南向接口可以连接并适配多种数据库系统，并实现数据库内部跨schema、跨异构同构数据库数据内容连接功能，其北向接口作为唯一的数据对外接口，屏蔽异构数据库的对外影响，统一一致获取数据。数据库数据结合数据源、账号、schema模式、表视图名等信息，实现相同业务数据的数据集管理。结合数据适配器，完成数据集内部和数据集之间的逻辑模型建立和关联(既可以用传统的数据模型关联策略，也可以使用图数据库建立表关系和多表关联路径)，数据内容在字段级或数据标准级实现语义解释（中文汉字解释或者英文解释注释）、枚举值和别名标记、指标数据建立（定义指标内容、使用的字段和指标计算公式）。<br>　　<strong>大模型组件</strong>：大模型组件由大模型和大模型适配器组成，其和数据库组件类似，大模型可以是任何开源或者闭源的大模型软件，典型如deepseek&#x2F;OpenAI&#x2F;Qwen等,大模型适配器的南向接口可以连接并适配多种大模型，其北向接口作为唯一的大模型对外接口，屏蔽内部异构大模型的对外影响。从长远来看，这个组件可以有更丰富的内容，可以向其中添加成熟固定的模型算法包(实现数据统计、趋势分析、异常检测等功能)、反应快速灵活以解决特定问题的小模型（语义分词、意图识别、实体关联和SQL生成）或多模态模型（prompt、grounding、语义推理），以实现对数据更丰富的处理。<br>　　<strong>通用管理组件</strong>：该组件由软件系统的最基本内容构成，以实现用户、角色和租户管理、权限控制（查询SQL显示字段检查用户是否有查看权限，权限控制粒度到字段级）、登录认证、隐私保护(加解密)、运维管理、资源管理、负载均衡、接口管理、数据集场景管理（不同数据集构成不同业务场景）、全生命周期管理等软件通用管理功能。<br>　　<strong>前台组件</strong>：用户交互组件，是用户直接使用、感知最强的IT组件，最少由输入器和输出器组成。输入器和输入器最低支持文字输入和呈现。结合通用管理组件的接口管理功能，可以灵活适配和使用已有系统的前台页面和新添加的对话式交互工具，根据用户需要实现数据SQL呈现、SQL修改、SQL二次执行、可视化呈现等功能。在对话式交互中实现多轮连续会话分析。未来这里可以实现数据导出、看板报告等内容生成和分享。<br>　　<strong>语义组件</strong>：根据尼尔森《十大可用性原则》贴近场景原则，构建提示词、常用词、近义词、同义词、通用词、专业术语、专有名词(方言)映射关系和数据库，构建语义词库，可结合业务需要个性化配置，实现日期、时间等通配符转换，适配不同格式情况，提升反馈准确性。记录高频会话中的常用信息，或对典型语句做收藏保存，实现语义信息的高效调用。这里可以使用模式关系抽取技术（schema linking），实现对schema智能筛选、自动获取查询相关的表结构和字段信息。或者使用分词技术，将一句话分解为多个词组，结合语义词库，实现对词组和逻辑模型信息的匹配，实现用户输入内容的完整意图识别。<br>　　<strong>质量安全控制组件</strong>：结合通用管理组件，完成对用户输入信息意图理解校验（从简单的讲可以是看用户输入是否符合Prompt模板，或者是其内容是否超过token数量限制）、生成SQL的语法校验（确保SQL内容符合数据库使用的高效语法）、生成SQL一致性校验（在产生多条SQL时确保生产SQL的一致性）、数据结果权限校验（判断用户是否有查询该数据的权限）、数据库安全校验等功能，还可以校验处理LLM的“幻觉”问题。<br>　　<strong>Text2SQL组件</strong>：也可以是Text2SQL引擎，实现从前台组件获取用户自然语言需求内容，调用大模型组件、数据库组件、语义组件转换为查询SQL，调用查询SQL返回SQL和数据给前台组件。其由语言解析器和SQL执行器组成：语言解析器：对文本进行解析后转换为SQL语句，包括根据问题和数据库的映射找到问题需要使用的表和对应的字段列，将识别结果和包含信息后，生成满足对应语法要求的SQL语句。SQL执行器：执行SQL语句内容。<br>　　针对Text2SQL组件，当前主要使用监督微调技术（SFT-Based）和提示词工程方案(Prompt-Engineer)。监督微调技术（SFT-Based）就是以通用大模型为基础，利用微调技术，有针对性地训练大模型在某一方面的专有能力。针对提示词工程方案(Prompt-Engineer)，实际业务场景中，通过搜集历史数据，用户90%的常用场景可以收集完整，同一类场景SQL查询基本相似，进而可以构造出公共模板。可以先通过对问题信息提取做出场景归类，再根据场景选择对应的查询提示词模板，在利用RAG和外挂知识库，动态增强NL2SQL能力。</p><h2 id="关键技术"><a href="#关键技术" class="headerlink" title="关键技术"></a>关键技术</h2><p><strong>提示词工程方案(Prompt-Engineer)</strong>:设计问题和指令，获取大模型最佳输出的过程。为了清晰地传达内容和信息，将问题结构化，利用具体和多样化的示例来帮助大模型生成准确结果，并用约束来限制大模型输出的内容范围，避免偏离和越权问题。其会涉及的提升类型有直接提示(zero-shot)、带示例提示(one-shot&#x2F;few-shot&#x2F;multi-shot)、思维链提示（CoT）、指定所需输出格式等。<br>　　<strong>提示词测试方案(Prompt-Test)</strong>:通过迭代测试的方式，编写出合适的Prompt模板。<br><strong>Prompt模板构建方法</strong>：<br>　　1. 构造测试集；根据问题和答案，挑选一定数量的典型样例，构造数据集，以方便每次修改Prompt后都可以跑通测试，确定Prompt哪里表现有问题。<br>　　2. 分析并编写问题从接收到解决全过程的关键要点：包括自身角色（任务是什么，实现什么功能）、功能介绍（任务操作哪些表和字段，目标是什么，就像教育新人要干什么，用什么，结果是什么）、使用数据集、问题描述、问题定义、样例、输出格式、字段必要信息、必要指令等，利用LLM优化构造完整的Prompt格式模板内容。<br>　　3. 测试迭代优化：利用测试集，测试Prompt模板，根据结果修改Prompt模板内容。<br><strong>Prompt模板注意点</strong>：<br>　　4. 不同大模型对Prompt的理解不一样，Prompt模板需要手动不断调整。<br>　　5. 构建模块过程中可以了解大模型的知识特点和Prompt技巧，加快Prompt模板生成。<br>　　6. 在Prompt模板中可以通过添加关键字设置，加快高频使用场景下的反应速度。<br>　　<strong>Prompt构造模板内容</strong>:使用五元组&lt;Role, Instructions, Database, Examples, Output Format&gt;构成，Role定位Prompt的身份功能任务角色,Instructions进一步解释功能，并添加限制性因素和要求，方便后期对大模型结果进行提取, Database包含数据库名称、数据表名、字段名、字段类型、主外键等数据元信息，指出查询的业务信息对应的使用哪些数据库。 Examples由问题（query）和回复参数(response)组成：问题是常用的自然语言查询问题，回复参数(response)是对应自然语言问题对应数据表、数据字段和取值信息等。Output Format定义模型返回的数据格式, 和Examples中的回复参数(response)保持一致。<br>　　<strong>Prompt典型论文</strong>：<a href="https://arxiv.org/abs/2211.01910">Large Language Models Are Human-Level Prompt Engineers</a>：2023年Yongchao Zhou等人提出,Automatic Prompt Engineer (APE)来指导大语言模型提示词的生成和选择。GitHub地址：<a href="https://github.com/keirp/automatic_prompt_engineer">https://github.com/keirp/automatic_prompt_engineer</a></p><h2 id="产品设计要点"><a href="#产品设计要点" class="headerlink" title="产品设计要点"></a>产品设计要点</h2><ol><li>整体思路：分析清楚业务问题是什么，归总人工工作流程，针对流程环节情况，确定哪些地方可以做大模型替代。可以先选择难度低的问题，把流程跑通后，再向高难度迭代，核心不是技术的高级与否，核心在解决用户的问题。</li><li>系统中如果希望实现模型训练或者微调，如果不是已经现成的模型，最好是不添加这些功能。理由：大模型的训练和微调目前来看仍然是难度比较高的活动，模型训练一般耗时比较长、微调的方式不同呈现的效果也不一样，对用户的能力要求比较高，从用户体验角度看，这个功能不一定能给用户带来特别大的使用体验的提升，可能还有点费力不讨好的风险。个性化定制化的设计一定要慎重谨慎。</li><li>优先聚集在高频、通用作业数据使用场景，建立样板点或样板间，提升大模型在用户侧的体验感知。可以建立典型问题集，通过和用户沟通，获取典型高频的问题，构建用户问题+后台SQL操作映射表，根据对映射表的分析，初步理解用户的使用特点、难点所在。</li><li>数据质量是大模型性能和结果可靠性的前置条件。分析用户使用数据过程，根据用户问题和常用数据，确定所使用的数据结构形式和内容，理解数据库表和字段及其背后的业务语言和逻辑，理解业务问题、使用表和字段信息、真实回复、查询SQL内容、SQL检索方式。</li><li>通用管理组件中的运维管理功能，要实现对系统的持续监控。监控结果、用户反馈和用户需求一起考虑，促进系统的迭代优化和高效执行。</li></ol><h2 id="设计实施过程"><a href="#设计实施过程" class="headerlink" title="设计实施过程"></a>设计实施过程</h2><p>从实现的功能进化角度，可以先实现快速找数，快速掌握数据全貌，其次通过技术实现数据的复杂组合分析，最后探索根据提供的分析组合叠加数据的关联关系，进一步辅助分析决策。</p><h2 id="优秀实践产品"><a href="#优秀实践产品" class="headerlink" title="优秀实践产品"></a>优秀实践产品</h2><table><thead><tr><th align="left">产品</th><th align="left">介绍</th></tr></thead><tbody><tr><td align="left"><a href="https://cn.aliyun.com/product/quick-bi?from_alibabacloud=">阿里瓴羊Quick BI</a></td><td align="left">中国唯一且连续6年入选 Gartner 的 ABI 产品。大模型时代的全场景数据分析 AI+BI 产品。是阿里云推出的大模型驱动数据分析产品，深度融合 BI 与 AI Agent 能力，帮助企业快速构建数据分析系统。用户可通过自然语言与分析 Agent “智能小Q” 交互，实现自动数据洞察、报表生成和多轮深度分析，并支持拖拽式制作仪表板、电子表格与数据大屏。同时，产品支持多数据源连接、一键报告生成和多端共享，推动企业从“被动看数”迈向“主动用数”，赋能业务智能决策与高效协同。无缝对接企业系统，灵活嵌入，安全增强数据协作。自然语言开启多维问数，多步取数，挖掘深层洞见。自动生成真实深度报告，优化策略，支持二次编辑。海量数据毫秒响应，实时计算，高效支撑业务决策。</td></tr><tr><td align="left"><a href="https://help.aliyun.com/zh/model-studio/brief-introduction-of-gbi-products">阿里析言GBI产品</a></td><td align="left">基于阿里云通义大语言模型在数据分析领域专门增强的原生数据助理，通过自然语言交互实现NL2SQL、数据问答、分析、洞察等多维度的大模型智能分析应用，实现数据查询、分析与报告生成，适用于业务变化较快、数据分析时效性强的一线业务场景。</td></tr><tr><td align="left"><a href="https://www.digitforce.com/product/sa/">智能分析助手(SwiftAgent)</a></td><td align="left">数势科技开发的、基于大模型和AI Agent的企业数据分析与决策产品，可实现基于自然语言的业务数据洞察、报告总结和决策建议。功能上支持文本、语音、Excel等数据类型输入，方便客户在移动端和PC端快速完成数据查询。支持用户通过对业务数据进行维度归因、因子归因、时间序列归因与相关性分析等多种洞察方法，完成数据波动归因。支持对结果数据集进行如同环比、占比、排序和高级聚合等多种高级统计分析方式。可贴合用户问题，自动匹配生成多种分析图表。基于数据分析结果及行业微调大模型，生成贴合业务场景的行业报告。</td></tr><tr><td align="left"><a href="https://percent.cn/Product/syznxt.html">百分点商业智能系统(Clever BI)</a></td><td align="left">一款AI驱动的数据分析产品，依托自然语言处理和深度学习技术，用户可通过自然语言与系统进行可视化交互，轻松实现多源数据关联、挖掘分析，支持数据整合、分析、可视化的全流程自助式操作，大大降低数据分析的门槛，为数据应用和决策研判提供有力支撑。</td></tr><tr><td align="left"><a href="https://console.bce.baidu.com/ai_apaas/app/debugger/gbi/e5a2f884-744c-423b-ae31-c11172944ec4/123e4567-e89b-12d3-a456-426655440025">百度GBI</a></td><td align="left">一款生成式商业智能产品，具有强大的创新驱动功能。它集成了数据集成、智能分析等功能于一身，帮助企业快速发现新的商业机会和创新点。百度GBI具有支持自然语言交互、跨数据库分析和专业知识学习三方面能力。</td></tr></tbody></table><h2 id="产品实践案例"><a href="#产品实践案例" class="headerlink" title="产品实践案例"></a>产品实践案例</h2><table><thead><tr><th align="left">产品</th><th align="left">介绍</th></tr></thead><tbody><tr><td align="left"><a href="https://www.53ai.com/news/zhinenghuagaizao/2025010809175.html">喜马拉雅基于大模型ChatBl实践探索</a></td><td align="left">喜马拉雅基于大模型 ChatBI 的实践探索，文中将重点介绍关于准确性和大模型幻觉等问题的解决方案。</td></tr><tr><td align="left"><a href="https://zhuanlan.zhihu.com/p/1943744657868690237">Bilibili Data AI 探索和实践</a></td><td align="left">介绍从数据整合到决策提效：B 站数据知识库与增强分析协同的端到端实践。</td></tr><tr><td align="left"><a href="https://zhuanlan.zhihu.com/p/692442114">ChatBI：基于文心一言的生成式数据分析技术探索</a></td><td align="left">本文将深入剖析商业智能(BI)与生成式模型结合带来的业务价值和技术实践经验。重点从三个视角和大家进行了交流分享。第一，从技术趋势和业务需求视角，论证了生成式智能BI必然技术趋势和带来的巨大业务价值；第二，从系统设计视角，介绍了百度数据中台ChatBI设计思路和关键点。第三，从新技术实践实践视角，介绍了ChatBI在百度落地过程中遇到的问题和解决思路。</td></tr><tr><td align="left"><a href="https://www.53ai.com/news/zhinenghuagaizao/2024103138175.html">腾讯基于LLM的智能数据分析平台OlaChat的落地实践</a></td><td align="left">在当今快速发展的数据分析领域，智能分析平台正经历从传统 BI 到敏捷分析，再到智能分析的转变。随着移动互联网的兴起和大语言模型的出现，数据分析变得愈加普及，用户可以通过自然语言与系统进行互动，获取所需数据。然而，即使在敏捷分析阶段，仍然存在一定的学习成本。大语言模型的引入为数据分析带来了新的机遇，它不仅提升了语言理解和生成能力，还使得逻辑推理与工具使用变得更加高效。通过对用户自然语言指令的理解和转化，智能分析平台能够实现更直观的数据查询和分析过程，为用户提供更为便捷的服务。本文将分享腾讯基于 LLM 的智能数据分析平台 OlaChat 的落地实践.</td></tr><tr><td align="left"><a href="https://zhuanlan.zhihu.com/p/11603873458">腾讯在 ABI 工程领域的探索与实践</a></td><td align="left">本次分享将深入探讨腾讯在智能化 BI 系统中的技术实践，详细阐述如何通过工程架构、微调模型、引导补全、前端指令层设计等手段，提升数据分析的智能化水平，以实现更高效、更智能的商业智能分析。</td></tr><tr><td align="left"><a href="https://www.modb.pro/db/1861269331712159744">腾讯云ChatBI：基于大模型的效果调优方案探索</a></td><td align="left">本文主要探讨了腾讯云 ChatBI 如何利用大模型技术进行效果调优，以提供更高效、便捷的数据分析体验。</td></tr></tbody></table><h2 id="技术挑战"><a href="#技术挑战" class="headerlink" title="技术挑战"></a>技术挑战</h2><ol><li>数据质量和语法差异：数据集和逻辑模型的建立，依赖企业内部的数据管理和信息架构。对于交易型数据系统内数据的主外键缺失、字段注释解释缺失等数据质量问题，导致查询效率比较低，影响数据查询结果的准确性。分析型数据系统内为加快查询而不使用主外键设置，使用“粒度”作为查询和关联字段信息的媒介，也会影响逻辑模型间的数据关联，进而影响数据结果的正确反馈。不同的数据库使用不同的数据库语法，导致写出的SQL执行情况有差异。</li><li>自然语言理解的准确性：大模型本身的“幻觉”问题、自然语言的多样性、歧义性可能造成查询错误，导致数据准确性和系统可靠性下降，影响用户使用信心。</li><li>领域差异：隔行如隔山，每个领域都有自己的专业术语、黑话方言等语言特点，造成需求差异比较大，理解困难，如何结合这些语言特点去适配不同领域的查询诉求，是比较大的挑战。</li><li>实时性：业务数据仓库内的数据表和数据量都比较大，大模型上下文token也有成本和数量限制，组件之间的网络通讯IO等交互有局限，Text2SQL转换的SQL查询性能不一定是性能最优化的查询SQL，这些因素导致数据搜索只能在有限的数据范围（例如一定范围内的公开数据）和一定的时间（分钟或小时级）内反馈搜索数据结果，制约用户对在多轮会话下数据查询使用体验。</li><li>数据安全：调用外部大模型时，确保其本地或者核心数据不外泄，用户权限控制时能正确看到自己可以看到的数据，会影响数据和隐私安全的合规要求。</li><li>数据评判标准：现有的评判数据集例如Spider和Bird等，还不能完全适应真实的生产环境。多语言下的NL2SQL的性能下降明显，需要改进优化。</li><li>shema linking技术挑战：需要根据用户问题从数据库中找都需要使用的表和列，但是数据库中的表复杂而且专业性强，这给搜索带来了挑战。</li></ol><h2 id="长远设想"><a href="#长远设想" class="headerlink" title="长远设想"></a>长远设想</h2><ol><li>用户助手：结合业务自身数据和使用特点，通过对大模型的微调，构建用户个性化大模型，对接或替换大模型组件，实现业务个性化支持。</li><li>个性化记录、反馈和自适应学习：通过点赞、投币、收藏、保存等用户反馈和操作行为，结合推荐系统和日常积累，加强反馈给用户的数据准确性，主动推荐推送用户可能关心的数据和指标内容，优化查询和理解能力。记录系统在生成SQL语句过程中的推理步骤和依据, 利用可视化工具展示给用户看。</li><li>通用自然语言支持：对多国语言、多种方言、跨领域的自然语言支持，实现系统对广泛人群的使用和覆盖。</li><li>跨平台和技术组件的支持和集成：用户本地数据库、私有公有云化数据库、Office办公软件、NoSQL数据库、Hadoop大数据组件、ERP系统、本地存量软件系统等系统平台的对接和支持。</li><li>LLM反问和追问来提升用户体验：根据尼尔森《十大可用性原则》容错原则，在前台和用户交流部分，通过设置反问和追问功能，逐步精细化用户诉求，提升用户自然语言对问题适配模型和数据的精细化描述，进而提升用户体验。</li><li>引入Agent智能体：利用Agent智能体的规划设计和反思等技能，实现更复杂的任务拆分和组合、工具调用、归因总结等。</li><li>隐私保护，对schema、表字段等结构化数据库元信息进行匿名化处理，确保实现隐私保护。</li><li>系统性能扩展：考虑到Text2SQL(NL2SQL)技术需要前后处理等多个重要步骤，可以通过使用分布式架构和模型块化设计搭建整个解决方案，进一步增强系统的处理能力。</li><li>shema linking技术处理：可以使用embedding模型将表名和列名进行向量化，然后对于每个查询检索出最为相似的表名和列名，可以调用语义组件，实现表名和列表标准替换，结合Prompt+LLM,转换为查询SQL。</li><li>连接智算硬件：结合当前智算硬件设备(例如智算一体机等)，直接连接企业现成本地数据库系统，让企业用户通过智算硬件设备，直接就完成智能业务数据分析和处理。</li></ol><h2 id="个人思考"><a href="#个人思考" class="headerlink" title="个人思考"></a>个人思考</h2><p>个人思考:<br>　　Text2SQL(NL2SQL)只是一个开始，NL2DSL可以适配任何DSL专用领域的使用需求。用AI的通用能力在算力和特定领域数据的支持下，去消灭领域内的“土围子”。智能BI看起来好像只是一个问数和数据分析系统，但其实如果往深了想，这种对数据的查询和分析是否可以成为日后通用统一查询流量入口或者搜索查询入口，完全可以淘汰传统的企业内部简单的查询分析系统。如果在此基础上进一步配合MCP协议、Agent智能体，成长为企业内部的数据咨询专家，也是可以进一步期待的。从产品设计的角度考虑，这个系统产品未来是很有潜力的。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] A. Popescu, A. Armanasu, O. Etzioni, D. Ko, and A. Yates. Modern natural language interfaces to databases: Composing statistical parsing with semantic tractability. In COLING, 2004.<br>[2] F. Li and H. V. Jagadish. Constructing an interactive natural language interface for relational databases. PVLDB, 8(1):73–84, 2014.<br>[3] D. Saha, A. Floratou, K. Sankaranarayanan, U. F. Minhas, A. R. Mittal, and F. O¨ zcan. ATHENA: an ontology-driven system for natural language querying over relational data stores. PVLDB, 9(12):1209–1220, 2016.<br>[4] N. Yaghmazadeh, Y. Wang, I. Dillig, and T. Dillig. Sqlizer: query synthesis from natural language. PACMPL, 1(OOPSLA):63:1–63:26, 2017.<br>[5] P. Pasupat and P. Liang. Compositional semantic parsing on semi-structured tables. In ACL, pages 1470–1480, 2015.<br>[6] C. Baik, H. V. Jagadish, and Y. Li. Bridging the semantic gap with SQL query logs in natural language interfaces to databases. In ICDE, pages 374–385, 2019.<br>[7] S. Iyer, I. Konstas, A. Cheung, J. Krishnamurthy, and L. Zettlemoyer. Learning a neural semantic parser from user feedback. In ACL, pages 963–973, 2017.<br>[8] F. Basik, B. H¨attasch, A. Ilkhechi, A. Usta, S. Ramaswamy, P. Utama, N. Weir, C. Binnig, and U. C¸ etintemel. Dbpal: A learned nl-interface for databases. In SIGMOD, pages 1765–1768, 2018.<br>[9] V. Zhong, C. Xiong, and R. Socher. Seq2sql: Generating structured queries from natural language using reinforcement learning. CoRR, abs&#x2F;1709.00103, 2017.<br>[10] X. Xu, C. Liu, and D. Song. Sqlnet: Generating structured queries from natural language without reinforcement learning. CoRR, abs&#x2F;1711.04436, 2017.<br>[11] M. Lapata and L. Dong. Coarse-to-fine decoding for neural semantic parsing. In ACL, pages 731–742, 2018.<br>[12] S. Yavuz, I. Gur, Y. Su, and X. Yan. Dialsql: Dialogue based structured query generation. In ACL, pages 1339–1349, 2018.<br>[13] P. Huang, C. Wang, R. Singh, W. Yih, and X. He. Natural language to structured query generation via meta-learning. In NAACL-HLT, pages 732–738, 2018.<br>[14] M. Zhou, G. Cao, T. Liu, N. Duan, D. Tang, B. Qin, X. Feng, J. Ji, and Y. Sun. Semantic parsing with syntax- and table-aware SQL generation. In ACL, pages 361–372, 2018.<br>[15] T. Yu, Z. Li, Z. Zhang, R. Zhang, and D. R. Radev. Typesql: Knowledge-based type-aware neural text-to-sql generation. In NAACL-HLT, pages 588–594, 2018.<br>[16] C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In ICML, pages 1126–1135, 2017.<br>[17] T. Yu, R. Zhang, K. Yang, M. Yasunaga, D. Wang, Z. Li, J. Ma, I. Li, Q. Yao, S. Roman, Z. Zhang, and D. R. Radev. Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task. In EMNLP, pages 3911–3921, 2018.<br>[18] T. Yu, M. Yasunaga, K. Yang, R. Zhang, D. Wang, Z. Li, and D. R. Radev. Syntaxsqlnet: Syntax tree networks for complex and cross-domain text-to-sql task. In EMNLP, pages 1653–1663, 2018.<br>[19] B. Bogin, J. Berant, and M. Gardner. Representing schema structure with graph neural networks for text-to-sql parsing. In ACL, pages 4560–4565, 2019.<br>[20] J. Guo, Z. Zhan, Y. Gao, Y. Xiao, J. Lou, T. Liu, and D. Zhang. Towards complex text-to-sql in cross-domain database with intermediate representation. In ACL, pages 4524–4535, 2019.<br>[21] Pasupat P, Liang P. Compositional semantic parsing on semi-structured tables[J]. arXiv preprint arXiv:1508.00305, 2015.<br>[22] Hemphill C T, Godfrey J J, Doddington G R. The ATIS spoken language systems pilot corpus[C]&#x2F;&#x2F;Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley, Pennsylvania, June 24-27, 1990. 1990.<br>[23] Sun N, Yang X, Liu Y. Tableqa: a large-scale chinese text-to-sql dataset for table-aware sql generation[J]. arXiv preprint arXiv:2006.06434, 2020.<br>[24] Min Q, Shi Y, Zhang Y. A pilot study for chinese sql semantic parsing[J]. arXiv preprint arXiv:1909.13293, 2019.<br>[25] Wang L, Zhang A, Wu K, et al. ChiTeSQL: A Large-Scale and Pragmatic Chinese Text-to-SQL Dataset[C]&#x2F;&#x2F;Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020: 6923-6935.<br>[26] Bogin B, Gardner M, Berant J. Global reasoning over database structures for text-to-sql parsing[J]. arXiv preprint arXiv:1908.11214, 2019.<br>[27] 刘译璟, 徐林杰, 代其锋. 基于自然语言处理和深度学习的 NL2SQL 技术及其在 BI 增强分析中的应用[J]. 中国信息化, 2019, 11.<br>[28] <a href="https://zhuanlan.zhihu.com/p/1920428002006795984">25年不可错过的大模型应用方向-Text2SQL: 企业提效50%以上</a><br>[29] <a href="https://zhuanlan.zhihu.com/p/269478469">语义解析 (Text-to-SQL) 技术研究及应用 下篇</a><br>[30] <a href="https://originalstatic.aminer.cn/misc/billboard/CAIIAM/%E5%88%98%E8%AF%91%E7%92%9F-%E5%9F%BA%E4%BA%8E%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84NL2SQL%E6%8A%80%E6%9C%AF%E5%8F%8A%E5%85%B6%E5%9C%A8BI%E5%A2%9E%E5%BC%BA%E5%88%86%E6%9E%90%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8.pdf">基于自然语言处理和深度学习的NL2SQL技术及其在BI增强分析中的应用</a><br>[31] <a href="https://www.tellius.com/resources/blog/is-a-semantic-layer-necessary-for-enterprise-grade-ai-agents">Is a Semantic Layer Necessary for Enterprise-Grade AI Agents?</a><br>[32] <a href="https://arxiv.org/pdf/2411.08599">A PREVIEW OF XIYAN-SQL: A MULTI-GENERATOR ENSEMBLE FRAMEWORK FOR TEXT-TO-SQL</a><br>[33] <a href="https://arxiv.org/abs/2410.01943">CHASE-SQL: Multi-Path Reasoning and Preference Optimized Candidate Selection in Text-to-SQL</a><br>[34] <a href="https://hub.baai.ac.cn/papers">智源社区</a><br>[35] <a href="https://www.datafocus.ai/category/BI%203.0?page=2">ChatBI</a><br>[36] <a href="https://blog.csdn.net/sinat_39620217/article/details/137603846">NL2SQL基础系列(1)：业界顶尖排行榜、权威测评数据集及LLM大模型（Spider vs BIRD）全面对比优劣分析[Text2SQL、Text2DSL]</a><br>[37] <a href="https://github.com/eosphoros-ai/Awesome-Text2SQL/blob/main/README.zh.md">Awesome Text2SQL</a><br>[38] <a href="https://github.com/yechens/NL2SQL">Text2SQL 语义解析数据集、解决方案、paper资源整合项目</a><br>[39] <a href="https://mp.weixin.qq.com/s/FtsA4O_VTUqhhYS3Gq3G8Q">语义解析 (Text-to-SQL) 技术研究及应用</a><br>[40] <a href="https://www.shaqiu.cn/article/AGlZV58ELBJm">沙丘智库《2025年“大模型+数据分析”最佳实践报告》正式发布</a></p>]]></content>
      
      
      <categories>
          
          <category> 应用 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
            <tag> 算法 </tag>
            
            <tag> 应用 </tag>
            
            <tag> BI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AI应用实践-Text2SQL(NL2SQL, Natural Language to SQL)</title>
      <link href="/2025/09/17/AI%E5%BA%94%E7%94%A8%E5%AE%9E%E8%B7%B5-Text2SQL(NL2SQL,%20Natural%20Language%20to%20SQL)/"/>
      <url>/2025/09/17/AI%E5%BA%94%E7%94%A8%E5%AE%9E%E8%B7%B5-Text2SQL(NL2SQL,%20Natural%20Language%20to%20SQL)/</url>
      
        <content type="html"><![CDATA[<h1 id="AI应用实践-Text2SQL-NL2SQL-Natural-Language-to-SQL"><a href="#AI应用实践-Text2SQL-NL2SQL-Natural-Language-to-SQL" class="headerlink" title="AI应用实践-Text2SQL(NL2SQL, Natural Language to SQL)"></a>AI应用实践-Text2SQL(NL2SQL, Natural Language to SQL)</h1><p>Text2SQL，学术领域常称NL2SQL，即将输入的自然语言查询自动转换为数据库可执行的SQL查询语句的技术，是基于LLM提升数据分析效率的智能查询技术。</p><h2 id="Text2SQL-NL2SQL-技术发展历史"><a href="#Text2SQL-NL2SQL-技术发展历史" class="headerlink" title="Text2SQL(NL2SQL)技术发展历史"></a>Text2SQL(NL2SQL)技术发展历史</h2><p>技术发展历史：<br>　　<strong>1973年</strong>:Woods开发LUNAR系统，用于回答月球岩石样本问题。<br>　　<strong>1978年</strong>:Hendrix设计LIFER&#x2F;LADDER自然语言接口，用于连接美国海军数据库。<br>　　<strong>本世纪初</strong>:基于规则的方法(Rule-based Methods)出现，包括<a href="https://aclanthology.org/C04-1021.pdf">PRECISE</a>、<a href="https://www.vldb.org/pvldb/vol8/p73-li.pdf">NaLIR</a>、<a href="https://www.vldb.org/pvldb/vol9/p1209-saha.pdf">ATHENA</a>、<a href="https://dl.acm.org/doi/10.1145/3133887">SQLizer</a>、<a href="https://arxiv.org/abs/1902.00031">Templar</a>。该方法依赖预定义模板和严格SQL语法规则来说生成可执行的物流SQL，缺点是无法处理并理解复杂多样的自然语言问题，泛化能力差。<br>　　<strong>2019年</strong>:随着深度学习的崛起，基于深度学习的Text2SQL(NL2SQL, Natural Language to SQL)方法(Deep Learning-based Approaches)开始流行。基于深度学习的方法包括<a href="https://arxiv.org/abs/1709.00103">基于Pointer Netword改进Seq2SQL方法</a>、<a href="https://arxiv.org/abs/1711.04436">基于Sequence-to-set的改进SQLNet方法</a>、<a href="https://aclanthology.org/P19-1444/">基于TRANX(自定向下文法生成)的改进IRNet方法</a>、<a href="https://aclanthology.org/D19-1378/">基于图的改进GlobalGNN方法</a>。方法较基于规则的方法(Rule-based Methods)更灵活，但是由于结构所限，其生成SQL的能力仍然很差，难以处理复杂的SQL操作。<br>　　<strong>2021年</strong>:基于预训练语言模型(PLM)的方案开始成为解决Text2SQL(NL2SQL, Natural Language to SQL)问题的实践方案。基于预训练语言模型的方案包括<a href="https://arxiv.org/abs/1905.08205">Towards Complex Text-to-SQL in Cross-Domain Database with Intermediate Representation</a>、<a href="https://arxiv.org/abs/2005.08314">TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data</a>.这类方案生成的SQL较基于深度学习的方案更准确，但是其严重依赖预训练的数据和模型微调，对资源要求比较高，对复杂数据库操作的处理仍然存在明显困难。<br>　　<strong>2023年</strong>:随着GPT为代表的大语言模型的流行，基于大语言模型(LLM)的Text2SQL(NL2SQL, Natural Language to SQL)解决方案，因其巨大的改进探索潜力，开始成为日益受到关注的新兴研究领域。<br>　　在Text2SQL(NL2SQL)的产品化过程中，<a href="https://www.hengshi.com/blog/1150.html">在Gartner定义的”增强分析”成熟曲线上，90%的准确率是商业应用的门槛线。</a></p><h2 id="Text2SQL-NL2SQL-技术关键环节"><a href="#Text2SQL-NL2SQL-技术关键环节" class="headerlink" title="Text2SQL(NL2SQL)技术关键环节"></a>Text2SQL(NL2SQL)技术关键环节</h2><p>技术关键环节：<br>　　<strong>预处理部分</strong>：用户使用日常自然语言和CUI(Conversation UI)界面描述分解自身数据需求，需要识别NL中的关键信息（包括关键属性、时间、限制等），以提高NL2SQL翻译的质量。<br>　　<strong>NL2SQL翻译部分</strong>：核心。需要识别数据库系统中的表、列和字段(通常使用Schema linking技术)，最后结合NL系统生成标准SQL语句。<br>　　<strong>后处理部分</strong>：需要优化生成SQL，修复SQL中的语法等问题，最后数据库执行SQL语句后返回查询结果。</p><h2 id="Text2SQL-NL2SQL-任务评测数据集"><a href="#Text2SQL-NL2SQL-任务评测数据集" class="headerlink" title="Text2SQL(NL2SQL)任务评测数据集"></a>Text2SQL(NL2SQL)任务评测数据集</h2><p>功能：为模型训练和评估确立标准，鼓励技术合作交流。</p><h3 id="英文典型数据集"><a href="#英文典型数据集" class="headerlink" title="英文典型数据集"></a>英文典型数据集</h3><table><thead><tr><th align="left">数据集名</th><th align="left">内容</th></tr></thead><tbody><tr><td align="left"><a href="https://github.com/salesforce/WikiSQL">WikiSQL</a></td><td align="left">2017年Salesforce等人提出的、数据库表26521张、问题和SQL对数80657对、单轮、多领域、英文标注NL2SQL数据集。论文：<a href="https://arxiv.org/pdf/1709.00103">https://arxiv.org/pdf/1709.00103</a></td></tr><tr><td align="left"><a href="https://yale-lily.github.io/spider">Spider 1.0</a></td><td align="left"><strong>重点数据集</strong>，2018年耶鲁大学提出的、分布在200个独立数据库的5693张表、问题和SQL对数10181对、单轮、覆盖138个不同领域、英文标注NL2SQL数据集。论文：<a href="https://arxiv.org/pdf/1809.08887">https://arxiv.org/pdf/1809.08887</a>; Github: <a href="https://github.com/taoyds/spider">https://github.com/taoyds/spider</a></td></tr><tr><td align="left"><a href="https://spider2-sql.github.io/">Spider 2.0</a></td><td align="left">2024年香港大学等团队提出文本到SQL基准测试框架，包含632个源自企业级数据库用例的真实文本转SQL工作流问题。Spider 2.0中的数据库来自真实的数据应用程序，通常包含超过1000 列，并存储在本地或云数据库系统（例如 BigQuery 和 Snowflake）中。</td></tr><tr><td align="left"><a href="https://github.com/Exploration-Lab/BookSQL">BookSQL</a></td><td align="left">2024年由印度理工学院提出的、会计领域的大规模文本到SQL数据集。数据集总规模达100万。该数据集由财务专家监督编制。数据集包含27家企业，每家企业的交易量约为3.5万至4万笔。</td></tr><tr><td align="left"><a href="https://bird-bench.github.io/">BIRD</a></td><td align="left"><strong>重点数据集</strong>。2023年由香港大学和阿里达摩院等团队提出的, 首个专为缩小文本到SQL解析的学术研究与实际应用间差距而设计的、大规模的基准测试集。BIRD 包含超过12751个独特的问题-SQL对，95 个大型数据库，总大小达33.4GB。它还涵盖了37个专业领域，例如区块链、曲棍球、医疗保健和教育等。论文：<a href="https://arxiv.org/pdf/2305.03111">https://arxiv.org/pdf/2305.03111</a></td></tr><tr><td align="left"><a href="https://bull-text-to-sql-benchmark.github.io/">BULL 1.0</a></td><td align="left">2024年由浙江大学的Chao Zhang等人提出,来源于恒生电子实战金融分析业务，包含基金、股票、宏观经济数据库的金融文本转SQL数据集，各数据库共有31、28和19个表，由金融专业人士注释的4966个自然语言问题SQL对。</td></tr><tr><td align="left"><a href="https://github.com/ppasupat/WikiTableQuestions">WikiTableQuestions</a></td><td align="left">2015年斯坦福大学提出的、数据来源自维基百科的数据集。</td></tr><tr><td align="left"><a href="https://www.kaggle.com/siddhadev/ms-cntk-atis">The Air Travel Information System(ATIS)</a></td><td align="left">1990年德州仪器公司提出的经典数据集。</td></tr></tbody></table><h3 id="中文典型数据集"><a href="#中文典型数据集" class="headerlink" title="中文典型数据集"></a>中文典型数据集</h3><table><thead><tr><th align="left">数据集名</th><th align="left">内容</th></tr></thead><tbody><tr><td align="left"><a href="https://tianchi.aliyun.com/competition/entrance/231716/information">NL2SQL</a></td><td align="left">2019年阿里天池数据算法赛，首届中文NL2SQL挑战赛追一科技的提供的，使用金融以及通用领域的表格数据作为数据源。</td></tr><tr><td align="left"><a href="https://taolusi.github.io/CSpider-explorer/">CSpider</a></td><td align="left">2019年西湖大学提出的、中文大规模复杂跨域语义解析和文本转 SQL 数据集，CSipder 是由 Spider 翻译而来,包含200个数据库上的10181个问题和5693个独特的复杂SQL查询，具有涵盖138个不同领域的多个表的数据库。论文：<a href="https://arxiv.org/pdf/1909.13293">https://arxiv.org/pdf/1909.13293</a></td></tr><tr><td align="left"><a href="https://aclanthology.org/2020.emnlp-main.562.pdf">DuSQL</a></td><td align="left">2020年百度构建的一个大规模、多领域、多表的中文Text-to-SQL数据集，数据集覆盖160+领域，包含200个Database以及对应的2.3979万对(question, SQL query)，其中18602对用于训练集，2039用于验证集，3156用于测试集,该数据集不仅覆盖了SQL常见关键词，还覆盖了更多实际应用中的问题类型，如排序、比较、计算等。网站地址：<a href="https://aistudio.baidu.com/competition/detail/47/0/task-definition">https://aistudio.baidu.com/competition/detail/47/0/task-definition</a></td></tr></tbody></table><h3 id="其他典型数据集"><a href="#其他典型数据集" class="headerlink" title="其他典型数据集"></a>其他典型数据集</h3><p>GeoQuery&#x2F;Scholar&#x2F;Academic&#x2F;IMDB&#x2F;Yelp&#x2F;Advising&#x2F;Restaurants&#x2F;<a href="https://arxiv.org/pdf/1906.02285">SParC</a>&#x2F;<a href="https://arxiv.org/pdf/1909.05378">CoSQL</a>&#x2F;<a href="https://arxiv.org/pdf/2006.06434">TableQA</a>&#x2F;<a href="https://aclanthology.org/2021.acl-long.180.pdf">CHASE</a>&#x2F;<a href="https://arxiv.org/abs/2106.11455">KaggleDBQA</a></p><h2 id="Text2SQL-NL2SQL-数据集评测维度"><a href="#Text2SQL-NL2SQL-数据集评测维度" class="headerlink" title="Text2SQL(NL2SQL)数据集评测维度"></a>Text2SQL(NL2SQL)数据集评测维度</h2><p>单&#x2F;多轮、单&#x2F;多表、鲁棒性、歧义性<br>　　<strong>单&#x2F;多轮测试</strong>：构建单&#x2F;多轮对话测试，测试系统在单&#x2F;多轮对话中的NL2SQL能力。<br>　　<strong>单&#x2F;多表测试</strong>：构建单&#x2F;多表对话测试，测试系统在涉及单&#x2F;多表时的NL2SQL能力。<br>　　<strong>鲁棒性测试</strong>：验证系统在NL2SQL过程中承受不利扰动干预的能力。<br>　　<strong>歧义性测试</strong>：验证系统在模糊环境下，领会正确意图产生正确SQL能力。</p><h2 id="Text2SQL-NL2SQL-数据集评分标准"><a href="#Text2SQL-NL2SQL-数据集评分标准" class="headerlink" title="Text2SQL(NL2SQL)数据集评分标准"></a>Text2SQL(NL2SQL)数据集评分标准</h2><p>根据阿里天池数据算法赛的比赛评分标准等公开评测指标，常用评分标准有<strong>Logic Form Accuracy、Execution Accuracy和Valid Efficiency Score</strong>。<br>　　<strong>Logic Form Accuracy(Accqm)或Exact Match Accuracy(精确匹配率, EM)</strong>: 预测完全正确的SQL语句。其中，列的顺序并不影响准确率的计算(即Select A, B from T&#x3D; Select B, A from T)。预测完全正确的SQL语句和标注SQL的匹配程度。<br>　　<strong>Execution Accuracy(执行准确率， EX)</strong>: 预测的SQL的执行结果与真实SQL的执行结果一致。 预测的SQL的执行结果正确数量在数据集中的比例。<br>　　<strong>Valid Efficiency Score(有效效率分数， VES)</strong>:在考虑SQL准确性和效率的情况下，对比验证预测SQL的运算时间和真实标注SQL的运行时间的相对差异。<br>　　阿里天池数据算法赛排行榜以$Score_{lf}$与$Score_{ex}$的平均值排序。<br>　　<br>$$<br>Score_{lf} &#x3D; \begin{cases}<br>1, &amp; SQL^{‘} &#x3D; SQL\<br>0, &amp; SQL^{‘} \neq SQL<br>\end{cases}<br>$$</p><p>$$<br>Acc_{lf} &#x3D; \frac{1}{N}\sum^{N}<em>{n &#x3D; 1}Score</em>{lf}^{n}&#x3D; \frac{预测SQL和真实SQL所有子句匹配成功的问题数}{所有问题数}<br>$$</p><p>$$<br>Score_{ex} &#x3D; \begin{cases}<br>1, &amp; Y^{‘} &#x3D; Y\<br>0, &amp; Y^{‘} \neq Y<br>\end{cases}<br>$$</p><p>$$<br>Acc_{ex} &#x3D; \frac{1}{N}\sum^{N}<em>{n &#x3D; 1}Score</em>{ex}^{n}<br> &#x3D; \frac{回答正确的问题数}{所有问题数}<br>$$</p><p>其中,$N$表示数据量，$SQL^{‘}$和$SQL$分别代表预测的SQL语句和真实SQL语句，$Score_{lf}$表示Logic Form准确率;$Y^{‘}$和$Y$分别表示预测的SQL和真实SQL的执行结果，$Score_{ex}$表示Execution准确率。</p><h2 id="学术界解决方案"><a href="#学术界解决方案" class="headerlink" title="学术界解决方案"></a>学术界解决方案</h2><h3 id="Text2SQL-NL2SQL-技术综述"><a href="#Text2SQL-NL2SQL-技术综述" class="headerlink" title="Text2SQL(NL2SQL)技术综述"></a>Text2SQL(NL2SQL)技术综述</h3><p>[1] <a href="https://arxiv.org/abs/2208.13629">A Survey on Text-to-SQL Parsing: Concepts, Methods, and Future Directions</a><br>[2]  <a href="https://arxiv.org/abs/2408.05109">A Survey of Text-to-SQL in the Era of LLMs: Where are we, and where are we going?</a><br>[3] <a href="https://www.semanticscholar.org/paper/NL2SQL-is-a-solved-problem...-Not!-Floratou-Psallidas/ff2d31ac2fbd08ba088e6851f1ccbe482465aea5">NL2SQL is a solved problem… Not!</a><br>[4] <a href="https://arxiv.org/abs/2406.08426">Next-Generation Database Interfaces:A Survey of LLM-based Text-to-SQL</a><br>[5] <a href="https://arxiv.org/abs/2208.13629">A Survey on Text-to-SQL Parsing: Concepts, Methods, and Future Directions</a><br>[6]  <a href="https://arxiv.org/abs/2208.10099">Recent Advances in Text-to-SQL: A Survey of What We Have and What We Expect</a><br>[7]<a href="https://arxiv.org/abs/2208.04415">Deep Learning Driven Natural Languages Text to SQL Query Conversion: A Survey</a></p><h3 id="Spider数据集下NL2SQL方案"><a href="#Spider数据集下NL2SQL方案" class="headerlink" title="Spider数据集下NL2SQL方案"></a>Spider数据集下NL2SQL方案</h3><table><thead><tr><th align="left">论文</th><th align="left">内容</th><th align="left">GitHub地址</th></tr></thead><tbody><tr><td align="left">[1] <a href="https://arxiv.org/abs/2302.05965">RESDSQL: Decoupling Schema Linking and Skeleton Parsing for Text-to-SQL</a></td><td align="left">2023年Haoyang Li等人提出，引入了一种新的 Text-to-SQL 解析器——RESDSQL(Ranking -enhanced Encoding plus a Skeleton -aware Decoding framework for Text-to- SQL)，它试图将Schema Linking和Skeleton Parsing分离，以降低 Text-to-SQL 的难度, 具体做法是先进行Schema Linking使得输入模型encoder的schema只包括与问题最相关的表和列；在生成SQL时，模型decoder先生成SQL骨架再生成实际的SQL查询。所有实验均在单块 NVIDIA A100 80G GPU 上进行。</td><td align="left"><a href="https://github.com/RUCKBReasoning/RESDSQL">https://github.com/RUCKBReasoning/RESDSQL</a></td></tr><tr><td align="left">[2] <a href="https://arxiv.org/abs/2304.11015">DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction</a></td><td align="left">2023年Mohammadreza Pourreza, Davood Rafiei提出,通过将SQL生成问题分解为子问题，并将这些子问题的解决方案输入 LLM，可以显著提升其在文本到 SQL这一高难度任务上的性能。</td><td align="left"><a href="https://github.com/MohammadrezaPourreza/Few-shot-NL2SQL-with-prompting">https://github.com/MohammadrezaPourreza/Few-shot-NL2SQL-with-prompting</a></td></tr><tr><td align="left">[3] <a href="https://arxiv.org/abs/2307.07306">C3: Zero-shot Text-to-SQL with ChatGPT</a></td><td align="left">2023年Xuemei Dong等人提出的，一种基于ChatGPT的零样本Text-to-SQL方法：C3，在 Spider 测试集上的执行准确率达到了 82.3%。</td><td align="left"><a href="https://github.com/bigbigwatermalon/C3SQL">https://github.com/bigbigwatermalon/C3SQL</a></td></tr><tr><td align="left">[4] <a href="https://arxiv.org/abs/2308.15363">Text-to-SQL Empowered by Large Language Models: A Benchmark Evaluation</a></td><td align="left">2023年Dawei Gao等人提出的，一种名为 DAIL-SQL 的全新集成解决方案，2023年8月20日Spider数据集排名第一的解决方案。</td><td align="left"><a href="https://github.com/BeachWang/DAIL-SQL">https://github.com/BeachWang/DAIL-SQL</a></td></tr><tr><td align="left">[5] <a href="https://arxiv.org/abs/2401.10506">FinSQL: Model-Agnostic LLMs-based Text-to-SQL Framework for Financial Analysis</a></td><td align="left">2024年Chao Zhang等人提出的，一个基于模型无关的大型语言模型 (LLM) 的金融分析Text-to-SQL框架：FinSQL，并基于LLM的Text-to-SQL框架FinSQL从快速构建、高效参数微调和输出校准的角度为金融Text-to-SQL提供了系统的解决方案.</td><td align="left"><a href="https://github.com/bigbigwatermalon/FinSQL?tab=readme-ov-file">https://github.com/bigbigwatermalon/FinSQL?tab=readme-ov-file</a></td></tr><tr><td align="left">[6] <a href="https://arxiv.org/abs/2402.13284">Structure-Guided Large Language Models for Text-to-SQL Generation</a></td><td align="left">2024年Qinggang Zhang等人提出的，一种新颖的结构引导Text-to-SQL框架:SGU-SQL，该框架结合了基于语法的提示功能来增强LLM的SQL生成能力。</td><td align="left">代码未开源</td></tr><tr><td align="left">[7] <a href="https://arxiv.org/abs/2411.02948">Grounding Natural Language to SQL Translation with Data-Based Self-Explanations</a></td><td align="left">2025年Yuankai Fan等人提出的，基于数据的自解释自然语言到SQL转换，主要探讨了如何通过迭代反馈机制提高自然语言（NL）到 SQL 的翻译精度，提出了名为CYCLESQL的框架。CYCLESQL的核心思想是在传统的NL到SQL翻译过程中引入基于数据的自然语言解释，利用这些解释作为自提供的反馈，通过自我验证的方式不断优化翻译结果。</td><td align="left"><a href="https://github.com/Kaimary/CycleSQL">https://github.com/Kaimary/CycleSQL</a></td></tr><tr><td align="left">[8] <a href="https://arxiv.org/abs/2109.05093">PICARD: Parsing Incrementally for Constrained Auto-Regressive Decoding from Language Models</a></td><td align="left">2021年Torsten Scholak等人提出PICARD模型，一种通过增量解析约束语言模型的自回归解码器的方法。PICARD通过在每个解码步骤拒绝不可接受的token输出来帮助找到有效的输出序列。在具有挑战性的Spider和CoSQL数据集的Text-to-SQL任务中，本文展示了PICARD将性能尚可的T5模型转换为最先进的解决方案。</td><td align="left"><a href="https://github.com/ServiceNow/picard">https://github.com/ServiceNow/picard</a></td></tr><tr><td align="left">[9] <a href="https://arxiv.org/abs/1911.04942">RAT-SQL: Relation-Aware Schema Encoding and Linking for Text-to-SQL Parsers</a></td><td align="left">2021年Bailin Wang等人提出的统一的框架，基于关系感知的自我注意机制，来处理Text-to-SQL编码器中的模式编码、模式链接和特征表示。</td><td align="left"><a href="https://github.com/Microsoft/rat-sql">https://github.com/Microsoft/rat-sql</a></td></tr></tbody></table><h3 id="Bird数据集下NL2SQL方案"><a href="#Bird数据集下NL2SQL方案" class="headerlink" title="Bird数据集下NL2SQL方案"></a>Bird数据集下NL2SQL方案</h3><table><thead><tr><th align="left">论文</th><th align="left">内容</th><th align="left">GitHub地址</th></tr></thead><tbody><tr><td align="left">[1] <a href="https://arxiv.org/abs/2410.01943">CHASE-SQL: Multi-Path Reasoning and Preference Optimized Candidate Selection in Text-to-SQL</a></td><td align="left">2024年Mohammadreza Pourreza等人提出的新框架CHASE-SQL，旨在通过利用多智能体建模和测试时计算来改进候选语句的生成和选择，从而解决文本转 SQL 任务中大型语言模型 (LLM) 的性能挑战。</td><td align="left">无</td></tr><tr><td align="left">[2] <a href="https://arxiv.org/abs/2405.16755">CHESS: Contextual Harnessing for Efficient SQL Synthesis</a></td><td align="left">2024年Shayan Talaei等人提出的，一种名为 CHESS（Contextual Harnessing for Efficient SQL Synthesis）的多代理框架，旨在有效地将自然语言问题转换为 SQL 查询，即文本到 SQL 的合成。CHESS 旨在应对文本到 SQL 合成中的一系列挑战，包括数据库目录的庞大规模、复杂的数据库架构推理、生成查询的功能有效性以及自然语言问题的歧义性。</td><td align="left"><a href="https://github.com/ShayanTalaei/CHESS">https://github.com/ShayanTalaei/CHESS</a></td></tr><tr><td align="left">[3] <a href="https://www.arxiv.org/abs/2409.16751">E-SQL: Direct Schema Linking via Question Enrichment in Text-to-SQL</a></td><td align="left">2024年Hasan Alp Caferoğlu等人提出的，一种名为E-SQL的新型管道，通过直接架构链接和候选谓词增强来应对文本到SQL转换中的挑战。该方法以增强自然语言查询为基础，将相关的数据库元素（如表、列和数值）直接纳入查询中，以便更好地匹配用户意图与数据库结构。</td><td align="left"><a href="https://github.com/HasanAlpCaferoglu/E-SQL?tab=readme-ov-file">https://github.com/HasanAlpCaferoglu/E-SQL?tab=readme-ov-file</a></td></tr><tr><td align="left">[4] <a href="https://arxiv.org/abs/2406.01265">The Dawn of Natural Language to SQL: Are We Fully Ready?</a></td><td align="left"><strong>重点</strong>。2024年Boyan Li等人提出的，一个多角度的NL2SQL评估测试框架：NL2SQL360，以便为研究人员设计和测试新的NL2SQL方法。通过NL2SQL360，在不同的数据领域和SQL特性等应用场景下对主流NL2SQL方法进行了详细比较，为选择特定需求的最合适的NL2SQL方法提供了有价值的见解。此外，探索了NL2SQL设计空间，利用NL2SQL360自动识别针对用户特定需求的最佳NL2SQL解决方案。</td><td align="left"><a href="https://github.com/BugMaker-Boyan/NL2SQL360/?tab=readme-ov-file">https://github.com/BugMaker-Boyan/NL2SQL360/?tab=readme-ov-file</a></td></tr><tr><td align="left">[5] <a href="https://arxiv.org/abs/2402.16347">CodeS: Towards Building Open-source Language Models for Text-to-SQL</a></td><td align="left">2024年Haoyang Li等人提出的，一个开源的专门用于Text2SQL任务的LLM——CodeS，有多个参数规模的版本(1B~15B)，它是基于StarCdoer基座模型，使用Text2SQL相关的数据集继续训练得到的。同时，论文提出了在训练这个模型和使用这个模型一些方法。</td><td align="left">GitHub地址：<a href="https://github.com/RUCKBReasoning/codes">https://github.com/RUCKBReasoning/codes</a></td></tr><tr><td align="left">[6] <a href="https://arxiv.org/abs/2312.11242">MAC-SQL: A Multi-Agent Collaborative Framework for Text-to-SQL</a></td><td align="left">2025年Bing Wang等人提出的，一个新颖的基于LLM的多代理协作框架MAC-SQL。该框架由三个Agents组成:Selector、Decomposer和Refiner。选择器压缩数据库并为用户查询保留相关的表模式。分解器将复杂的用户查询分解成更简单的子问题，并逐步解决它们。细化器验证和细化SQL查询。</td><td align="left"><a href="https://github.com/wbbeyourself/MAC-SQL">https://github.com/wbbeyourself/MAC-SQL</a></td></tr><tr><td align="left">[7] <a href="https://arxiv.org/abs/2505.19988">Automatic Metadata Extraction for Text-to-SQL</a></td><td align="left">2025年Vladislav Shkapenyuk等人提出，探索自动元数据提取技术，以支持Text-to-SQL生成，2025年3月11日Bird数据集排名第一的解决方案</td><td align="left">代码未开源</td></tr><tr><td align="left">[8] <a href="https://arxiv.org/abs/2402.13284">Structure Guided Large Language Model for SQL Generation</a></td><td align="left">2025年Qinggang Zhang等人提出，一种Text-to-SQL框架，利用内在的Text信息来改进LLM的SQL生成。具体而言，引入Text引导SQL（SGU-SQL）生成模型。SGU-SQL首先以Text增强的方式链接用户查询和数据库。然后，它使用语法树分解复杂的链接结构，逐步指导LLM生成SQL。</td><td align="left">代码未开源</td></tr><tr><td align="left">[9] <a href="https://arxiv.org/abs/2408.03256">Synthesizing Text-to-SQL Data from Weak and Strong LLMs</a></td><td align="left">2024年Jiaxi Yang等人提出,通过合成数据方法增强NL2SQL的领域泛化能力，该方法将由更大、更强大的模型（强模型）生成的数据与由较小、未很好对齐的模型（弱模型）生成的错误信息数据相结合。该方法不仅增强了文本到SQL模型的领域泛化能力，还通过偏好学习探索了错误数据监督的潜力。此外，我们将合成数据方法应用于开放源代码LLMs的指令调优，最终得到了SENSE，一个专门的文本到SQL模型。</td><td align="left"><a href="https://github.com/Yangjiaxi/Sense">https://github.com/Yangjiaxi/Sense</a></td></tr><tr><td align="left">[10] <a href="https://arxiv.org/abs/2508.17590v1">RubikSQL: Lifelong Learning Agentic Knowledge Base as an Industrial NL2SQL System</a></td><td align="left">2025年Zui Chen等人提出, 基于规则挖掘和用户查询增强构建面向智能体的持续学习库，旨在解决实际企业级NL2SQL应用中的核心挑战，如隐式意图（Implicit Intent）、领域特定术语（Private Domain Knowledge）、宽表模式（Wide Table Schema）以及上下文敏感性（Context Sensitivity）。该系统将NL2SQL视为一个终身学习任务，需要持续的知识库（Knowledge Base, KB）维护和 SQL 生成。其已经在华为内部财经小魔方系统中应用。</td><td align="left"><a href="https://github.com/Yangjiaxi/Sense">https://github.com/Yangjiaxi/Sense</a></td></tr><tr><td align="left">[11] <a href="https://arxiv.org/abs/2408.04919">SEA-SQL: Semantic-Enhanced Text-to-SQL with Adaptive Refinement</a></td><td align="left">2025年Chaofan Li等人提出了一种名为SEA-SQL（语义增强文本到SQL的自适应精炼）的框架，用于解决文本到SQL的任务，尤其是在资源开销较低的情况下提高SQL查询的准确性和可执行性。</td><td align="left"><a href="https://github.com/545999961/SEA-SQL">https://github.com/545999961/SEA-SQL</a></td></tr><tr><td align="left">[12] <a href="https://arxiv.org/abs/2405.00527">ChatBI: Towards Natural Language to Complex Business Intelligence SQL</a></td><td align="left">2024年Jinqing Lian等人提出, 研究了一种将自然语言转换为复杂商业智能 SQL 的技术——ChatBI。其目标是帮助不熟悉数据库的非专业用户通过自然语言进行数据分析。该技术方案已经百度数据平台上部署。</td><td align="left">代码未开源</td></tr></tbody></table><h3 id="NL2SQL的RAG方案"><a href="#NL2SQL的RAG方案" class="headerlink" title="NL2SQL的RAG方案"></a>NL2SQL的RAG方案</h3><table><thead><tr><th align="left">论文</th><th align="left">内容</th><th align="left">GitHub地址</th></tr></thead><tbody><tr><td align="left">[1] <a href="https://arxiv.org/abs/2408.14717">Text2SQL is Not Enough: Unifying AI and Databases with TAG</a></td><td align="left">2024年Asim Biswal等人提出,TAG(Table-Augmented Generation):表增强生成, TAG不仅具备对结构化数据的高效处理(不仅限于查询和分析)，还要通过LLM的引入实现语义推断和复杂多步推断。其中对Text2SQL、RAG、TAG之间的关系。</td><td align="left"><a href="https://github.com/TAG-Research/TAG-Bench">https://github.com/TAG-Research/TAG-Bench</a></td></tr></tbody></table><h3 id="NL2SQL的监督微调技术（SFT-Based）方案"><a href="#NL2SQL的监督微调技术（SFT-Based）方案" class="headerlink" title="NL2SQL的监督微调技术（SFT-Based）方案"></a>NL2SQL的监督微调技术（SFT-Based）方案</h3><table><thead><tr><th align="left">论文</th><th align="left">内容</th><th align="left">Hugging Face地址</th></tr></thead><tbody><tr><td align="left">[1] <a href="https://arxiv.org/abs/2411.02059">TableGPT2: A Large Multimodal Model with Tabular Data Integration</a></td><td align="left">2024年浙江大学Aofeng Su等人提出，经过预训练和微调训练的，基于qwen2.5d针对NL2SQ任务的TableGPT2大模型。</td><td align="left">Hugging Face:<a href="https://huggingface.co/tablegpt/TableGPT2-7B">https://huggingface.co/tablegpt/TableGPT2-7B</a> ;教程：<a href="https://tablegpt.github.io/tablegpt-agent/reference/">https://tablegpt.github.io/tablegpt-agent/reference/</a></td></tr><tr><td align="left">[2] <a href="https://arxiv.org/pdf/2411.08599">A PREVIEW OF XIYAN-SQL: A MULTI-GENERATOR ENSEMBLE FRAMEWORK FOR TEXT-TO-SQL</a></td><td align="left">2024年阿里巴巴集团提出，经过微调训练的，基于LLM的Text-to-SQL框架。</td><td align="left">Hugging Face:<a href="https://huggingface.co/collections/XGenerationLab/xiyansql-models-67c9844307b49f87436808fc">https://huggingface.co/collections/XGenerationLab/xiyansql-models-67c9844307b49f87436808fc</a> ;析言GBI:<a href="https://cn.aliyun.com/product/bailian/xiyan?from_alibabacloud=">https://cn.aliyun.com/product/bailian/xiyan?from_alibabacloud=</a></td></tr></tbody></table><h3 id="大语言模型LLM在TexttoSQL上性能"><a href="#大语言模型LLM在TexttoSQL上性能" class="headerlink" title="大语言模型LLM在TexttoSQL上性能"></a>大语言模型LLM在TexttoSQL上性能</h3><table><thead><tr><th align="left">论文</th><th align="left">内容</th></tr></thead><tbody><tr><td align="left">[1] <a href="https://arxiv.org/abs/2310.10190">大语言模型LLM之战:Dolly、LLaMA 、Vicuna、Guanaco、Bard、ChatGPT–在自然语言转SQL的比较</a></td><td align="left">将六种流行的大型语言模型进行对比，在九个基准数据集上，使用五种不同的提示策略，系统地评估它们的文本到SQL解析能力，涵盖零样本和少样本场景。</td></tr></tbody></table><h2 id="开源工具和项目"><a href="#开源工具和项目" class="headerlink" title="开源工具和项目"></a>开源工具和项目</h2><table><thead><tr><th align="left">工具名</th><th align="left">内容</th><th align="left">GitHub地址</th></tr></thead><tbody><tr><td align="left">MindSQL</td><td align="left">一个文本到SQL生成的Python包，开源、使用RAG(检索增强生成)在大型语言模型LLM的帮助下创建适合数据库的精确SQL查询, 其精度高而且支持高并发。MindSQL与PostgreSQL、MySQL、SQLite等知名数据库无缝集成，还通过扩展接口将其功能扩展到Snowflake、BigQuery等主流数据库IDatabase。这个包的安装不容易，需要额外处理。</td><td align="left"><a href="https://github.com/Mindinventory/MindSQL">https://github.com/Mindinventory/MindSQL</a></td></tr><tr><td align="left"><a href="https://sqlglot.com/sqlglot.html">SQLGlot</a></td><td align="left">Python开发的开源SQL解析器、转译器、优化器和引擎，可以格式化SQL以及支持在20种不同方言和SQL之间进行转化, 用相应的函数来进行SQL查询语句的转换，无需复杂的配置和设置，可用于Text2SQL的后处理阶段。</td><td align="left"><a href="https://github.com/tobymao/sqlglot">https://github.com/tobymao/sqlglot</a></td></tr><tr><td align="left">Microsoft.Recognizers.Text</td><td align="left">微软开源多种语言下的数字、单位、日期、时间等实体的识别和解析器，帮助开发者在其应用程序中集成强大的实体识别功能。</td><td align="left"><a href="https://github.com/microsoft/Recognizers-Text/tree/master">https://github.com/microsoft/Recognizers-Text/tree/master</a></td></tr><tr><td align="left">lotus</td><td align="left">基于LLM的开源Python处理结构化和非结构化数据的查询引擎。</td><td align="left"><a href="https://github.com/lotus-data/lotus">https://github.com/lotus-data/lotus</a></td></tr><tr><td align="left">SQLCoder</td><td align="left"><strong>重点</strong>，专为NL2SQL优化的开源LLM模型族。这是一个拥有150亿参数的模型，在自然语言到 SQL 生成任务上，其性能略微超过了 gpt-3.5-turbo 并且显著地超越了所有流行的开源模型。目前SQLCoder是基于StarCoder(StarCoder参考： <a href="https://www.datalearner.com/ai-models/pretrained-models/StarCoder)%E5%BE%AE%E8%B0%83%E5%BE%97%E5%88%B0%E3%80%82%E5%AE%83%E7%9A%84%E4%BB%A3%E7%A0%81%E5%BC%80%E6%BA%90%E5%8D%8F%E8%AE%AE%E6%98%AFApache">https://www.datalearner.com/ai-models/pretrained-models/StarCoder)微调得到。它的代码开源协议是Apache</a> 2.0，而模型预训练结果的开源协议是CC BY-SA-4.0并加上OpenRAIL-M clauses for responsible协议，运行大家免费商用。但是，如果你修改了该模型的权重，则必须也按照CC BY-SA-4.0协议开源，这意味着修改具有传染性！商用的用户需要注意！</td><td align="left"><a href="https://github.com/defog-ai/sqlcoder">https://github.com/defog-ai/sqlcoder</a>; <a href="https://www.datalearner.com/blog/1051692667851329">SQLCoder详细简介</a>; <a href="https://defog.ai/sqlcoder-demo/">SQLCoder在线演示地址</a></td></tr><tr><td align="left">Vanna</td><td align="left">基于MIT许可、开源、Python的、RAG(检索增强生成)框架，使用检索增强功能来帮助使用LLM为数据库生成准确的SQL查询。</td><td align="left"><a href="https://github.com/vanna-ai/vanna">https://github.com/vanna-ai/vanna</a></td></tr><tr><td align="left">LLaMA-Factory</td><td align="left">简单易用且高效的大型语言模型LLM训练与微调平台框架。其整合主流的各种高效训练微调技术，适配市场主流开源模型。</td><td align="left"><a href="https://github.com/hiyouga/LLaMA-Factory/tree/main">https://github.com/hiyouga/LLaMA-Factory/tree/main</a></td></tr><tr><td align="left"><a href="https://github.com/tencentmusic/supersonic/blob/master/README_CN.md">SuperSonic</a></td><td align="left">国内首个开源NL2SQL项目。通过引入Headless BI，构建数据加工整合连接数据，解决数据中口径不一致、指标无法复用、治理困难的问题。基于数据库数据构建语义模型, 使用语义查询语言S2SQL构建逻辑SQL, 将逻辑SQL翻译为数据库可以使用的物流SQL。</td><td align="left"><a href="https://github.com/tencentmusic/supersonic/tree/master">https://github.com/tencentmusic/supersonic/tree/master</a>; <a href="https://supersonicbi.github.io/">项目教程</a></td></tr><tr><td align="left">DDParser</td><td align="left">DDParser(Baidu Dependency Parser)是百度自然语言处理部基于深度学习平台飞桨(PaddlePaddle)和大规模标注数据研发的、开源的依存句法分析工具。</td><td align="left"><a href="https://github.com/baidu/DDParser">https://github.com/baidu/DDParser</a>; <a href="https://pypi.org/project/ddparser/">pypi地址</a></td></tr><tr><td align="left">ChatSQL</td><td align="left">基于ChatGLM-6B&#x2F;MOSS,实现nl2sql，直连数据库并返回查询结果 目前仅支持MYSQL语法,后续支持多数据库语法查询。</td><td align="left"><a href="https://github.com/cubenlp/ChatSQL">https://github.com/cubenlp/ChatSQL</a></td></tr><tr><td align="left"><a href="https://www.yuque.com/eosphoros/dbgpt-docs">DB-GPT</a></td><td align="left">DB-GPT是蚂蚁开源的AI原生数据应用开发框架。目的是构建大模型领域的基础设施，通过开发多模型管理(SMMF)、Text2SQL效果优化、RAG框架以及优化、Multi-Agents框架协作、AWEL(智能体工作流编排)等多种技术能力，让围绕数据库构建大模型应用更简单，更方便。</td><td align="left"><a href="https://github.com/eosphoros-ai/DB-GPT?tab=readme-ov-file">https://github.com/eosphoros-ai/DB-GPT?tab=readme-ov-file</a></td></tr><tr><td align="left"><a href="https://dataease.cn/sqlbot/v1/">SQLBot</a></td><td align="left">SQLBot 是一款基于大语言模型(LLM)和RAG(检索增强生成)的智能问数系统。借助 SQLBot，用户可以实现数据的即问即答，快速提炼获取所需的数据信息及可视化图表，并且支持进一步开展智能分析。</td><td align="left"><a href="https://github.com/dataease/SQLBot?tab=readme-ov-file">https://github.com/dataease/SQLBot?tab=readme-ov-file</a></td></tr><tr><td align="left"><a href="https://docs.getwren.ai/">WrenAI</a></td><td align="left">利用LLMs和RAG技术的优势，将自然语言转换为SQL查询，并从数据库中检索数据。</td><td align="left"><a href="https://github.com/canner/wrenai">https://github.com/canner/wrenai</a></td></tr><tr><td align="left"><a href="https://www.dataherald.com/">Dataherald</a></td><td align="left">强大的NLtoSQL引擎，其核心是一个基于LLM的智能代理，它利用思维链(CoT)推理和多种工具，从用户提问中生成高精度的SQL查询。</td><td align="left"><a href="https://github.com/Dataherald/dataherald">https://github.com/Dataherald/dataherald</a></td></tr><tr><td align="left"><a href="https://chat2db-ai.com/">Chat2DB</a></td><td align="left">阿里开源的专为现代数据驱动型企业打造的数据库管理、数据开发及数据分析工具。作为一款AI原生的产品，Chat2DB 将人工智能技术与传统数据库管理功能深度融合，旨在提供更为智能、便捷的工作体验，助力用户高效地管理数据库、开展数据开发和分析工作。</td><td align="left"><a href="https://github.com/codePhiliaX/chat2db">https://github.com/codePhiliaX/chat2db</a></td></tr></tbody></table><h2 id="个人设想"><a href="#个人设想" class="headerlink" title="个人设想"></a>个人设想</h2><p>个人设想：<br>　　SQL语言本身在不同数据库下的语法比较复杂而且同时存在差异性，NL2SQL本身也许不一定是“自然语言问数”的最佳范式。现有问数基本是三种技术路线：NL2SQL&#x2F;NL2SQL-&gt;DSL2SQL&#x2F;NL2语义SQL-&gt;语义SQL-&gt;可执行物理SQL，核心都是在解决SQL的可信和准确性问题，而不管是SQL还是DSL这类语法树，LLM和数据库交互的中介仍然是语法描述，这导致中介被数据库方言所绑架，进而严重依赖LLM的数据库适用能力，忽视逻辑执行计划本身才是业务逻辑和数据库物理实现真正的核心。采用执行计划本身作为自然语言和数据库交互的中介，越过各类SQL引擎，更加靠近物理执行引擎，进而在缩短执行链路的同时，规避SQL方言的问题，直接生成数据库的物理执行计划，进而实现更快速和直接的自然语言问数形式。</p><h2 id="技术主要用途"><a href="#技术主要用途" class="headerlink" title="技术主要用途"></a>技术主要用途</h2><p><strong>技术主要用途</strong>：智能BI(智能问数与智能数据分析)</p><h2 id="相似工作"><a href="#相似工作" class="headerlink" title="相似工作"></a>相似工作</h2><p><strong>相似工作</strong>：有基于图谱的智能QA问答、自动化代码编程等。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] A. Popescu, A. Armanasu, O. Etzioni, D. Ko, and A. Yates. Modern natural language interfaces to databases: Composing statistical parsing with semantic tractability. In COLING, 2004.<br>[2] F. Li and H. V. Jagadish. Constructing an interactive natural language interface for relational databases. PVLDB, 8(1):73–84, 2014.<br>[3] D. Saha, A. Floratou, K. Sankaranarayanan, U. F. Minhas, A. R. Mittal, and F. O¨ zcan. ATHENA: an ontology-driven system for natural language querying over relational data stores. PVLDB, 9(12):1209–1220, 2016.<br>[4] N. Yaghmazadeh, Y. Wang, I. Dillig, and T. Dillig. Sqlizer: query synthesis from natural language. PACMPL, 1(OOPSLA):63:1–63:26, 2017.<br>[5] P. Pasupat and P. Liang. Compositional semantic parsing on semi-structured tables. In ACL, pages 1470–1480, 2015.<br>[6] C. Baik, H. V. Jagadish, and Y. Li. Bridging the semantic gap with SQL query logs in natural language interfaces to databases. In ICDE, pages 374–385, 2019.<br>[7] S. Iyer, I. Konstas, A. Cheung, J. Krishnamurthy, and L. Zettlemoyer. Learning a neural semantic parser from user feedback. In ACL, pages 963–973, 2017.<br>[8] F. Basik, B. H¨attasch, A. Ilkhechi, A. Usta, S. Ramaswamy, P. Utama, N. Weir, C. Binnig, and U. C¸ etintemel. Dbpal: A learned nl-interface for databases. In SIGMOD, pages 1765–1768, 2018.<br>[9] V. Zhong, C. Xiong, and R. Socher. Seq2sql: Generating structured queries from natural language using reinforcement learning. CoRR, abs&#x2F;1709.00103, 2017.<br>[10] X. Xu, C. Liu, and D. Song. Sqlnet: Generating structured queries from natural language without reinforcement learning. CoRR, abs&#x2F;1711.04436, 2017.<br>[11] M. Lapata and L. Dong. Coarse-to-fine decoding for neural semantic parsing. In ACL, pages 731–742, 2018.<br>[12] S. Yavuz, I. Gur, Y. Su, and X. Yan. Dialsql: Dialogue based structured query generation. In ACL, pages 1339–1349, 2018.<br>[13] P. Huang, C. Wang, R. Singh, W. Yih, and X. He. Natural language to structured query generation via meta-learning. In NAACL-HLT, pages 732–738, 2018.<br>[14] M. Zhou, G. Cao, T. Liu, N. Duan, D. Tang, B. Qin, X. Feng, J. Ji, and Y. Sun. Semantic parsing with syntax- and table-aware SQL generation. In ACL, pages 361–372, 2018.<br>[15] T. Yu, Z. Li, Z. Zhang, R. Zhang, and D. R. Radev. Typesql: Knowledge-based type-aware neural text-to-sql generation. In NAACL-HLT, pages 588–594, 2018.<br>[16] C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In ICML, pages 1126–1135, 2017.<br>[17] T. Yu, R. Zhang, K. Yang, M. Yasunaga, D. Wang, Z. Li, J. Ma, I. Li, Q. Yao, S. Roman, Z. Zhang, and D. R. Radev. Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task. In EMNLP, pages 3911–3921, 2018.<br>[18] T. Yu, M. Yasunaga, K. Yang, R. Zhang, D. Wang, Z. Li, and D. R. Radev. Syntaxsqlnet: Syntax tree networks for complex and cross-domain text-to-sql task. In EMNLP, pages 1653–1663, 2018.<br>[19] B. Bogin, J. Berant, and M. Gardner. Representing schema structure with graph neural networks for text-to-sql parsing. In ACL, pages 4560–4565, 2019.<br>[20] J. Guo, Z. Zhan, Y. Gao, Y. Xiao, J. Lou, T. Liu, and D. Zhang. Towards complex text-to-sql in cross-domain database with intermediate representation. In ACL, pages 4524–4535, 2019.<br>[21] Pasupat P, Liang P. Compositional semantic parsing on semi-structured tables[J]. arXiv preprint arXiv:1508.00305, 2015.<br>[22] Hemphill C T, Godfrey J J, Doddington G R. The ATIS spoken language systems pilot corpus[C]&#x2F;&#x2F;Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley, Pennsylvania, June 24-27, 1990. 1990.<br>[23] Sun N, Yang X, Liu Y. Tableqa: a large-scale chinese text-to-sql dataset for table-aware sql generation[J]. arXiv preprint arXiv:2006.06434, 2020.<br>[24] Min Q, Shi Y, Zhang Y. A pilot study for chinese sql semantic parsing[J]. arXiv preprint arXiv:1909.13293, 2019.<br>[25] Wang L, Zhang A, Wu K, et al. ChiTeSQL: A Large-Scale and Pragmatic Chinese Text-to-SQL Dataset[C]&#x2F;&#x2F;Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020: 6923-6935.<br>[26] Bogin B, Gardner M, Berant J. Global reasoning over database structures for text-to-sql parsing[J]. arXiv preprint arXiv:1908.11214, 2019.<br>[27] 刘译璟, 徐林杰, 代其锋. 基于自然语言处理和深度学习的 NL2SQL 技术及其在 BI 增强分析中的应用[J]. 中国信息化, 2019, 11.<br>[28] <a href="https://zhuanlan.zhihu.com/p/1920428002006795984">25年不可错过的大模型应用方向-Text2SQL: 企业提效50%以上</a><br>[29] <a href="https://zhuanlan.zhihu.com/p/269478469">语义解析 (Text-to-SQL) 技术研究及应用 下篇</a><br>[30] <a href="https://originalstatic.aminer.cn/misc/billboard/CAIIAM/%E5%88%98%E8%AF%91%E7%92%9F-%E5%9F%BA%E4%BA%8E%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84NL2SQL%E6%8A%80%E6%9C%AF%E5%8F%8A%E5%85%B6%E5%9C%A8BI%E5%A2%9E%E5%BC%BA%E5%88%86%E6%9E%90%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8.pdf">基于自然语言处理和深度学习的NL2SQL技术及其在BI增强分析中的应用</a><br>[31] <a href="https://www.tellius.com/resources/blog/is-a-semantic-layer-necessary-for-enterprise-grade-ai-agents">Is a Semantic Layer Necessary for Enterprise-Grade AI Agents?</a><br>[32] <a href="https://arxiv.org/pdf/2411.08599">A PREVIEW OF XIYAN-SQL: A MULTI-GENERATOR ENSEMBLE FRAMEWORK FOR TEXT-TO-SQL</a><br>[33] <a href="https://arxiv.org/abs/2410.01943">CHASE-SQL: Multi-Path Reasoning and Preference Optimized Candidate Selection in Text-to-SQL</a><br>[34] <a href="https://hub.baai.ac.cn/papers">智源社区</a><br>[35] <a href="https://www.datafocus.ai/category/BI%203.0?page=2">ChatBI</a><br>[36] <a href="https://blog.csdn.net/sinat_39620217/article/details/137603846">NL2SQL基础系列(1)：业界顶尖排行榜、权威测评数据集及LLM大模型（Spider vs BIRD）全面对比优劣分析[Text2SQL、Text2DSL]</a><br>[37] <a href="https://github.com/eosphoros-ai/Awesome-Text2SQL/blob/main/README.zh.md">Awesome Text2SQL</a><br>[38] <a href="https://github.com/yechens/NL2SQL">Text2SQL 语义解析数据集、解决方案、paper资源整合项目</a><br>[39] <a href="https://mp.weixin.qq.com/s/FtsA4O_VTUqhhYS3Gq3G8Q">语义解析 (Text-to-SQL) 技术研究及应用</a><br>[40]<a href="https://www.shaqiu.cn/article/AGlZV58ELBJm">沙丘智库《2025年“大模型+数据分析”最佳实践报告》正式发布</a></p>]]></content>
      
      
      <categories>
          
          <category> 应用 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
            <tag> 算法 </tag>
            
            <tag> 应用 </tag>
            
            <tag> Text2SQL </tag>
            
            <tag> NL2SQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>可解释人工智能及其研究-SHAP算法应用篇</title>
      <link href="/2025/09/16/%E5%8F%AF%E8%A7%A3%E9%87%8A%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%8F%8A%E5%85%B6%E7%A0%94%E7%A9%B6-SHAP%E7%AE%97%E6%B3%95%E5%BA%94%E7%94%A8%E7%AF%87/"/>
      <url>/2025/09/16/%E5%8F%AF%E8%A7%A3%E9%87%8A%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%8F%8A%E5%85%B6%E7%A0%94%E7%A9%B6-SHAP%E7%AE%97%E6%B3%95%E5%BA%94%E7%94%A8%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h3 id="SHAP算法在成人人口普查数据的应用"><a href="#SHAP算法在成人人口普查数据的应用" class="headerlink" title="SHAP算法在成人人口普查数据的应用"></a>SHAP算法在成人人口普查数据的应用</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用的是shap = 0.48.0 版本</span></span><br><span class="line"><span class="comment"># 使用的是xgboost= 3.0.4 版本</span></span><br><span class="line"><span class="comment"># 使用的是numpy = 1.26.4 版本</span></span><br><span class="line"><span class="comment"># 使用的是matplotlib = 3.10.5版本</span></span><br><span class="line"><span class="comment"># 使用的是sklearn= 1.6.1版本</span></span><br><span class="line"><span class="comment"># 使用的是lightgbm = 4.6.0版本</span></span><br><span class="line"><span class="comment"># 使用的是pandas = 2.2.3版本 </span></span><br><span class="line"><span class="comment"># 查看本地全部包和对应版本命令：pip list</span></span><br><span class="line"><span class="keyword">import</span> shap</span><br><span class="line"><span class="keyword">import</span> xgboost </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> lightgbm <span class="keyword">as</span> lgb</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果 display 为 True，则 X 包含不包含“Education”、“Target”和“fnlwgt”目标列和冗余列的原始数据。否则，X 包含没有“Target”和“fnlwgt”列的已处理数据。</span></span><br><span class="line"><span class="comment"># X，X_display都是32561 rows x 12 columns的数据</span></span><br><span class="line"><span class="comment"># y，y_display都是32561 rows x 1 columns的数据,数组返回的“Target”列,这里可能有bug.</span></span><br><span class="line">X, y = shap.datasets.adult()</span><br><span class="line">X_display, y_display = shap.datasets.adult(display=<span class="literal">True</span>) </span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">7</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># XGBClassifier是sklearn中XGBoost分类器的实现，集成多个决策树来改善模型预测精度</span></span><br><span class="line"><span class="comment"># eval_metric确认任务类型为分类任务，使用负对数似然函数&#x27;mlogloss&#x27;</span></span><br><span class="line"><span class="comment"># 模型初始化并做模型训练</span></span><br><span class="line">model = xgboost.XGBClassifier(eval_metric=<span class="string">&#x27;mlogloss&#x27;</span>).fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这是模型自带的特征重要性，可以查看特征重要性，plot_importance()函数基于XGBoost模型训练后计算的特征重要性分数来绘制图表，</span></span><br><span class="line"><span class="comment"># 默认使用importance_type = &#x27;weight&#x27;,特征在树中的平均权重做为重要性的度量.</span></span><br><span class="line"><span class="comment"># xgboost.plot_importance(model)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建Explainer解释器并利用训练数据X计算shap值</span></span><br><span class="line"><span class="comment"># shap_values的数据类型为 &lt;class &#x27;numpy.ndarray&#x27;&gt;</span></span><br><span class="line"><span class="comment"># shap_values2的数据类型为&lt;class &#x27;shap._explanation.Explanation&#x27;&gt;</span></span><br><span class="line">explainer = shap.Explainer(model) </span><br><span class="line">shap_values = explainer.shap_values(X) <span class="comment"># 第一个特征的shap values:shap_values[0][0]</span></span><br><span class="line">shap_values2 = explainer(X) </span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"># 全局条形图：summary plot是针对全部样本预测的解释，是取每个特征的shap values的平均绝对值来获得标准条形图，这个其实就是全局重要度</span></span><br><span class="line"><span class="string"># Summary_plot 为每一个样本绘制其每个特征的Shapley value，它说明哪些特征最重要，以及它们对数据集的影响范围。</span></span><br><span class="line"><span class="string"># 另一种是通过散点简单绘制每个样本的每个特征的shap values，通过颜色可以看到特征值大小与预测影响之间的关系，同时展示其特征值分布</span></span><br><span class="line"><span class="string"># 两个图都可以看到Relationship全局重要度是最高的，其次是Age。第一个图可以看到各个特征重要度的相对关系，虽然Capital Gain是第三，但是重要度只有Relationship的60%，</span></span><br><span class="line"><span class="string"># shap.summary_plot使用的是shap_values的数据类型为 &lt;class &#x27;numpy.ndarray&#x27;&gt;</span></span><br><span class="line"><span class="string"># shap.plots.bar使用的是shap_values2的数据类型为&lt;class &#x27;shap._explanation.Explanation&#x27;&gt;</span></span><br><span class="line"><span class="string">#shap.plots.bar(shap_values2, max_display = 12)</span></span><br><span class="line"><span class="string">shap.summary_plot(shap_values, X, plot_type=&quot;bar&quot;)</span></span><br><span class="line"><span class="string">#按照计算公式，可得其数值表格化如下</span></span><br><span class="line"><span class="string">feature_importance = pd.DataFrame()</span></span><br><span class="line"><span class="string">feature_importance[&#x27;feature&#x27;] = X.columns</span></span><br><span class="line"><span class="string">feature_importance[&#x27;importance&#x27;] = np.abs(shap_values).mean(0)</span></span><br><span class="line"><span class="string">feature_importance.sort_values(&#x27;importance&#x27;, ascending=False)</span></span><br><span class="line"><span class="string">print(feature_importance)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># y 轴上的位置由特征确定，x 轴上的位置由每 Shapley value 确定。颜色表示特征值（红色高，蓝色低），颜色使我们能够匹配特征值的变化如何影响风险的变化。</span></span><br><span class="line"><span class="comment"># 重叠点在 y 轴方向抖动，因此我们可以了解每个特征的 Shapley value分布，并且这些特征是根据它们的重要性排序的。</span></span><br><span class="line"><span class="comment"># 由颜色深浅则可以看到Relationship和Age都是值越大，个人年收入超过5万美元的可能性越大，平均而言年龄是最重要的特征，与年轻（蓝色）人相比，收入超过 5 万美元的可能性较小</span></span><br><span class="line"><span class="comment"># 也使用可以shap.plots.beeswarm绘制一样的蜂群图，在显示数据集中的TOP特征如何影响模型输出的信息密集摘要。</span></span><br><span class="line"><span class="comment"># 点的 x 位置由该特征的 SHAP 值 ( shap_values.value[instance,feature]) 确定，并且点沿每个特征行“堆积”以显示密度。</span></span><br><span class="line"><span class="comment">#shap.summary_plot(shap_values, X)</span></span><br><span class="line"><span class="comment">#shap.plots.beeswarm(shap_values2, order=np.abs(shap_values).mean(0).argsort()[::-1], max_display = 12)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 可以使用matplotlib构建自定义颜色图，color默认使用shap.plots.colors.red_blue的颜色</span></span><br><span class="line"><span class="comment">#shap.plots.beeswarm(shap_values2,  color=plt.get_cmap(&quot;cool&quot;))</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 局部特征重要性条形图, 条形是每个特征的 SHAP 值。最左侧的灰色显示数值是特征值</span></span><br><span class="line"><span class="comment">#shap.plots.bar(shap_values2[1], max_display = 12)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制男性和女性特征重要性的全局摘要</span></span><br><span class="line"><span class="comment">#sex = [&quot;Women&quot; if shap_values2[i,&quot;Sex&quot;].data == 0 else &quot;Men&quot; for i in range(shap_values2.shape[0])]</span></span><br><span class="line"><span class="comment">#shap.plots.bar(shap_values2.cohorts(sex).abs.mean(0))</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用Explanation对象的自动群组功能来创建一个群组，调用Explanation.cohorts(N)将创建N个队列，</span></span><br><span class="line"><span class="comment"># 使用 sklearn DecisionTreeRegressor 最佳地分离实例的 SHAP 值，图例中的方括号显示的是每个队列中的实例数</span></span><br><span class="line"><span class="comment">#shap.plots.bar(shap_values2.cohorts(2).abs.mean(0), max_display = 12)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 拟合相对于目标变量 y 的特征 X 的分层聚类模型,可以在SHAP中通过模型损失比较来测量特征冗余</span></span><br><span class="line"><span class="comment"># 计算聚类并传递给条形图，就可以同时可视化特征冗余结构和特征重要性。默认只会显示距离 &lt; 0.5 的聚类部分</span></span><br><span class="line"><span class="comment">#clustering = shap.utils.hclust(X, y) </span></span><br><span class="line"><span class="comment">#shap.plots.bar(shap_values2, clustering=clustering, max_display = 12)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># SHAP Partial dependence plot (PDP or PD plot) 依赖图显示了一个或两个特征对机器学习模型的预测结果的边际效应</span></span><br><span class="line"><span class="comment"># PDP 的一个假设是第一个特征与第二个特征不相关。如果违反此假设，则 PDP 计算的平均值将包括极不可能甚至不可能的数据点</span></span><br><span class="line"><span class="comment"># 参数interaction_index用于设置交互项，用于验证特征之间是否存在交互效应</span></span><br><span class="line"><span class="comment"># Dependence plot 是一个散点图，显示单个特征对整个数据集的影响。每个点都是来自数据集的单个预测（行）。</span></span><br><span class="line"><span class="comment"># x 轴是数据集中的实际值。（来自 X 矩阵，存储在 中shap_values.data）。</span></span><br><span class="line"><span class="comment"># y 轴是该特征的 SHAP 值（存储在 中shap_values.values），它表示该特征值对该预测的模型输出的改变程度</span></span><br><span class="line"><span class="comment">#先看某个特征是如何影响到模型预测结果的</span></span><br><span class="line"><span class="comment">#看到Age越大，个人年收入超过5万美元的可能性越大，但是在65岁后出现波动，80岁后可能性下降</span></span><br><span class="line"><span class="comment">#shap.dependence_plot(&#x27;Age&#x27;, shap_values, X, interaction_index=None)</span></span><br><span class="line"><span class="comment">#相同的图片画法</span></span><br><span class="line"><span class="comment">#plt.figure(figsize=(7.5, 5))</span></span><br><span class="line"><span class="comment">#plt.scatter(X[&#x27;Age&#x27;], shap_values[:, 0], s=10, alpha=1)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 一个特征是如何和另一个特征交互影响到模型预测结果的，这里以Age和Capital Gain为例子</span></span><br><span class="line"><span class="comment"># 散点还是由Age绘制，但是Age的预测其实是有其他特征相互作用的，散点图垂直分散就是由相互作用效应驱动，</span></span><br><span class="line"><span class="comment"># 所以用Capital Gain进行着色以突出显示可能的相互作用</span></span><br><span class="line">shap.dependence_plot(<span class="string">&#x27;Age&#x27;</span>, shap_values, X,  display_features=X_display, interaction_index=<span class="string">&#x27;Capital Gain&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#shap values绘制归因关系是有其他特征的相互作用使图垂直分散的，可以使用shap_interaction_values消除这种相互作用</span></span><br><span class="line">shap_interaction_values = explainer.shap_interaction_values(X)</span><br><span class="line">shap.dependence_plot((<span class="string">&#x27;Age&#x27;</span>, <span class="string">&#x27;Age&#x27;</span>), shap_interaction_values, X, interaction_index=<span class="literal">None</span>)</span><br><span class="line"><span class="comment">#相同的图片画法</span></span><br><span class="line">plt.figure(figsize=(<span class="number">7.5</span>, <span class="number">5</span>))</span><br><span class="line">plt.scatter(X[<span class="string">&#x27;Age&#x27;</span>], shap_interaction_values[:, <span class="number">0</span>, <span class="number">0</span>], s=<span class="number">10</span>, alpha=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># shap.plots.scatte图底部的浅灰色区域是显示数据值分布的直方图</span></span><br><span class="line"><span class="comment"># 在交互颜色方面，散点图则需要将整个 Explanation 对象传递给 color 参数</span></span><br><span class="line"><span class="comment">#shap_values2.display_data = X_display.values</span></span><br><span class="line"><span class="comment">#shap.plots.scatter(shap_values2[:, &quot;Age&quot;], color=shap_values2[:,&quot;Workclass&quot;])</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"># SHAP force plot 提供了单一模型预测的可解释性，可用于误差分析，找到对特定实例预测的解释</span></span><br><span class="line"><span class="string"># 如果不想用JS,需要在shap.force_plot传入matplotlib=True的参数。否则就需要使用shap.initjs()</span></span><br><span class="line"><span class="string"># 模型输出值是-6.75，模型基值是-1.297，绘图箭头下方数字是此实例的特征值，将预测推高的特征用红色表示，将预测推低的特征用蓝色表示</span></span><br><span class="line"><span class="string"># 箭头越长，特征对输出的影响越大。explainer.expected_value是解释模型的常数</span></span><br><span class="line"><span class="string">#shap.initjs() # 初始化JavaScript库</span></span><br><span class="line"><span class="string">#shap.force_plot(explainer.expected_value, shap_values[0,:], X_display.iloc[0,:])</span></span><br><span class="line"><span class="string"># 或者</span></span><br><span class="line"><span class="string">shap.force_plot(explainer.expected_value, shap_values[0,:], X_display.iloc[0,:], matplotlib=True)</span></span><br><span class="line"><span class="string">#其数值表格化如下：</span></span><br><span class="line"><span class="string">sample_0_shap = pd.DataFrame(X.iloc[0,:])</span></span><br><span class="line"><span class="string">sample_0_shap.rename(columns=&#123;0: &#x27;feature_value&#x27;&#125;, inplace=True)</span></span><br><span class="line"><span class="string">sample_0_shap[&#x27;shap_value&#x27;] = shap_values[0]</span></span><br><span class="line"><span class="string">sample_0_shap.sort_values(&#x27;shap_value&#x27;, ascending=False)</span></span><br><span class="line"><span class="string">print(sample_0_shap)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># explainer.shap_interaction_values是实现快速两两交互计算，将为每个预测返回一个矩阵，其中主要影响在对角线上，交互影响在对角线外</span></span><br><span class="line"><span class="comment">#shap.summary_plot(explainer.shap_interaction_values(X), X) # 第一个特征的shap interaction values:explainer.shap_interaction_values(X)[0][0]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Decision plot决策图：SHAP 决策图显示复杂模型如何得出其预测（即模型如何做出决策）</span></span><br><span class="line"><span class="comment"># 决策图中间灰色垂直直线标记了模型的基础值，彩色线是预测，表示每个特征是否将输出值移动到高于或低于平均预测的值。特征值在预测线旁边以供参考。</span></span><br><span class="line"><span class="comment"># 从图的底部开始，预测线显示 SHAP value 如何从基础值累积到图顶部的模型最终分数</span></span><br><span class="line"><span class="comment">#expected_value = explainer.expected_value</span></span><br><span class="line"><span class="comment">#features = X.iloc[range(20)]# 限制20个样本</span></span><br><span class="line"><span class="comment">#shap_values = explainer.shap_values(features)[1]# 展示第一条样本</span></span><br><span class="line"><span class="comment">#features_display = X_display.loc[features.index]</span></span><br><span class="line"><span class="comment">#shap.decision_plot(expected_value, shap_values, features_display)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 决策图支持将对link=&#x27;logit&#x27;数几率转换为概率。</span></span><br><span class="line"><span class="comment"># 使用虚线样式highlight=misclassified突出显示一个错误分类的观察结果</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">shap_values = explainer.shap_values(features)</span></span><br><span class="line"><span class="string">y_pred = (shap_values.sum(1) + expected_value) &gt; 0</span></span><br><span class="line"><span class="string">misclassified = y_pred != y[:20]</span></span><br><span class="line"><span class="string">shap.decision_plot(expected_value, shap_values, </span></span><br><span class="line"><span class="string">                   features_display, </span></span><br><span class="line"><span class="string">                   link=&#x27;logit&#x27;, </span></span><br><span class="line"><span class="string">                   highlight=misclassified)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># 通过单独绘制来检查错误分类的观察结果</span></span><br><span class="line"><span class="string">shap.decision_plot(expected_value, shap_values[misclassified], </span></span><br><span class="line"><span class="string">                   features_display[misclassified],</span></span><br><span class="line"><span class="string">                   link=&#x27;logit&#x27;, </span></span><br><span class="line"><span class="string">                   highlight=0)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># 错误分类观察的力图</span></span><br><span class="line"><span class="string">shap.force_plot(expected_value, shap_values[misclassified], </span></span><br><span class="line"><span class="string">                features_display[misclassified],</span></span><br><span class="line"><span class="string">                link=&#x27;logit&#x27;)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 瀑布图旨在显示单个预测的解释，因此将解释对象的单行作为输入。瀑布图从底部的模型输出的预期值开始，</span></span><br><span class="line"><span class="comment"># 每一行显示每个特征的是正（红色）或负（蓝色）贡献，即如何将值从数据集上的模型预期输出值推动到模型预测的输出值</span></span><br><span class="line"><span class="comment"># waterfall绘图显示单个样本数据</span></span><br><span class="line"><span class="comment">#shap.plots.waterfall(shap_values2[5], max_display = 12)</span></span><br><span class="line"><span class="comment">#shap.plots.scatter(shap_values2[:,&quot;Relationship&quot;])</span></span><br></pre></td></tr></table></figure><h3 id="参考资料："><a href="#参考资料：" class="headerlink" title="参考资料："></a>参考资料：</h3><p>[1] <a href="https://zhuanlan.zhihu.com/p/441302127">用 SHAP 可视化解释机器学习模型的输出实用指南</a><br>[2] <a href="https://zhuanlan.zhihu.com/p/103370775">SHAP的理解与应用</a><br>[3]<a href="https://www.biaodianfu.com/shap.html">机器学习可解释性工具：SHAP </a></p><h3 id="以后的研究注意方向：使用-GPU-加速-SHAP-解释机器学习模型预测"><a href="#以后的研究注意方向：使用-GPU-加速-SHAP-解释机器学习模型预测" class="headerlink" title="以后的研究注意方向：使用 GPU 加速 SHAP 解释机器学习模型预测"></a>以后的研究注意方向：<a href="https://developer.nvidia.com/zh-cn/blog/explain-your-machine-learning-model-predictions-with-gpu-accelerated-shap/">使用 GPU 加速 SHAP 解释机器学习模型预测</a></h3>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
            <tag> 算法 </tag>
            
            <tag> 应用 </tag>
            
            <tag> 可解释 </tag>
            
            <tag> SHAP算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>可解释人工智能及其研究-SHAP算法说明篇</title>
      <link href="/2025/09/16/%E5%8F%AF%E8%A7%A3%E9%87%8A%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%8F%8A%E5%85%B6%E7%A0%94%E7%A9%B6-SHAP%E7%AE%97%E6%B3%95%E8%AF%B4%E6%98%8E%E7%AF%87/"/>
      <url>/2025/09/16/%E5%8F%AF%E8%A7%A3%E9%87%8A%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%8F%8A%E5%85%B6%E7%A0%94%E7%A9%B6-SHAP%E7%AE%97%E6%B3%95%E8%AF%B4%E6%98%8E%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h3 id="SHAP可解释性算法"><a href="#SHAP可解释性算法" class="headerlink" title="SHAP可解释性算法"></a>SHAP可解释性算法</h3><ol><li>算法介绍：SHAP(SHapley Additive exPlanation)，一种基于博弈论思想为载体的、模型无关(可以用于任何机器学习)的、事后机器学习可解释算法。能衡量单次预测结果中的特征贡献，也能聚合局部结果作为对模型的整体解释。其中：<ul><li>SHapley ：代表对每个样本中的每一个特征变量，都计算出它的Shapley Value；</li><li>Additive ：代表对每一个样本而言，特征变量对应的Shapley Value是可加的；</li><li>exPlanation：代表对每个样本的解释，即每个特征变量是如何影响模型的预测值。</li></ul></li></ol><h3 id="算法原理：夏普利值-shapley-value"><a href="#算法原理：夏普利值-shapley-value" class="headerlink" title="算法原理：夏普利值(shapley value)"></a>算法原理：夏普利值(shapley value)</h3><ol><li>算法历史：<ul><li>2012年诺贝尔经济学奖得主LIoyd.S.Shapley在1953年提出的分配方式（论文《A Value for n-Person Games》）。</li><li>2010年， Erik Štrumbelj 和 Igor Kononenko 发表题为《An Efficientterpretation of individual Classification using Game Theory》”的论文，首次提出使用 Shapley 值来解释机器学习模型的预测。</li><li>2017年，Scott Lundberg 和 Su-In Lee 发表题为《A unified approach to interpreting model predictions》的论文，详细介绍了SHapley Additive exPlanations (SHAP)用于机器学习可解释性。</li></ul></li><li>价值：是模型可解释性领域为数不多、理论严谨的算法，是唯一无偏地满足可加性、对称性、冗员性和有效性的指标。</li><li>解决问题：主要用于按照合作各方对合作总目标的贡献程度，按照贡献度分配，避免平均主义，来解决在合作博弈中各方的利益分配问题。</li><li>基础公理：可加性、对称性、冗员性和有效性，LIoyd.S.Shapley证明，原始Shapley值是唯一满足这四条公理的合作博弈解，而且最具合理性和公平性的分配方式也是唯一的。<ul><li>可加性(Additivity)：不同模型在相同输入单元中计算得到的重要性数值是线性可加的；表示在有多种合作时，每种合作的利益分配方式与其他合作结果无关。</li><li>对称性（Symmetry）：始终起到相同作用的两个输入单元，其得到的重要性数值也是相同的；表示合作获利的分配不随每个人在合作中的记号或者次序变化，即博弈顺序不影响博弈收益。</li><li>冗员性(Dummy)：输入样本中的冗员单元的重要性等于其独立作用于神经网络的输出，这里的冗员单元定义为不与其它任何单元相互作用的输入单元；表示如果一个成员对于任何他参与的合作联盟都没有贡献，则他不应该从全体合作中获利。</li><li>有效性（Efficiency 效率）：所有输入单元的重要性之和恰好为神经网络的输出；表示合作各方获利总和等于合作获利。</li></ul></li><li>shapley value和可解释性关系：在机器学习领域，“模型特征”可以用shapley value去刻画每个模型特征对最后模型结果的“贡献”，类似于特征重要性。</li><li>shapley value在机器学习模型解释中的难点：<ul><li>每个子集或联盟收益情况判断：一般我们知道组合最大或最大子集的情况下的收益，无法知道从单个子集到最大子集等所有组合子集的收益情况。</li><li>计算复杂度指数级计算：由于shapley value依赖子集逐个计算，导致shapley value计算复杂度随指数级增长。即$O(2^{M})$,M为子集个数</li></ul></li></ol><h3 id="shapley-value数学原理解释"><a href="#shapley-value数学原理解释" class="headerlink" title="shapley value数学原理解释"></a>shapley value数学原理解释</h3><p>模型的输入特征表示为“玩家”， 对于给定输入基于特征子集$S$进行预测表示“游戏”，特征子集$S$表示“合作”，使用子集$S$时的模型预测值和基线预测值之差被定义为“价值函数”$\upsilon(S)$, $\phi_{i}(f, x)$量化第$i$个特征对预测模型$f(x)$的平均边际贡献，它本身通过考虑所有不含特征$i$的特征子集$S$，计算特征$i$加入S时带来的预测值变化（即边际贡献），然后对所有边际贡献进行加权平均。得到数据表达式如下：<br>$$<br>\phi_{i}(f, x) &#x3D; \sum_{S \subseteq F \setminus  {i} } \frac{|S|! \cdot (|F| - |S| - 1)!}{|F|!} \cdot [\upsilon(S \cup { i}) - \upsilon(S)]<br>$$<br>其中：$F$表示所有特征集合，$|F|$表示总特征数，$S$表示$F$中不包含特征$i$的一个子集，$[\upsilon(S \cup { i})$表示当特征$i$加入特征子集$S$时的模型（或者价值函数）的输出，$\upsilon(S)$表示仅适用特征子集$S$时的模型（或者价值函数）的输出。<br>这个数学公式核心思想：一个特征的重要性取决于它在各种可能的特征组合中加入时，平均能带来多大的预测改变，进而确保按贡献分配的公平性。</p><h3 id="SHAP数学原理解释"><a href="#SHAP数学原理解释" class="headerlink" title="SHAP数学原理解释"></a>SHAP数学原理解释</h3><p>SHAP利用shapley value的原理，在 shapley value原理基础上，增加常数项$\phi_{0}$,则shapley value的含义变为在预测均值的基础上，对预测值所起到的作用。<br>对于单个样本，事后解释模型$g$的表达形式为：<br>$$<br>g(x) &#x3D; \phi_{0} + \sum_{j &#x3D; 1}^{M}\phi_{j}<br>$$<br>其中，$M$是模型中的特征数量，$\phi_{0} $是所有样本预测值的均值。<br>对于特定的预测模型$f(x)$, 其对应的解释模型$g$可以表示为各个特征归因值（SHAP值）的线性组合：</p><p>$$<br>g(z^{\prime}) &#x3D; \phi_{0} + \sum_{j &#x3D; 1}^{M}\phi_{j} z^{\prime}_{j}<br>$$</p><p>其中：$g(z^{\prime})$表示解释模型的输出，$z^{\prime}$是简化的二进制输入向量（$z^{\prime} &#x3D; 1$表示特征$j$“存在”，$z^{\prime} &#x3D; 0$表示特征$j$“缺失”），$M$表示特征总数，$\phi_{0}$表示基线值（通常表示模型在数据集上的平均预测$E[f(X)]$）, $\phi_{j}$表示是特征$j$的SHAP值。</p><p>SHAP值是Shapley值在机器学习和模型解释领域的特定应用。SHAP值不仅继承Shapley值满足四条公理的特性，还具有以下三条对模型解释至关重要的理想性质：（原始模型$f$和解释模型$g$正向相关）</p><ul><li><strong>局部准确性(Local Accuracy)</strong>：对于特定的输入$x$,解释模型$g(z^{\prime})$(当所有特征都“存在”时，即$z^{\prime}$对应于$x$的完整特征表示)的输出必须精确等于原始模型$f(x)$的输出。表示所有特征的SHAP值之和加上基准值$\phi_{0}$等于该实例的预测值$f(x)$。<br>$$<br>f(x) &#x3D; \phi_{0} + \sum_{j &#x3D; 1}^{M}\phi_{j}(当所有z_{j}^{\prime} &#x3D; 1时)<br>$$<br>这保证解释完整性，所有特征的贡献总和恰好等于预测值和基线值之差。</li><li><strong>缺失性(Missingness)</strong>：如果一个特征在简化输入$z^{\prime}$中被视为“缺失”（即$z_{j}^{\prime}&#x3D; 0$），那么其对应的SHAP值$\phi_{j}$在解释模型$g(z^{\prime})$中的贡献应为零(即$\phi_{j}\cdot z^{\prime} &#x3D; 0$)。即如果一个特征$x_{j}$对预测没有贡献，那么他的SHAP值$\phi_{j}$应该是零。</li><li><strong>一致性(Consistency)</strong>：如果一个模型被修改后，某个特征的边际贡献在任何特征组合下都增加或者保持不变，那么该特征的SHAP值也应该增加或者保持不变。该属性保证SHAP值能够可靠的反应特征重要性的真实变化，避免某些方法可能出现的反直觉行为。</li></ul><h3 id="SHAP值难点计算解决方案"><a href="#SHAP值难点计算解决方案" class="headerlink" title="SHAP值难点计算解决方案"></a>SHAP值难点计算解决方案</h3><ol><li>近似算法：<ul><li>模型无关算法：Kernel SHAP</li><li>模型相关算法TreeSHAP（适用于基于树模型的算法，Interventional TreeSHAP和Path-dependent TreeSHAP）、DeepSHAP算法（适用于深度学习模型）</li></ul></li><li>Kernel SHAP算法：在结合LIME可解释算法基础上，使用拟合加权线性模型来估算shapley value。算法五步骤如下（此部分可看参考文献[2] 第161页）：<ul><li><p>采样：采样联盟$z^{\prime}_{k} \in {0,1}^{M}, k \in {1, …, K }$(1 &#x3D; 联盟中存在的特征，0 &#x3D; 不存在的特征)</p></li><li><p>对采样值进行预测：对$z^{\prime}<em>{k}$预测，方法是首先将$z^{\prime}</em>{k}$转换为原始特征空间，然后应用模型$f:f(h_{x}( z^{\prime}_{k}))$。</p></li><li><p>使用SHAP核来计算每个$z^{\prime}_{k}$的权重。其中SHAP核如下：$M$是最大联盟大小，$|z^{\prime}|$实例$z^{\prime}$中当前特征的数量。用这个核权重的线性回归会产生Shapley值。</p></li></ul></li></ol><p>$$<br>\pi_{x}(z^{\prime}) &#x3D; \frac{(M - 1)}{\begin{pmatrix}<br>M\<br>|z^{\prime}|\<br>\end{pmatrix}|z^{\prime}|(M - z^{\prime})}<br>$$</p><ul><li>拟合加权线性模型，可以使用损失函数$L$来训练线性函数$g$:</li></ul><p>$$<br>g(z^{\prime}) &#x3D; \phi_{0} + \sum_{j &#x3D; 1}^{M}\phi_{j} z^{\prime}_{j}<br>$$</p><p>$$<br>L(f, g, \pi_{x}) &#x3D; \sum_{z^{\prime} \in Z}[f(h_{x}(z^{\prime}))-g(z^{\prime})]^{2}\pi_{x}(z^{\prime})<br>$$<br>其中$Z$是训练数据。</p><ul><li>返回shapley值$\phi_{k}$，即线性模型的系数。</li></ul><ol start="4"><li>算法优缺点:<ul><li>算法优点：模型无关性和通用性，理论上适用于任何类型的机器学习模型。</li><li>算法缺点：需要大量扰动样本进行模型评估，计算成本高。要求特征之间必须相互独立，如果特征之间存在高度相关，则可能会出现在现实中不可能出现的数据点，从而对SHAP值的估计产生误差。</li></ul></li></ol><h3 id="算法应用价值"><a href="#算法应用价值" class="headerlink" title="算法应用价值"></a>算法应用价值</h3><ol><li><p>业务领域：对异常变动敏感的业务领域，可以使用SHAP算法进行异常分析，快速定位异常原因，也可以在业务规则之外，提供新的“解释”。</p></li><li><p>模型解释和调试：可以用来解释“黑盒”是AI预测模型，既可以进行局部解释（单点数据样例），也可以实现全局可解释（数据集）。</p></li><li><p>利益分配：合作博弈的核心在于合理公平地分配合作成果，可以SHAP算法用于按照对项目的贡献分配奖金，衡量影响力大小对决策的影响。</p></li><li><p>领域应用：在金融、保险领域，SHAP可用来解释信贷评分、保险理赔反欺诈监测（中国人寿）和隐私合规的要求。分布式可解释算法已在蚂蚁集团内部反欺诈、反洗钱等场景中有落地。</p></li></ol><h3 id="算法工具：SHAP开源算法库"><a href="#算法工具：SHAP开源算法库" class="headerlink" title="算法工具：SHAP开源算法库"></a>算法工具：<a href="https://github.com/slundberg">SHAP开源算法库</a></h3><ol><li>介绍：由Scott Lundberg开发，内置Kernel SHAP&#x2F;Tree SHAP&#x2F;Deep SHAP等多种API.</li><li>安装：PyPI和Conda</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip install shap</span><br><span class="line">or</span><br><span class="line">conda install -c conda-forge shap</span><br></pre></td></tr></table></figure><ol start="3"><li><a href="https://shap.readthedocs.io/en/latest/api.html">API方法</a></li></ol><table><thead><tr><th>命令</th><th>说明</th><th>是否和模型相关</th></tr></thead><tbody><tr><td>shap.Explainer(model[, masker, link, …])</td><td>使用 Shapley 值来解释任何机器学习模型或 python 函数。</td><td></td></tr><tr><td>shap.TreeExplainer(model[, data, …])</td><td>使用树 SHAP 算法来解释集成树模型的输出。</td><td>用于树模型</td></tr><tr><td>shap.GPUTreeExplainer(model[, data, …])</td><td>reeExplainer 的实验性 GPU 加速版本。支持与 TreeExplainer 相同的能力，同时支持基于 GPU 进行加速</td><td>用于树模型</td></tr><tr><td>shap.LinearExplainer(model, masker[, link, …])</td><td>计算线性模型的 SHAP 值，可以选择考虑特征间相关性。</td><td>用于线性模型</td></tr><tr><td>shap.PermutationExplainer(model, masker[, …])</td><td>通过迭代输入的排列来近似 Shapley 值，通过遍历输入的排列变换来估计 Shapely values，能够保证 Local accuracy 性质</td><td>模型无关</td></tr><tr><td>shap.PartitionExplainer(model, masker, *[, …])</td><td>通过特征层次结构递归计算 Shapley 值，该层次结构定义特征联盟并根据博弈论得出 Owen 值。</td><td>模型无关</td></tr><tr><td>shap.SamplingExplainer(model, data, **kwargs)</td><td>使用 Shapley 抽样值解释方法（也称为 IME）的扩展计算 SHAP 值。</td><td>模型无关</td></tr><tr><td>shap.AdditiveExplainer(model, masker[, …])</td><td>计算广义加性模型(GAM) 的 SHAP 值，并假设模型仅具有一阶交叉。</td><td>用于GAM</td></tr><tr><td>shap.DeepExplainer(model, data[, session, …])</td><td>旨在近似深度学习模型的 SHAP 值。shap实现为DeepLIFT 算法（Deep SHAP）的增强版本，与 Kernel SHAP 类似，我们使用选择的背景样本来近似 SHAP 值的条件期望。支持 tf 模型和 torch 模型</td><td>用于深度学习模型</td></tr><tr><td>shap.KernelExplainer(model, data[, …])</td><td>Kernel SHAP 是一种使用特殊的加权线性回归来计算每个特征的重要性的方法。计算的重要性值是来自博弈论的Shapley值以及来自局部线性回归的系数。</td><td>模型无关</td></tr><tr><td>shap.GradientExplainer(model, data[, …])</td><td>使用预期梯度（积分梯度的扩展）解释模型</td><td></td></tr><tr><td>shap.ExactExplainer(model, masker[, link, …])</td><td>通过优化的精确枚举计算 SHAP 值。</td><td>模型无关</td></tr><tr><td>shap.explainers.other.Coefficient(model)</td><td>只需将模型系数作为特征归因返回</td><td></td></tr><tr><td>shap.explainers.other.Random(model, masker)</td><td>仅返回随机（正态分布）特征归因</td><td></td></tr><tr><td>shap.explainers.other.LimeTabular(model, data)</td><td>简单地包裹 lime.lime_tabular。LimeTabularExplainer 到公共 shap 接口中.</td><td></td></tr><tr><td>shap.explainers.other.Maple(model, data)</td><td>只需将 MAPLE 包装到通用 SHAP 接口中即可</td><td></td></tr><tr><td>shap.explainers.other.TreeMaple(model, data)</td><td>只需将 MAPLE 树到通用 SHAP 接口中即可</td><td></td></tr><tr><td>shap.explainers.other.TreeGain(model)</td><td>仅返回树模型的全局增益&#x2F;基尼特征重要性</td><td></td></tr></tbody></table><ol start="4"><li>构建 shap解释器：</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">shap.Explainer(model,  # 要评估的方法或者模型</span><br><span class="line">   masker=None,  # 为 Explainer 提供 mask 后的背景数据</span><br><span class="line">   link=links.identity, # 指定输出结果的单位与 SHAP value 单位的关系，通常使用相同单位，也可以使用log关系</span><br><span class="line">   algorithm=&quot;auto&quot;, # 使用哪种算法来估计Shapely value，通常使用auto来自动选择</span><br><span class="line">   output_names=None, # 模型输出的名称，主要用于后续可视化</span><br><span class="line">   feature_names=None, # 输入中每一列的名称</span><br><span class="line">   linearize_link=True, # 这个代码里没解释……</span><br><span class="line">   seed=None, # 如果要复现结果，指定seed为某个int值</span><br><span class="line">   **kwargs)</span><br></pre></td></tr></table></figure><ul><li>用途：构建SHAP库的主要解释器接口类，能够根据用户传入的 model、masker 等信息，自动的选择合适的 explainer 来作为评估的实例。</li><li>常用参数：<ul><li>model：需要解释的模型对象或者函数对象，用于根据获取到样本数据集计算这些样本的模型输出；</li><li>algorithm：分为“auto”, “permutation”, “partition”, “tree”, or “linear”，表示用于估计 Shapley 值的算法。默认情况下，“auto”选项会尝试在给定传递的模型model和masker的情况下做出最佳选择；</li><li>seed： 随机数种子，用于重复测验。</li></ul></li><li>方法：<ul><li>shap.Explainer().<strong>init</strong>():传递模型构建新解释器；</li><li>shap.Explainer().explain_row()：解释单行并返回由（row_values、row_expected_values、row_mask_shapes、main_effects）构成的元组。抽象方法，需要有子类实现。返回对象是由（row_values， row_expected_values， row_mask_shapes）构成的元祖。 row_values 是每个样本归因值数组，row_expected_values 是表示每个样本模型预期值数组，row_mask_shapes 是所有输入形状的列表；</li><li>shap.Explainer().supports_model_with_masker(model, masker):确定此解释器是否可以处理给定模型，返回是False或True.抽象方法，需要有子类实现；</li><li>shap.Explainer().load(in_file)：从给定文件流in_file中加载解释器；</li><li>shap.Explainer().save(out_file)：将解释器写入给定的文件流out_file中。</li></ul></li></ul><h3 id="SHAP开源算法库可视化交互探索工具"><a href="#SHAP开源算法库可视化交互探索工具" class="headerlink" title="SHAP开源算法库可视化交互探索工具"></a><a href="https://github.com/slundberg">SHAP开源算法库</a>可视化交互探索工具</h3><ol><li><p>可视化交互探索工具按照返回值解释情况分类：</p><ul><li>局部可解释性(Local Interper)：侧重解释单个预测的生成，对单条或者多条样本预测值的解释。可以使用waterfall(瀑布图)&#x2F;decision图&#x2F;force图&#x2F;bar图（柱状图）.</li><li>全局可解释性(Global Interger):得到模型的总体结构(overall structure).</li></ul></li><li><p>瀑布图(Waterfall Plot)：</p><ul><li>命令：shap.plots.waterfall()</li><li>解释：展示了对于单个预测，各个特征的 SHAP 值如何一步步地将模型的预测输出从基线值（通常是平均预测值）推向最终的预测值。</li></ul></li><li><p>力图(Force Plot)：</p><ul><li>解释：以一种“力”的隐喻来可视化单个预测的特征贡献，针对单个样本预测解释，将shap value可视化为force，每个特征增加或减少预测的force值，预测以基线开始，基线是解释模型的常数，每个归因值为一个箭头，增加(正值)或减少(负值)预测，红色为正共享，蓝色为负贡献。红色特征（正 SHAP 值）将预测推高，蓝色特征（负 SHAP 值）将预测拉低，直观地显示了驱动预测结果的关键因素及其相对强度</li><li>命令：shap.force_plot()</li><li>可视化：基线值：图中的起点表示模型的基线值（expected_value）也就是可视化当中的base value,是传入数据集上模型预测值的均值，可以通过自己计算来验证；特征贡献：每个特征的贡献通过带颜色的条表示，条的长度表示该特征对最终预测值的影响大小，红色条表示正向贡献，即该特征使预测值增加，蓝色条表示负向贡献，即该特征使预测值减少；预测值：终点表示模型对该样本的最终预测值，这是基线值加上所有特征贡献的总和.</li></ul></li><li><p>摘要图(Summary Plot &#x2F; Beeswarm Plot):</p><ul><li>解释：这是一种非常强大的全局解释图。它通常将每个特征的 SHAP 值分布以散点图（蜂群图）的形式展示出来，其中每个点代表一个样本的一个特征的 SHAP 值。点的颜色通常表示原始特征值的高低，这有助于揭示特征值与其对预测影响之间的关系。</li><li>命令：shap.summary_plot（）</li><li>作用：摘要图是SHAP常用的一种可视化方法，用于显示特征的重要性和特征值的影响方向，摘要图结合了特征重要性和特征效应图，展示了每个特征的SHAP值的分布情况，帮助我们理解每个特征对模型预测的贡献，这张可视化结果可在众多论文当中看见，当然你也可以通过参数cmap改变配色避免审美疲劳。包括：cmap：”viridis”：从黄色到蓝绿色。”Spectral”：从红色到蓝色，适用于有正负影响的特征。”coolwarm”：从冷到暖的颜色图。”RdYlGn”：从红到绿的颜色图。”RdYlBu”：从红到蓝的颜色图。”RdBu”：红蓝双色图。”RdGy”：红灰双色图。”PuOr”：从紫色到橙色的颜色图。”BrBG”：从棕色到蓝绿色的颜色图。”PRGn”：从紫色到绿色的颜色图。”PiYG”：从粉红色到绿色的颜色图。</li><li>颜色：粉红色点:表示特征值在这个观察模型中对模型预测产生了正面影响。蓝色点:表示该特征值在这个观察中对模型预测产生负面影响。每个样本每个特征的SHAP值，并允许发现预测异常值。每一行代表一个特征，横坐标代表SHAP值。每一个单代表一个样本，颜色代表特征值（红色高，蓝色低）。红色越集中在均值左侧，而蓝色集中在右侧，则说明特征的大小和模型预测值是负相关;而红色越集中在均值右侧，而蓝色集成中均值左侧，说明特征的大小和模型预测值负相关；红蓝混合在一起，则说明特征的噪声大。</li><li>水平轴(shap值)：显示每个特征对预测结果的影响大小，点越远离中心线（零点），表示该特征对模型输出的影响越大正的shap值表示正面影响，负的shap值表示负面影响</li><li>垂直轴(特征排列)：图中垂直排列的特征按影响力从上到小进行排序，上方的特征对模型输出的总影响更大，而下方的特征影响较小</li></ul></li><li><p>特征影响力解释：</p><ul><li>最上方特征(如：MedInc)：显示了大量的正面和负面影响，表明其在不同的观察值对模型预测的结果有很大不同的影响。</li><li>中部特征(如：AveRooms)：也显示出两种颜色的点，但点的分布更集中，影响相对较小</li><li>底部特征(如：Population)：对模型的影响最小，且大部分影响较为接近于零值，表示这些特征对模型预测的贡献较小。</li></ul></li><li><p>特征重要性(Feature Importance)：</p><ul><li>通过散点简单绘制每个样本每个特征的shap值，通过颜色看到特征值大小和预测影响之间的关系，同时取每个特征的shap值的绝对值平均值作为该特征的重要性，可获得标准条形图(shap.summary_plot(plot_type &#x3D; ‘bar’)或者shap.plots.bar())。</li></ul></li><li><p>依赖图 (Dependence Plot):</p><ul><li>解释：用于探索单个特征的取值与其对应的 SHAP 值之间的关系。图中每个点代表一个样本，横轴是特征值，纵轴是该特征的 SHAP 值。通过观察点的分布模式，可以了解特征对其贡献的影响是否是线性的、单调的或其他更复杂的形式</li><li>命令：shap.dependence_plot()</li><li>含义：依赖图用于显示一个特征的SHAP值与该特征值之间的关系，并可以展示特征之间的交互作用</li><li>参数解释：’MedInc’：这是你想要研究的特征名；shap_values：这是通过SHAP计算得到的特征重要性值；X_train：这是用于生成SHAP值的训练数据；interaction_index&#x3D;’AveOccup’：这是指定与主特征（MedInc）交互的特征，SHAP会在图中显示两者的交互效果。</li></ul></li><li><p>热图（Heatmap）</p><ul><li>命令：shap.plots.heatmap()</li><li>可视化解读：左侧y轴为重要性特征排名，特征按影响力从大到小进行排序，右侧y轴为其可视化，图像中颜色深浅表示SHAP值的大小，也就是该特征下值对模型的影响，颜色越深SHAP值绝对值越大越影响模型，顶部为模型在这些数值下的预测结果可视化。</li></ul></li><li><p>使用步骤：</p><ul><li>使用SHAP工具解释，需要先选项合适的解释器(Explainer),使用shap.Explainer时,可以自动选择合适的解释器来作为评估的实例。</li><li>选择Masker合适的背景数据。Masker通常选择经过采样的样本数据(1000条以内，默认100条为主)，主要用于让解释器了解模型所使用的大致数据分布，有利于更加准确的评估每个特征的贡献。对于存在多重共线性的特征，可以先使用shap.util.hclust()方法先聚类或者分组，再分析。</li><li>最后绘图显示特征贡献情况。</li></ul></li></ol><h3 id="SHAP开源算法库使用注意事项"><a href="#SHAP开源算法库使用注意事项" class="headerlink" title="SHAP开源算法库使用注意事项"></a><a href="https://github.com/slundberg">SHAP开源算法库</a>使用注意事项</h3><ol><li>SHAP算法验证时，代码优先使用Jupyter Notebook 或 IPython 环境中运行代码，SHAP算法的图表会在运行后，自动显示。而使用PyCharm时，则不会显示，使用时，需要配合matplotlib库一起使用，配置参数show &#x3D; False, matplotlib &#x3D; True，例如shap.force_plot。</li><li>在PyCharm+matplotlib库一起使用时，单个预测和所有预测所导致的画图差别，在使用K最近邻算法时，对于所有预测，matplotlib库使用shap.force_plot（）画图无效</li><li>数据集使用，shap.datasets.adult()首次运行时会尝试从github上下载数据集并缓存，这容易导致命令在jupyter notebook中可以运行，但是在Pycharm中会报超时连接的错误。</li><li>使用代码时，导入包必须注明版本号，以防因为包的升级进化，导致存在参数和方法失效的问题。</li><li>shap.plots.initjs()是初始化交互式力图所需的javascript库，需要在仅按照IPython笔记本环境中使用。</li></ol><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><p>[1] <a href="https://zh.wikipedia.org/wiki/%E5%8F%AF%E8%A7%A3%E9%87%8B%E4%BA%BA%E5%B7%A5%E6%99%BA%E6%85%A7">可解释人工智能</a><br>[2] <a href="https://book.douban.com/subject/37102010/">可解释机器学习：黑盒模型可解释性理解指南（第2版）</a> 【德】 Christoph Molnar著，郭涛译.电子工业出版社<br>[3] <a href="https://book.douban.com/subject/35693817/">可解释机器学习(模型、方法与实践)</a> 邵平等著.机械工业出版社<br>[4] <a href="https://book.douban.com/subject/35862965/">可解释人工智能导论</a> 杨强等著.电子工业出版社<br>[5] <a href="https://book.douban.com/subject/36059612/">AI可解释性（Python语言版）</a> 列奥尼达·詹法纳(Leonida Gianfagna) &#x2F; 安东尼奥·迪·塞科(Antonio Di Cecco)著.清华大学出版社<br>[6] <a href="https://book.douban.com/subject/36818500/">可解释AI实战（PyTorch版）</a> 阿杰伊·塔姆佩（Ajay Thampi）著.清华大学出版社<br>[7] Feng, T., Zhou, Z., Tarun, J., &amp; Nair, V. N. (2022). <a href="(https://arxiv.org/pdf/2208.06096)">Comparing Baseline Shapley and Integrated Gradients for Local Explanation: Some Additional Insights</a>. arXiv preprint arXiv:2208.06096.<br>[8] <a href="https://book.douban.com/subject/36077228/">Python可解释AI（XAI）实战</a> 丹尼斯·罗斯曼(Denis Rothman)著.清华大学出版社<br>[9] Sundararajan, M., &amp; Najmi, A. (2020, November). <a href="https://proceedings.mlr.press/v119/sundararajan20b.html">The many Shapley values for model explanation</a>. In International conference on machine learning (pp. 9269-9278). PMLR.<br>[10] <a href="https://book.douban.com/subject/37092060/">面向从业者的可解释人工智能</a> Michael Munn著，陈志鸿译.东南大学出版社.<br>[11] <a href="https://zhuanlan.zhihu.com/p/473931620">夏普利值：看诺奖获得者提出的广告效果归因分析新思路</a><br>[12] <a href="https://www.zhihu.com/question/23180647">能不能形象的介绍一下 shapley 值法</a><br>[13] <a href="https://clearcode.cc/blog/game-theory-attribution/">博弈论归因：您可能从未听说过的模型</a><br>[14] <a href="https://zhuanlan.zhihu.com/p/395674023">可解释性：完善Shapley value理论体系，建模并学习基准值</a><br>[15]<a href="https://baike.baidu.com/item/Shapley%E5%80%BC%E6%B3%95/5909624?fr=aladdin">Shapley值法</a><br>[16] <a href="https://shap.readthedocs.io/en/latest/index.html">SHAP 文档</a><br>[17] <a href="https://www.infoq.cn/article/XIYtQjiIC5sPSp04aDK9">打开 AI 的黑盒子：模型可解释性的现状、应用前景与挑战</a><br>[18]  Jialin Wu and Raymond J. Mooney.<a href="http://arxiv.org/abs/1809.02805">Faithful Multimodal Explanation for Visual Question Answering</a>arXiv preprint arXiv:1809.02805<br>[19] <a href="https://www.51cto.com/aigc/5957.html">告别AI“黑箱”！SHAP全面指南，让模型解释不再难</a><br>[20] <a href="https://arxiv.org/pdf/1705.07874">A Unified Approach to Interpreting Model Predictions</a><br>[21] <a href="https://mp.weixin.qq.com/s?__biz=Mzk0NDM4OTYyOQ==&mid=2247484880&idx=1&sn=a86f37b85fe21de3e6797838d8797b15&chksm=c3242942f453a05491b5db2131973c436d3ee789d52fbc93c596eca09d2e51c2be40ba279832&scene=21#wechat_redirect">SHAP全解析：机器学习、深度学习模型解释保姆级教程</a><br>[22]<a href="https://developer.volcengine.com/articles/7418786913525760054">SHAP进阶解析：机器学习、深度学习模型解释保姆级教程</a><br>[23] <a href="https://colab.research.google.com/#scrollTo=5fCEDCU_qrC0">Colab工具</a><br>[24] <a href="https://e0hyl.github.io/BLOG-OF-E0/LIMEandSHAP/#lime-%E5%BA%93%E7%9B%B8%E5%85%B3%E6%8E%A5%E5%8F%A3">可解释方法LIME和SHAP代码实战</a></p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
            <tag> 算法 </tag>
            
            <tag> 可解释 </tag>
            
            <tag> SHAP算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>可解释人工智能及其研究-基础篇</title>
      <link href="/2025/09/16/%E5%8F%AF%E8%A7%A3%E9%87%8A%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%8F%8A%E5%85%B6%E7%A0%94%E7%A9%B6-%E5%9F%BA%E7%A1%80%E7%AF%87/"/>
      <url>/2025/09/16/%E5%8F%AF%E8%A7%A3%E9%87%8A%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%8F%8A%E5%85%B6%E7%A0%94%E7%A9%B6-%E5%9F%BA%E7%A1%80%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<p><strong>简单来说，可解释性是指一个人能够理解一个决定的原因的程度。</strong></p><h3 id="模型可解释性"><a href="#模型可解释性" class="headerlink" title="模型可解释性"></a>模型可解释性</h3><ol><li><p>也称可解释机器学习，是指对模型内部机制的解释以及对模型结果的解释。而更广泛定义认为：模型能用通俗易懂的语言进行表达，是一种被人类理解的能力，即能够将模型的预测过程转化为具备逻辑关系的规则的能力。</p></li><li><p>作为数据科学家，我们在运用模型过程中，不仅要防止模型偏见问题的发生，还要能解释模型是如何正确的产出结果的，进而正确的使用模型，越是重要严苛的应用场景，越需要说明模型是如何运作的，并且展示避免偏见和错误的证据。</p></li></ol><h3 id="可解释性人工智能分类-Explainable-AI-XAI"><a href="#可解释性人工智能分类-Explainable-AI-XAI" class="headerlink" title="可解释性人工智能分类(Explainable AI, XAI)"></a>可解释性人工智能分类(Explainable AI, XAI)</h3><ol><li><strong>建模前可解释性，也称基于数据的可解释性</strong>：<ul><li>目标是可解释的数据探索、基于统计分析的辅助决策。</li><li>特点是数据分析和可视化。</li><li>技术栈包括数据可视化技术、可解释的特征工程、聚类、降维和统计数据分析等，在sklearn开源机器学习库中，很多模型中有importance接口(<a href="https://scikit-learn.org/stable/modules/permutation_importance.html">Permutation feature importance</a>)，通过查看模型特征的重要性，来体现模型的可解释性。</li></ul></li><li><strong>内在可解释模型，也称基于模型内在的可解释性（Intrinsic Interpretability</strong>）：<ul><li>目标是使用和开发内在可解释的模型</li><li>特点使模型本身变得可解释，模型本身就可以告知为什么这么做，模型不只给答案，还要给出得到这个答案的原因。</li></ul></li></ol><p>内在可解释模型技术栈：</p><ol><li><p>包括explanation generation、prototype netwrok, explanatory graph，使用可解释性的机器学习方法。优化模型增强可解释性（如优化后的深度神经网络）、基于图的可解释性（知识图谱等）等，其中：</p></li><li><p>explanation generation典型方法VQA  explanation：在训练模型的同时也训练一个模型对应的语言解释器。这样既得到Answer，也得到了Explanation。详细方法可以看这个论文：<a href="https://arxiv.org/abs/1809.02805">Faithful Multimodal Explanation for Visual Question Answering</a></p></li><li><p>prototype netwrok典型方法：在模型设计的时候，按照仿生学的方法，让模型构造出的数据加工方式和人类自身思考方式类似，在产生结果结合工作方式，进而理解结果产生的原因。详细方法可以看这个论文：<a href="https://arxiv.org/abs/1806.10574">This Looks Like That: Deep Learning for Interpretable Image Recognition</a></p></li><li><p>广义加性可解释神经网络模型（GAMxNN模型，Explainable Neural Network based on Generalized Additive Model），该模型提供整体和局部可解释性，并用数据可视化的方式呈现。</p></li><li><p>基于具有结构化交互作用的广义加性可解释神经网络模型（GAMINET模型，An explainable neural network based on generalized additive models with structured interactions）:张军爱教授团队提出，在GAMxNN模型基础上结合特征交互项的研究，对GAMxNN模型做进一步改良和优化。其数学形式如下：<br>$$<br>g(E(y|x)) &#x3D; \mu + \sum_{j \in S_{1}} h_{j}(x_{j}) + \sum_{(j, k) \in S_{2}} f_{jk}(x_{j}, x_{k})<br>$$<br>其中$\mu$表示截距项，$S_{1}$表示主效应集合， $S_{2}$表示交互效应集合，表示式右边第二项为单个特征拟合岭函数加和，右边第三项为交互特征拟合函数加和，这里假设每个主效应和成对交互效应的平均值为零。详情请看论文：<a href="https://www.sciencedirect.com/science/article/pii/S0031320321003484#bib0009">GAMI-Net: An explainable neural network based on generalized additive models with structured interactions</a>，模型源码请看：<a href="https://github.com/ZebinYang/gaminet">https://github.com/ZebinYang/gaminet</a> 。</p></li><li><p>可解释增强机（EBM模型， Explainable Boosting Machine）：是可解释性高的广义加法模型（GAM）中的一种，其和GAMINET模型的重要差别在于，其抓取特征的函数关系，使用的boosting方法，而非神经网络。其数学形式如下：<br>$$<br>g(E[y]) &#x3D;\beta_0 +  \sum f_{j}(x_{j})<br>$$<br>其中$g$是使广义加法模型（GAM）适应不同设置（例如回归或分类）的链接函数</p></li><li><p><strong>建模后可解释性，也称基于结果或者事后的可解释性（Post-hoc Interpretability）</strong>：</p><ul><li>目标是通过假设检验，去估计、推断和验证模型决策的流程</li><li>特点是和模型无关和黑盒分析，通过观测模型的行为，去判断为什么产生这样的结果，进而建模其可解释性。</li><li>技术栈包括：Surrogate model、additive feature、attribution methods、Saliency map、局部依赖图、特征归因方法和代理模型等。其中，Surrogate model（代理模型）典型方法：在模型局部采用一种简单可解释的模型去近似原有的黑盒模型，当精度足够逼近的时候，在用代理模型来解释原黑盒模型。典型算法是VI（变量重要性，Variable Importance）、PDP（Partial Dependence Plot，部分依赖图）、ICE（Individual Conditional Expectation Plot，个体条件期望图）、<a href="https://mp.weixin.qq.com/s/fEy1A0IjwQC46JS6PBIByA">ALE</a>（累积局部效应图，Accumulated Local Effects plot）、<a href="https://arxiv.org/abs/1602.04938">LIME(Local Interpretable Model-Agnostic Explanations)</a>算法（github码源：<a href="https://github.com/marcotcr/lime%EF%BC%89%E5%92%8C[SHAP">https://github.com/marcotcr/lime）和[SHAP</a>(Shapley Additive Explanations)](<a href="https://arxiv.org/abs/1705.07874)%E7%AE%97%E6%B3%95%E3%80%82">https://arxiv.org/abs/1705.07874)算法。</a></li></ul></li></ol><p>根据是否是局部还是全局，也可以分为：</p><ol><li>局部可解释性(Local Interpretable)：当一个样本或者一组样本的输入值发生变化时，需要解释其预测结果发生的变化原因。</li><li>全局可解释性(Global Interpretable)：在基于完整数据集的整个模型从输入到输出的理解解释，可以从中得到普遍规律和统计判断，理解每个特征对模型的影响。</li></ol><p>一般认为模型可解释性和模型准确性不可兼得：简单的模型容易解释，但拟合效果不好；复杂的模型效果好，但是却不容易解释。</p><h3 id="当前模型可解释性方法的挑战和问题"><a href="#当前模型可解释性方法的挑战和问题" class="headerlink" title="当前模型可解释性方法的挑战和问题"></a>当前模型可解释性方法的挑战和问题</h3><ol><li>算法成熟度：基于模型内在的可解释性和模型、场景绑定，通用性受限。基于结果或者事后的可解释性使用的算法本身是模型的近似，存在对采样的依赖，结果不一定稳定的问题。</li><li>算力成本：基于结果或者事后的可解释性的算法，其算法复杂度太高，算力成本现对较高。其中KernelSHAP算法就比较慢，特别是涉及多实例计算Shapley值的过程中。、</li><li>数据匮乏：基于模型内在的可解释性训练出解释器的过程，是有监督的训练过程，依赖样本和标准数据，而这类数据比较稀缺。</li></ol><h3 id="AI模型在应用场景的典型使用问题"><a href="#AI模型在应用场景的典型使用问题" class="headerlink" title="AI模型在应用场景的典型使用问题"></a>AI模型在应用场景的典型使用问题</h3><ol><li>无法挖掘因果关系或者是因果关系错判：黑盒模型内部结构复杂，使用黑盒模型做预测时，我们会根据一些模型的评价指标（如AUC）去评估模型的好坏，但即使AUC很高，我们也依然不清楚黑盒模型的判断依据是否正确。如果模型无法给出合理的因果关系，那么模型的结果也将很难使人信服。</li><li>模型安全问题：模型安全问题是指人工智能模型在训练、部署和使用过程中面临的各种安全风险，包括数据泄露、模型被滥用（如用于欺诈、虚假信息生成）、输出错误（如模型幻觉）、数据投毒、以及系统漏洞等。这些风险可能导致商业机密失窃、用户隐私受损、产生偏见歧视，甚至引发社会混乱。这些问题导致模型大范围应用在敏感领域应用举步维艰。</li><li>模型偏见问题：要是指模型在生成内容或做出决策时存在的某种偏好或倾向，这些偏好或倾向往往是由于训练数据的不平衡、不完整性或社会文化背景等因素导致的。这些问题的出现也导致模型在应用过程中，对模型结果的怀疑。</li></ol><h3 id="模型可解释性价值"><a href="#模型可解释性价值" class="headerlink" title="模型可解释性价值"></a>模型可解释性价值</h3><p>根据Gartner2019企业年度调查报告《人工智能治理三基石：可信、透明和多样性》可知，人工智能系统存在三难点：选择训练数据集带来的机器学习困局（多样性问题）、决策精度差异（结果是否可解释并可信）和恰到好处的可接受结果。可解释就是回答“why”的问题</p><p>可解释性对模型应用的价值，从模型的生产周期来看，可以分为模型开发、模型运行和模型推广阶段：</p><ol><li>在模型开发阶段，模型问题定位和使用安全。由于数据和应用场景的限制和变化，模型无法做到绝对精准，因而无法保证结果的绝对安全性，而对复杂模型结构和参数调优，犹如玄学炼丹。模型可解释性有助于在新场景和新数据样本进入时，判断模型的使用条件和依据，也有助于模型发生错误时，可以及时定位问题，采取针对性的优化措施。</li><li>在模型运行阶段：建立信任，坚定信心。模型可解释性最大价值在于建立信任，使用者通常不会简单地要求模型表现好，更在于能有理有据的给出推理依据，再给出模型结论，这样说服力更强，结论更容易接受，使人类相信模型的判断，提升模型可信度，实现业务的推广。</li><li>在模型推广阶段：探索因果关系。当前模型拟合绝大多数依据误差最小化标准，模型本身更加擅长挖掘相关关系，而非因果关系，模型可解释性可以通过解读相关关系，对其中的特征重要性评估，探索出相关关系下深层次的因果规律，避免因为数据分布不均导致的“<a href="https://zhuanlan.zhihu.com/p/466660368">辛普森悖论</a>”问题，进而定位出真正的根因原因，确保模型学习到合理知识，同时给具体的业务带来指导。避免偏见和法律合规。模型的结果需要符合业务合规标准，而仅仅依靠训练样本数据得到的结果可能是存在偏见。另一方面，欧盟GDPR条例等法律法规要求，模型做出解释，让使用者知道模型的决策是如何影响他们的。模型可解释性可以辅助用户判断模型结果是否合规，是否符合预期，进而决定是否接受模型使用和模型结论，使得模型所学能反哺人类。</li></ol><h3 id="模型可解释性工具：lnterpretML"><a href="#模型可解释性工具：lnterpretML" class="headerlink" title="模型可解释性工具：lnterpretML"></a>模型可解释性工具：<a href="https://interpretml.com.cn/docs/">lnterpretML</a></h3><ol><li>介绍：开源，模型可解释Python框架</li><li>作者：微软研究院</li><li>授权协议：MIT</li><li>来源论文：<a href="https://arxiv.org/abs/1909.09223">InterpretML: A Unified Framework for Machine Learning Interpretability</a></li><li>github仓库：<a href="https://github.com/interpretml/interpret">https://github.com/interpretml/interpret</a></li><li>功能：提供机器学习可解释性算法，供研究者使用.InterpretML 能提供两种类型的可解释性：<ul><li>白盒（glassbox），这是针对可解释性设计的机器学习模型（比如线性模型、决策规则、决策树、可解释增强机、广义加性模型）。</li><li>黑箱（blackbox）可解释技术，用于解释已有的系统（比如部分依赖图、LIME解释器、SHAP解释模型、莫里斯敏感性分析）。</li></ul></li><li>优点：<ul><li>模型可解释性：帮助数据科学家等业务相关者了解机器学习模型。</li><li>易用性：提供统一API接口和丰富可视化的可解释性技术。</li><li>灵活可定制：通过解释器和交互式视觉技术来理解模型。</li><li>综合能力：可探索模型属性，在操作数据时可以查看对模型的影响，进行假设检验分析。</li></ul></li></ol><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><p>[1]  <a href="https://zh.wikipedia.org/wiki/%E5%8F%AF%E8%A7%A3%E9%87%8B%E4%BA%BA%E5%B7%A5%E6%99%BA%E6%85%A7">可解释人工智能</a><br>[2]  <a href="https://book.douban.com/subject/37102010/">可解释机器学习：黑盒模型可解释性理解指南（第2版）</a> 【德】 Christoph Molnar著，郭涛译.电子工业出版社<br>[3]  <a href="https://book.douban.com/subject/35693817/">可解释机器学习(模型、方法与实践)</a> 邵平等著.机械工业出版社<br>[4]  <a href="https://book.douban.com/subject/35862965/">可解释人工智能导论</a> 杨强等著.电子工业出版社<br>[5]  <a href="https://book.douban.com/subject/36059612/">AI可解释性（Python语言版）</a> 列奥尼达·詹法纳(Leonida Gianfagna) &#x2F; 安东尼奥·迪·塞科(Antonio Di Cecco)著.清华大学出版社<br>[6]  <a href="https://book.douban.com/subject/36818500/">可解释AI实战（PyTorch版）</a> 阿杰伊·塔姆佩（Ajay Thampi）著.清华大学出版社<br>[7]  Feng, T., Zhou, Z., Tarun, J., &amp; Nair, V. N. (2022). <a href="(https://arxiv.org/pdf/2208.06096)">Comparing Baseline Shapley and Integrated Gradients for Local Explanation: Some Additional Insights</a>. arXiv preprint arXiv:2208.06096.<br>[8]  <a href="https://book.douban.com/subject/36077228/">Python可解释AI（XAI）实战</a> 丹尼斯·罗斯曼(Denis Rothman)著.清华大学出版社<br>[9]  Sundararajan, M., &amp; Najmi, A. (2020, November). <a href="https://proceedings.mlr.press/v119/sundararajan20b.html">The many Shapley values for model explanation</a>. In International conference on machine learning (pp. 9269-9278). PMLR.<br>[10]  <a href="https://book.douban.com/subject/37092060/">面向从业者的可解释人工智能</a> Michael Munn著，陈志鸿译.东南大学出版社<br>[11]  <a href="https://zhuanlan.zhihu.com/p/473931620">夏普利值：看诺奖获得者提出的广告效果归因分析新思路</a><br>[12]  <a href="https://www.zhihu.com/question/23180647">能不能形象的介绍一下 shapley 值法</a><br>[13]  <a href="https://clearcode.cc/blog/game-theory-attribution/">博弈论归因：您可能从未听说过的模型</a><br>[14]  <a href="https://zhuanlan.zhihu.com/p/395674023">可解释性：完善Shapley value理论体系，建模并学习基准值</a><br>[15]  <a href="https://baike.baidu.com/item/Shapley%E5%80%BC%E6%B3%95/5909624?fr=aladdin">Shapley值法</a><br>[16]  <a href="https://shap.readthedocs.io/en/latest/index.html">SHAP 文档</a><br>[17]  <a href="https://www.infoq.cn/article/XIYtQjiIC5sPSp04aDK9">打开 AI 的黑盒子：模型可解释性的现状、应用前景与挑战</a><br>[18]   Jialin Wu and Raymond J. Mooney.<a href="http://arxiv.org/abs/1809.02805">Faithful Multimodal Explanation for Visual Question Answering</a>arXiv preprint arXiv:1809.02805<br>[19]  <a href="https://mp.weixin.qq.com/s/fEy1A0IjwQC46JS6PBIByA">事后模型归因解析Part 1</a><br>[20]  A, Z. Y. ,  B, A. Z. , &amp;  B, A. S. . (2021). Gami-net: an explainable neural network based on generalized additive models with structured interactions. #i{Pattern Recognition}.<br>[21]  <a href="https://interpret.ml/docs/ebm.html">Explainable Boosting Machine</a><br>[22]  Trevor Hastie and Robert Tibshirani. Generalized additive models: some applications. Journal of the American Statistical Association, 82(398):371–386, 1987.<br>[23]  <a href="https://hub.baai.ac.cn/view/30376">机器学习模型可解释性的综述</a><br>[24]  <a href="http://www.cs.hit.edu.cn/_upload/article/files/77/15/b48ec9654aaf9df9094b92e164a6/ef88ed43-925a-43d6-9375-c81a3f395136.pdf">机器学习的可解释性综述</a><br>[25]<a href="https://cloud.tencent.com/developer/article/1937611">机器学习的挑战：黑盒模型正面临这3个问题</a></p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
            <tag> 算法 </tag>
            
            <tag> 可解释 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>统计指数理论及其应用</title>
      <link href="/2025/09/16/%E7%BB%9F%E8%AE%A1%E6%8C%87%E6%95%B0%E7%90%86%E8%AE%BA%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/"/>
      <url>/2025/09/16/%E7%BB%9F%E8%AE%A1%E6%8C%87%E6%95%B0%E7%90%86%E8%AE%BA%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<h2 id="统计指数"><a href="#统计指数" class="headerlink" title="统计指数"></a>统计指数</h2><p>含义：综合反应不能直接相加，不能直接对比多种因素组成的经济现象在不同时间和空间条件下变动的相对数，是一种经济分析方法，反应研究对象的变化趋势，可用于分析度量。<br>常见指数：70个大中城市新建商品住宅销售价格指数、<a href="https://www.stats.gov.cn/zs/tjws/zytjzbqs/jmxxggzs/202409/t20240910_1956353.html">居民消费价格指数(CPI)</a>、<a href="https://www.stats.gov.cn/zs/tjws/zytjzbqs/gyscz/202409/t20240910_1956355.html">工业生产者出厂价格指数(PPI)</a>、沪深300证券指数、<a href="https://www.spglobal.com/spdji/zh/methodology/article/sp-china-indices-methodology-chinese/">标准普尔中国PMI指数(财新PMI)</a>、<a href="https://www.sse.net.cn/indexIntro?indexName=scfi">上海出口集装箱运价指数（SCFI）</a>、摩根士丹利社会感知指数(Social Perception Index)。</p><h3 id="价格指数"><a href="#价格指数" class="headerlink" title="价格指数"></a>价格指数</h3><ol><li>含义：反应一定时期内价格水平变动趋势和变动程度的相对数，可支持基于不同颗粒度的分析管理。</li><li>作用：结合分析对象、分析视角、计算度量等因素，综合反应历史价格趋势、预判未来盈利风险、支撑综合业务决策，通过对比同行同业指数数据，评估产品或公司竞争力。</li><li>分类：<ul><li>按比较的基期不同，指数分为定基指数和环比指数。定基指数指在数列中以某一固定时期的水平作对比基准的指数。环比指数则是以其前一时期的水平作为对比基准的指数。</li><li>按计算形式的不同，指数分为综合指数和平均数指数。综合指数是指两个总量指标对比计算出来的指数。平均数指数是依据非全面统计资料编制，实质是综合指数的变形，平均数指数可分为算术平均数指数、调和平均数指数和几何平均数指数。</li><li>按所反映现象的范围不同，指数分为个体指数和综合指数。个体指数是表明某单一要素构成现象变动的相对数，如个别产品的物量指数和个别商品的价格指数等。综合指数是表明多种要素构成现象的综合变动的相对数，如社会消费品零售总额指数和居民消费价格指数等。</li><li>按所反映现象的性质不同，指数分为数量指数和质量指数。数量指数也称物量指数，是表明总体单位数量、规模等数量变动的相对数，如产品产量指数和商品销售量指数等。质量指数是表明总体单位水平、工作质量等质量变动的相对数，如各种价格指数、单位成本指数和劳动生产率指数等。</li></ul></li></ol><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><ol><li>基期：基是指统计基数，即制定一个日期作为参考标准，是一开始作为基准的时期，使用基期概念是为了将开始时间定位，作为对比基准。</li><li>报告期：就是当前研究的阶段，是当期的意思，就是把现在研究的这一期和那一期进行对比。报告期与基期是一对指标，一般报告期都是针对基期而言的。</li><li>权数及权重：权数是用来衡量总体中各构成部分对总体影响程度的数值。综合指数或者平均数的构成变量中，每个变量对综合指数或平均数的影响程度就是这个变量的权数。从表现形式上来看，权数通常可以表现为绝对数形式权数（频数）和相对数形式权数（频率）。其中，相对数是用绝对数计算出来的百分数（％）或千分数（‰）表示，又称比重或权重，通常情况下权重之和等于1。<br>中国CPI（居民消费价格指数）以前有“中国猪肉指数（China Pig&#x2F;Pork Index）”的外号，就是因为猪肉是测算CPI的一篮子商品中权重最大的单品（约占2.5%），对CPI的拉动效果极为明显。</li><li>构建综合指标体系时，各级权数选取常用到以下几种方法：<ul><li>主观经验法，根据测算对象的分项结构利用主观经验进行赋值。</li><li>专家咨询法，即邀请专家讨论修改各级指标权重意见，直到意见趋于一致得出合理的赋权方案。</li><li>层次分析法，即将一个总目标分解为多级指标，在同层级内根据各指标重要性计算每项指标的相对优先权重。</li><li>多元分析法，利用主成分分析、因子分析等统计方法计算各权数。这几种方法既可单独使用，也可配合使用。</li></ul></li><li>代表规格品：简单来说，就是价格变动趋势和变动程度有较强的足够的代表性的产品或服务。</li></ol><h3 id="典型指数计算方法"><a href="#典型指数计算方法" class="headerlink" title="典型指数计算方法"></a>典型指数计算方法</h3><p>拉式指数公式$L_{p}$, 派式指数公式$P_{p}$</p><ol><li>同度量因素：拉式为基期销量，派式为报告器销量。</li><li>权重：拉式权重固定在基期，单纯反应价格的变动；派式权重固定在报告期，即反应价格的变化，又能反应量的变化。<br>$$<br>L_{p} &#x3D; \frac{\sum p_{1}q_{0}}{\sum p_{0}q_{0}}&#x3D; \sum \frac{p_{1}}{p_{0}} \frac{p_{0}q_{0}}{\sum p_{0}q_{0}} &#x3D; \sum \frac{p_{1}}{p_{0}}w_{0}<br>$$</li></ol><p>$$<br>P_{p} &#x3D; \frac{\sum p_{1}q_{1}}{\sum p_{0}q_{1}}&#x3D; \sum \frac{p_{1}}{p_{0}} \frac{p_{0}q_{1}}{\sum p_{0}q_{1}} &#x3D; \sum \frac{p_{1}}{p_{0}}w_{1}<br>$$<br>其中：下角标0表示基期，1表示报告期。<br>3. 注意事项：<br>     - 基期不一定是某个具体日期（时点，如1月1日），基期与报告期可能是一个时点，也可能是一个时段（时期，如某年、某月等），这取决于研究对象是时期指标还是时点指标。<br>     - 计算价格指数有“拉氏”和“派氏”两种加权公式，区别在于把“一篮子”商品和服务的消费数量固定在对比期还是报告期。固定在对比期的是拉氏公式，固定在报告期的是派氏公式.世界上绝大多数国家在编制CPI时均选择了拉氏公式，一是计算价格指数时通常无法及时取得报告期的消费数量，难以编制派氏指数。二是拉氏指数是假定消费数量不变，以观察和比较消费价格的变动情况，更具经济学意义。<br>     - 对于权数变动，按照拉氏公式理论，计算定基指数时，对比期为基期年，权数即为基期年购买固定篮子内各类商品或服务所支出金额占购买整个篮子所支出总金额的比重。这个权数是固定不变的，在《消费者价格指数手册：理论与实践》里，它被称为原权数。而在计算月度环比指数时，对比期为上月，权数即为上月购买固定篮子内各类商品或服务的支出金额占总支出的比重；计算同比指数时，对比期为上年同月，权数即为上年同月购买固定篮子内各类商品或服务的支出金额占总支出的比重。很显然，环比和同比指数的权数是随着每月价格变化自动调整的，在《消费者价格指数手册：理论与实践》里，它被称为按价格调整的权数。权数虽然在变化，但权数所代表的“一篮子”商品和服务的消费数量却是相同的、固定的，也就是“固定篮子”。</p><h3 id="指数计算公式解读"><a href="#指数计算公式解读" class="headerlink" title="指数计算公式解读"></a>指数计算公式解读</h3><p>$$<br>I_{price} &#x3D; \sum^{Top N}\frac{p_{1}}{p_{0}}w_{x}<br>$$</p><ol><li>代表标准或规格品${Top N}$ : 代表标准或规格品的筛选标准可以采用统计方法筛选、业务专家意见和定期审视刷新的策略，其中数据选择标准需要有代表性，覆盖广，数据本身要连续可得，同质可比。</li><li>基期和报告期(下角标0和1)：0：代表基期单价，为指数的初始计算时间点；1，代表报告期单价，为指数报告的时间点。对于研究对象波动大的，基期时距需要相对短一些；波动相对稳定的，则基期时距应该相对长一些。其中基期和报告期的选择标准，可以从负责编制的目的，考虑波动程度，再选择数据稳定的时期。</li><li>权重$w_{x}$:针对场景选择$x$,如果$x$取基期，则是固定权重的拉式指数；如果$x$为一段时间，则为相对权重的拉式变形指数；如果$x$取报告期，则为变动权重的派式指数。权重的选择标准，可以从内容选择要服从研究目的，形式选择要满足资料可得，时期选择要考虑业务经济意义考虑。</li><li>计算公式$I_{price}$：计算公式的选择标准，需要考虑加权指数为基本形式，计算结果有业务意义，计算简明，结果敏感。</li></ol><h3 id="指数举例：-70城市房价指数"><a href="#指数举例：-70城市房价指数" class="headerlink" title="指数举例： 70城市房价指数"></a>指数举例： <a href="https://www.stats.gov.cn/sj/zxfb/202507/t20250715_1960403.html">70城市房价指数</a></h3><ol><li>发布机构：中国国家统计局。</li><li>发布时间：每月15号左右发布上月数据。</li><li>数据覆盖范围：70个大中城市，覆盖一线、二线和三线城市。</li><li>数据分类：<ul><li>新建商品住宅：反应开发商定价趋势。</li><li>二手住宅：反应真实市场交易情况，更敏感。</li></ul></li><li>数据来源：房企网签数据和房产中介上报数据。</li><li>统计方法：加权平均法，具体统计方法参见<a href="https://jszd.stats.gov.cn/TrueCMS//gjtjjjsdczd/bbzd/content/6bffa669-d9c7-4e87-a4a3-fec88e612a1b.html">《房地产价格统计报表制度》</a></li><li>数据关键指标：<ul><li>定基指数：以2015年价格为基测（100），观测累计涨幅。</li><li>环比（MoM）：本月对比上月，反应短期波动情况。</li><li>同比（YoY）：本月对应去年同期，反应长期趋势。</li></ul></li><li>场景和判断：<ul><li>二手房价涨幅大于新房时，说明市场由真实需求驱动（而非开发商业务促销），可以视为房价企稳的重要判断，或者说是进入右侧交易。</li><li>房价连续3个月环比涨幅超过1%，有可能会触发政府限购加码；而同比下跌超过5%，政府可能出台救市政策。</li><li>《逃不开的经济周期》一书中认为，房地产是经济周期之母，需要格外研究。</li></ul></li></ol><h3 id="指数计算应用经验"><a href="#指数计算应用经验" class="headerlink" title="指数计算应用经验"></a>指数计算应用经验</h3><ol><li>指数可以做为引子，借助指数我们可以计算并获取很多从其他视角的其他信息，辅助决策。例如为了计算指数，需要获得权重和均价，借此可以建立不同产品的权重图和价格图，而价格一般的是通过金额和数量计算出来的，由此可以建立不同视角下的金额热力图、占比图或者数量图、分布图等。由于计算指数需要依赖标准或规格品，在挑选标准或规格品过程中，可以按照总量或数量获取前90%等的产品清单，进而实现通过对20%产品的指数监控实现80%左右的收益(二八法则)。</li><li>如果产品之间存在组成和传递关系，可以建立产品之间的量化关系，分析产品之间的波动规律十分合理，也可以通过指数形成波动信息的传导，并将多种指数可以放置在一个图中对比分析其中的gap,分析其中的偏差原因。</li></ol><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><p>[1]<a href="https://book.douban.com/subject/6061753/">统计指数理论, 方法与应用研究</a>，徐国祥等著. 上海人民出版社<br>[2] <a href="https://book.douban.com/subject/4932567/">统计指数理论及应用</a>，徐国祥著. 中国统计出版社.<br>[3] 统计指数理论与实践， 任建智著. 中国物资出版社<br>[4] <a href="https://www.douban.com/note/866626990/?_i=5827726a4yBb1W">领导干部应知应会主要统计指标诠释</a>. 本书编写组编.中国统计出版社.<br>[5] <a href="https://book.douban.com/subject/36473131/">统计预测和决策</a>.徐国祥著.上海财经大学出版社<br>[6] <a href="https://book.douban.com/subject/2329672/">国民经济核算原理与中国实践</a>.高敏学著.中国人民大学出版社<br>[7]<a href="https://www.stats.gov.cn/zs/tjws/zytjzbqs/zzxsjgzs/202411/t20241128_1957596.html">住宅销售价格指数的编制</a> .中国国家统计局<br>[8] <a href="https://www.stats.gov.cn/zs/tjws/zytjzbqs/zzxsjgzs/202501/t20250121_1958389.html">住宅销售价格数据的基础数据来源</a>.中国国家统计局<br>[9] <a href="https://www.stats.gov.cn/zs/tjws/jbtjzswd/tjbk/202507/t20250711_1960387.html">什么是统计数据的报告期和基期</a><br>[10] <a href="https://www.stats.gov.cn/zs/tjws/tjbk/202301/t20230101_1912948.html">什么是指数</a><br>[11] <a href="https://www.stats.gov.cn/zs/tjws/tjbk/202301/t20230101_1912959.html">什么是权数</a><br>[12] <a href="https://www.stats.gov.cn/zs/tjws/tjzb/202301/t20230101_1903623.html">住宅销售价格指数是如何编制的</a><br>[13] <a href="https://www.stats.gov.cn/zs/tjws/tjzb/202301/t20230101_1903972.html">什么是采购经理指数（PMI）</a><br>[14] <a href="https://www.stats.gov.cn/zs/tjws/tjzb/202301/t20230101_1903757.html">居民消费价格指数（CPI）是如何编制的</a><br>[15] <a href="https://www.stats.gov.cn/zs/tjws/zytjzbqs/cgzlzs/202501/t20250121_1958396.html">采购经理指数编制方法</a><br>[16] <a href="https://www.stats.gov.cn/zs/tjws/zytjzbqs/jmxxggzs/202411/t20241127_1957589.html">CPI 的编制方法</a><br>[17] <a href="https://tjyj.cbpt.cnki.net/portal">统计研究</a><br>[18] <a href="https://book.douban.com/subject/11516006/">消费者价格指数手册：理论与实践</a><br>[19] <a href="https://book.douban.com/subject/35587406/">宏观经济数据分析手册</a></p>]]></content>
      
      
      <categories>
          
          <category> 数学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
            <tag> 数学 </tag>
            
            <tag> 统计 </tag>
            
            <tag> 指数理论 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FastAPI使用注意事项</title>
      <link href="/2025/09/15/FastAPI%E4%BD%BF%E7%94%A8%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/"/>
      <url>/2025/09/15/FastAPI%E4%BD%BF%E7%94%A8%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/</url>
      
        <content type="html"><![CDATA[<h2 id="FastAPI"><a href="#FastAPI" class="headerlink" title="FastAPI"></a>FastAPI</h2><p>介绍：高性能、自动交互式文档生成、支持类型注解、支持异步请求和RESTful API的Web框架，兼容基于ASGI的其他框架，例如Pydantic。<br>作用：可以用于构建前后端分离和微服务的Web应用，构建RESTful API； 接收、返回和处理JSON数据。</p><p>安装：</p><ol><li>FastAPI依赖Python 3.8及以上版本</li><li>可使用pip install “fastapi[all]” 安装FastAPI和所有可选依赖。</li><li>使用注意事项<ul><li>使用时需要注意异步ASGI服务器 uvicorn 看看是否正确安装，他是支持高性能和异步编程的Web服务器，常用于开发和在生产环境运行FastAPI应用程序。</li><li>uvicorn命令样例：uvicorn main:app –reload.在FastAPI程序编写好后，可以用这个命令启动程序。<br>其中uvicorn 表示启动FastAPI应用程序，</li><li>main表示需要启动的命令程序所在的Python文件位置，需要确保当前工作目录中存在这个文件，而且这个文件内容要正确。</li><li>app表示指定文件main中需要启动的FastAPI实例对象，要确保main文件中定义了app对象。</li><li>–reload uvicorn参数，表示uvicorn在检测源代码更改时要自动重载应用程序，有利于修改代码后立即看到结果，无需手动重启服务。</li></ul></li></ol><p>４. 启动 uvicorn命令后，可以使用ctrl + c命令退出服务器<br>５. from fastapi import FastAPI:FastAPI是FastAPI的核心类，依赖它构建FastAPI应用实例，定义和管理应用组件和路由。 </p><h2 id="交互式API文档"><a href="#交互式API文档" class="headerlink" title="交互式API文档"></a>交互式API文档</h2><ol><li>用途：基于OpenAPI规范的实时反应代码的最新更改，可以直接在文档中做API测试，对输入参数类型和格式能自动验证。</li><li>风格和命令：<ul><li>Swagger UI风格：<a href="http://127.0.0.1:端口/docs">http://127.0.0.1:端口/docs</a></li><li>ReDoc 风格：<a href="http://127.0.0.1:端口/redoc">http://127.0.0.1:端口/redoc</a></li></ul></li><li>使用注意：<ul><li>修改完成代码后，要等到热重载机制生效，重新触发加载，关闭旧服务器进展，启动新服务器进程后，再ctrl + r 刷新才能看到API文档的变化。</li><li>为防止缓存影响查看API文档，可以先清楚浏览器缓存或者使用隐私模式或者无痕模式访问。</li></ul></li></ol><h2 id="路由、请求和响应配置"><a href="#路由、请求和响应配置" class="headerlink" title="路由、请求和响应配置"></a>路由、请求和响应配置</h2><ol><li>路由基本配置：FastAPI实例+@实例装饰器（配置路由路径和路径参数）+路由处理函数（可配置默认值的查询参数）<ul><li>路由路径：在路径中添加{}，可以创建动态路径。</li><li>查询参数：通过为函数参数设置默认值，可以将其作为查询参数。</li><li>请求体：使用pydantic库定义数据模型作为请求体结构，以包含多个默认值的字段。</li><li>响应模型：路由设置response_model参数，可以定义响应数据的结构。</li></ul></li><li>路由处理函数使用：路由处理函数中返回值为字典或pydantic库的实例，字典和实例会被FastAPI自动转化为JSON格式，做为响应的结果传回客户端。</li><li>请求头Header和Cookie数据：使用fastapi库中的Header和Cookie类型注解，获取请求头Header和Cookie数据</li><li>响应头JSONResponse：使用fastapi.responses中的JSONResponse，实现自定义响应头。</li><li>重定向：使用fastapi.responses中的RedirectResponse，实现重定向，将新路由路径重定向到旧的路由路径中。</li><li>异常反馈：raise HTTPException(status_code, detail)，为定位未捕获的异常导致的服务崩溃，可以在代码中添加from fastapi import HTTPException，添加异常处理来检查问题原因，返回自定义的状态码和详细信息。</li><li>注意：<ul><li>可以使用typing库中的Union类型，用于支持多种数据类型的参数注解，或者用于查询参数的多种数据类型适配。例如Union[str, None]</li></ul></li></ol><h2 id="FastAPI-Pydantic"><a href="#FastAPI-Pydantic" class="headerlink" title="FastAPI Pydantic"></a>FastAPI Pydantic</h2><ol><li>功能：数据验证和序列化的模型库，用于定义请求体、响应体和其他数据模型，实现类型检查和自动文档生成。</li><li>定义 Pydantic 模型：创建继承自 pydantic.BaseModel 的类，定义字段和数据类型，数据类型是任何有效的Python类型，字段可以设置默认值。</li><li>使用 Pydantic 模型：可以用Pydantic 模型作为请求体，可做为查询参数的数据类型，自动验证和解析客户端传入的 JSON 数据是否符合模型的定义，并将其转换为模型类型的实例。</li><li>数据转换和序列化：Pydantic 模型可以自动将数据转换为特定类型（例如 JSON）或反向序列化。</li></ol><h2 id="依赖项"><a href="#依赖项" class="headerlink" title="依赖项"></a>依赖项</h2><ol><li>定义和功能：函数，可以使用与路由处理函数相同参数的函数。用于在路由处理函数执行前或后运行的<strong>可复用</strong>的函数和逻辑，执行通用逻辑，如身份验证、数据库连接等。</li><li>类型：<br>预处理依赖项（Before）： 在路由处理函数执行前运行，用于预处理输入数据，验证请求等。<br>后处理依赖项（After）： 在路由处理函数执行后运行，用于执行一些后处理逻辑，如日志记录等。</li><li>依赖注入过程：<br>定义依赖项函数：from fastapi import Depends，依赖项函数定义和普通Python函数基本相同。<br>路由处理函数使用依赖项：(查询参数: 数据类型 &#x3D; Depends(依赖项函数名))，依赖项函数处理结果返回给路由处理函数，做为入参使用。</li><li>注意：<br>1 可以使用多个依赖项函数，依赖项函数之间可以这种依赖和继承。<br>2 异步依赖项： 依赖项函数和路由处理函数可以设置为异步，允许在它们内部执行异步操作</li></ol><h2 id="表单数据处理"><a href="#表单数据处理" class="headerlink" title="表单数据处理"></a>表单数据处理</h2><ol><li>定义：接收和处理用户通过 HTML 表单提交的数据。</li><li>声明表单数据模型： <ul><li>使用from fastapi import  Form，用Form()定义查询参数的默认类型。</li><li>使用 Pydantic 模型声明，使用from pydantic import Field， 用Field()类型声明每个表单字段，做默认类型，可以添加相应的验证规则。</li></ul></li><li>接收表单数据：在路由处理函数中，使用Form()类型接收表当数据，并和Pydantic 模型字段一一对应，实现表单数据验证和转换。</li><li>处理文件上传：使用from fastapi import File, UploadFile，如果表单包含文件上传，使用 UploadFile 类型处理，可以将文件的相关信息包装在 UploadFile 对象。</li></ol><h2 id="简单CRUD-API样例"><a href="#简单CRUD-API样例" class="headerlink" title="简单CRUD API样例"></a>简单CRUD API样例</h2><ol><li>数据模型, 首先，定义Item数据模型：</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from pydantic import BaseModel</span><br><span class="line">class Item(BaseModel):</span><br><span class="line">    name: str</span><br><span class="line">    description: str = None</span><br><span class="line">    price: float</span><br><span class="line">    tax: float = None</span><br></pre></td></tr></table></figure><ol start="2"><li>存储数据, 在这个示例中，我们将使用一个简单的字典来存储数据：</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">items = &#123;&#125;</span><br></pre></td></tr></table></figure><ol start="3"><li>创建（Create）创建一个新的item：</li></ol> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">@app.post(&quot;/items/&quot;, response_model=Item)</span><br><span class="line">def create_item(item: Item):</span><br><span class="line">    item_id = len(item) + 1</span><br><span class="line">    items[item_id] = item</span><br><span class="line">    return items</span><br></pre></td></tr></table></figure><ol start="4"><li>读取（Read）, 读取一个item：</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">@app.get(&quot;/items/&#123;item_id&#125;&quot;, response_model=Item)</span><br><span class="line">def read_item(item_id: int):</span><br><span class="line">    return items[item_id]</span><br></pre></td></tr></table></figure><ol start="5"><li>读取所有items：</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">@app.get(&quot;/items/&quot;, response_model=List[Item])</span><br><span class="line">def read_items():</span><br><span class="line">    return list(items.values())</span><br></pre></td></tr></table></figure><ol start="6"><li>更新（Update）更新一个item：</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">@app.put(&quot;/items/&#123;item_id&#125;&quot;, response_model=Item)</span><br><span class="line">def update_item(item_id: int, item: Item):</span><br><span class="line">    items[item_id] = item</span><br><span class="line">    return item</span><br></pre></td></tr></table></figure><ol start="7"><li>删除（Delete）删除一个item：</li></ol> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">@app.delete(&quot;/items/&#123;item_id&#125;&quot;, response_model=Item)</span><br><span class="line">def delete_item(item_id: int):</span><br><span class="line">    item = items.pop(item_id)</span><br><span class="line">    return item</span><br></pre></td></tr></table></figure><h2 id="使用注意"><a href="#使用注意" class="headerlink" title="使用注意"></a>使用注意</h2><ol><li><p>当使用uvicorn命令在本地（排查防火墙问题）正常启动API应用程序后发现，路径和端口调用没有反应，或者无法同时访问其他路由，或者修改完文件和重载后，交互式文件不能立即修改反应在界面上，可能是存在8000等端口占用的问题，导致后续对8000端口的请求在排队中。</p></li><li><p>排查方法：<br>使用命令检查端口占用情况：<br>Linux&#x2F;macOS: sudo lsof -i 8000，检查LISTEN的端口状态，如果端口未被占用，则不会有输出。<br>Windows：netstat -ano | findstr :8000 ,检查LISTENING的端口状态，如果端口未被占用，则不会有输出<br>或者使用不同端口调度api服务：uvicorn main:app –reload –port 8001，然后尝试访问 <a href="http://127.0.0.1:8001/%E3%80%82">http://127.0.0.1:8001/。</a><br>启动 Uvicorn，添加日志级别信息，查看日志输出：uvicorn main:app –reload –log-level debug<br>在控制台手动测试API服务端点命令：curl http:127.0.0.1:端口&#x2F;</p></li><li><p>在代码开发过程中，要注意确保服务端关闭的情况下，同时释放占用的端口。请求能快速反馈，没有长时间、执行耗时的阻塞或者耗时操作。FasfAPI内置支持使用信号来优雅关闭服务器并释放端口（signal库）</p></li><li><p>端口关闭方法：</p><table><thead><tr><th>操作系统</th><th>命令</th><th>含义</th></tr></thead><tbody><tr><td>linux&#x2F;macOS</td><td>终端窗口+kill -9 进程标识符</td><td>-9 表示强制终止进程</td></tr><tr><td>Windows</td><td>命令提示符+taskkill &#x2F;PID 占用进程号 &#x2F;F</td><td>&#x2F;PID 表示进程标识符，&#x2F;F表示强制结束进程</td></tr></tbody></table></li><li><p>TCP协议连接状态含义：</p><table><thead><tr><th>名称</th><th>含义</th><th>说明</th></tr></thead><tbody><tr><td>LISTENING</td><td>端口正在监听连接请求</td><td>服务端已启动，服务端在等待客户端发起的连接请求</td></tr><tr><td>CLOSE_WAIT</td><td>连接一方已收到关闭请求，在等待对端关闭连接</td><td>表示客户端关闭连接，服务器还没有处理完成这些连接，需要调度close()等来关闭连接</td></tr><tr><td>TIME_WAIT</td><td>连接已关闭，在等待一段时间后所有数据包都已传输完成</td><td>TIME_WAIT通常是正常的，如果有大量的TIME_WAIT状态，就需要调整系统参数来优化</td></tr></tbody></table></li></ol><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] <a href="https://fastapi.tiangolo.com/zh/tutorial/">FastAPI教程</a><br>[2] <a href="https://zhuanlan.zhihu.com/p/624779536">轻松上手Python的Web神器：FastAPI教程</a><br>[3] <a href="https://www.w3cschool.cn/fastapi/fastapi-tutorial.html">FastAPI 用户指南</a><br>[4] <a href="https://www.runoob.com/http/http-tutorial.html">HTTP 教程</a></p>]]></content>
      
      
      <categories>
          
          <category> API </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> RESful API </tag>
            
            <tag> Web框架 </tag>
            
            <tag> API </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Prompt Engineering(提示词工程)应用实践-Prompt自优化框架</title>
      <link href="/2025/09/14/Prompt%20Engineering(%E6%8F%90%E7%A4%BA%E8%AF%8D%E5%B7%A5%E7%A8%8B)%E5%BA%94%E7%94%A8%E5%AE%9E%E8%B7%B5-Prompt%E8%87%AA%E4%BC%98%E5%8C%96%E6%A1%86%E6%9E%B6/"/>
      <url>/2025/09/14/Prompt%20Engineering(%E6%8F%90%E7%A4%BA%E8%AF%8D%E5%B7%A5%E7%A8%8B)%E5%BA%94%E7%94%A8%E5%AE%9E%E8%B7%B5-Prompt%E8%87%AA%E4%BC%98%E5%8C%96%E6%A1%86%E6%9E%B6/</url>
      
        <content type="html"><![CDATA[<h2 id="Prompt-Engineering-提示词工程-应用实践-Prompt自优化框架"><a href="#Prompt-Engineering-提示词工程-应用实践-Prompt自优化框架" class="headerlink" title="Prompt Engineering(提示词工程)应用实践-Prompt自优化框架"></a>Prompt Engineering(提示词工程)应用实践-Prompt自优化框架</h2><h3 id="典型的Prompt自优化框架"><a href="#典型的Prompt自优化框架" class="headerlink" title="典型的Prompt自优化框架"></a>典型的Prompt自优化框架</h3><table><thead><tr><th>框架名称</th><th>论文名称</th><th>GitHub地址</th></tr></thead><tbody><tr><td>DSPy(Declarative Self-improved Language Programs in Python)</td><td><a href="https://arxiv.org/abs/2310.03714">DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines</a></td><td><a href="https://dspy-docs.vercel.app/docs/quick-start/installation">DSPy官网</a>;<a href="https://github.com/stanfordnlp/dspy">DSPy GitHub</a></td></tr><tr><td>TextGrad</td><td><a href="https://arxiv.org/abs/2406.07496">TextGrad: Automatic “Differentiation” via Text</a></td><td><a href="https://github.com/zou-group/TextGrad">TextGrad GitHub</a></td></tr><tr><td>PromptWizard</td><td><a href="https://arxiv.org/abs/2405.18369">PROMPTWIZARD:TASK-AWARE PROMPT OPTIMIZATION FRAMEWORK</a></td><td><a href="https://github.com/microsoft/PromptWizard">PromptWizar GitHub</a>;<a href="https://microsoft.github.io/PromptWizard/">微软地址</a></td></tr><tr><td>GRAD-SUM</td><td><a href="https://arxiv.org/abs/2407.12865">GRAD-SUM: Leveraging Gradient Summarization for Optimal Prompt Engineering</a></td><td></td></tr><tr><td>Ell</td><td>前OpenAI科学家William Guss开源项目, ELL是轻量级提示工程库，将提示视为函数，通过词法闭包实现提示版本控制、优化、跟踪、可读性和可视化</td><td><a href="https://github.com/MadcowD/ell">Ell GitHub</a></td></tr><tr><td>StraGo(Strategic-Guided Optimization)</td><td><a href="https://arxiv.org/abs/2410.08601">StraGo: Harnessing Strategic Guidance for Prompt Optimization</a></td><td></td></tr><tr><td>ERM(Exemplar-Guided Reflection with Memory mechanism)</td><td><a href="https://arxiv.org/abs/2411.07446">Efficient and Accurate Prompt Optimization: the Benefit of Memory in Exemplar-Guided Reflection</a></td><td></td></tr></tbody></table><p>一个界定良好的问题，就已经将问题解决了一半。——美国思想家杜威<br>如果给我1小时解答一道决定我生死的问题，我会花55分钟弄清楚这道题到底咋问什么。一旦清楚它到底在问什么，剩下的5分钟足够回答这个问题。——爱因斯坦，只有好的问题，才会有好的答案。</p><h3 id="Prompt模板基本框架示例"><a href="#Prompt模板基本框架示例" class="headerlink" title="Prompt模板基本框架示例"></a>Prompt模板基本框架示例</h3><p>Prompt &#x3D; Role + Instruction + [Context&#x2F;Background Information] + [Examples&#x2F;Templates] + [Specific Requirements&#x2F;Details] + Text Input&#x2F;User Question<br><strong>Role(角色)</strong>：Role位于首行，按应用需要固定一个角色；<br><strong>Instruction(指令)</strong>：简短，清晰，高度概括任务要求，前后风格保存一致<br><strong>Context&#x2F;Background Information(上下文&#x2F;背景信息)</strong>：独立成段，格式清晰；明确段落名称；保持清晰结构和段落逻辑<br>可采用静态&#x2F;动态Context方法，在系统运行时获取上下文信息或者在系统中利用穷举预置好上下文信息。<br><strong>Examples&#x2F;Templates(示例&#x2F;模板)</strong>：示例均衡全覆盖，要具备指导实施能力；独立成段，格式清晰；明确段落名称；保持清晰结构和段落逻辑<br>可以使用单样本提示(one-shot prompting)、少样本提示(few-shot prompting)方法，通过少量样本引导LLM对特定任务进行学习和执行，以产出相似风格或主题的内容。<br><strong>Specific Requirements&#x2F;Details(具体要求&#x2F;细节)</strong>:明确输出格式要求；明确任务细节要求，并完成对指令细化补充<br><strong>Text Input&#x2F;User Question(输入文本&#x2F;用户问题)</strong>：数据真实有效多样化；和Prompt其他元素隔离开</p><h3 id="Prompt模板典型技巧"><a href="#Prompt模板典型技巧" class="headerlink" title="Prompt模板典型技巧"></a>Prompt模板典型技巧</h3><p>将模糊要求细化成具体要求<br>描述输出格式<br>使用分割符号和强调符号<br>描述操作步骤<br>提供相关材料和背景<br>使用示例(FewShot)，解决相似的问题<br>使用思维链引导，让模型慢慢思考(CoT)</p><h2 id="DSPy-Prompt自优化框架"><a href="#DSPy-Prompt自优化框架" class="headerlink" title="DSPy-Prompt自优化框架"></a>DSPy-Prompt自优化框架</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>开发团队：斯坦福大学NLP团队(The Stanfold NLP Group)<br>开源日期：2023-01-09(<a href="https://api.github.com/repos/stanfordnlp/dspy">https://api.github.com/repos/stanfordnlp/dspy</a>)<br>开源协议：MIT license<br>论文名称：<a href="https://arxiv.org/abs/2310.03714">DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines</a><br>DSPy官网：<a href="https://dspy.ai/">https://dspy.ai/</a><br>DSPy中文文档：<a href="https://www.aidoczh.com/docs/dspy/docs/intro">https://www.aidoczh.com/docs/dspy/docs/intro</a><br>GitHub地址：<a href="https://github.com/stanfordnlp/dspy">https://github.com/stanfordnlp/dspy</a><br>GitHub数据(截止2025&#x2F;09&#x2F;20)：Fork 2.3k, Star 28.5k, Contributors 347, 最近版本3.0.3<br>设计目标：借鉴神经网络，实现对LLM的Prompt和权重进行自优化的框架<br>解决痛点：解决Prompt手动持续调整问题，实现自动调优；解决切换LLM模型Prompt适配问题，实现自动优化；Prompt开发流程工程化，规范迭代开发；</p><h3 id="总体设计"><a href="#总体设计" class="headerlink" title="总体设计"></a>总体设计</h3><p>思路：通过编程方式，将模块和每个步骤的参数(Prompt组件和LLM权重)分离，对组成提示词的部分进行模块化；引入LLM驱动的算法优化器，在给定希望最大化度量指标情况下，调整LLM的提升和权重。<br>DSPy应用程序构建过程及流图：</p><ol><li>收集数据集(Dataset)：提前准备输入输出示例(问答对)，用于后续Prompt优化。</li><li>编写DSPy程序：利用DSPy的签名(Signature)和模块(Module)功能，定义程序逻辑和组件之间的信息流动步骤，用于完成Prompt优化任务。</li><li>定义验证程序：利用指标(Metrics)和优化器，优化程序性能和确定优化后效果的验证方向。</li><li>编译并运行DSPy程序：调用优化器内的编译器方法(compile)，将程序代码、优化器、验证指标、数据集调动起来，程序自动优化Prompt，包括调整提提示或微调。</li><li>迭代改进：不断通过优化数据、程序和验证逻辑重复上述过程，知道获得满意的Prompt为止。<img src="https://pica.zhimg.com/v2-332b6c3ea0fe5fba16d7fd7cf905aac6_1440w.jpg" alt="img"> 于PyTorch灵活组合通用模块构建神经网络一样，DSPy可以在任何LLM的应用中自由组合模块。编译DSPy应用程序的过程类似于PyTorch训练神经网络的过程。</li></ol><h3 id="核心模块"><a href="#核心模块" class="headerlink" title="核心模块"></a>核心模块</h3><p><strong>签名(Signature)</strong>:声明性规范，由输入内容的详细描述、输出内容的详细描述、任务描述组成。将输入内容、输入内容设置在变量中，在推理过程中转换为提示词内容。实现定义模块的输入输出行为，指导LLM执行什么任务的功能, 专注定义任务的预期结果。<br>使用方法：标准定义、内联定义两种。<br>内联定义(简单方法)：使用简单字符串表示，用“-&gt;”符号关联输入和输出</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import dspy</span><br><span class="line">qa = dspy.ChainOfThought(&quot;question -&gt; answer&quot;)</span><br></pre></td></tr></table></figure><p>标准定义：涉及输入输出数据多而且需要添加备注时，用来定义复杂的输入输出关系。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import dspy</span><br><span class="line">class AnalyzeRepository(dspy.Signature):</span><br><span class="line">    &quot;&quot;&quot;Analyze a repository structure and identify key components.&quot;&quot;&quot;</span><br><span class="line">    readme_content: str = dspy.InputField(desc=&quot;README.md content&quot;)</span><br><span class="line">    project_purpose: str = dspy.OutputField(desc=&quot;Main purpose and goals of the project&quot;, prefix = &quot;Question&#x27;s Answer:&quot;)</span><br></pre></td></tr></table></figure><p><strong>模块(Module)</strong>:Prompt组装。将提示词中的关键部分设置到变量中，在使用LLM前，将提示词各要素组装完整。<br>关键特性：</p><ol><li>每个内置模块负责特定的Prompt Engineering技术并处理DSPy签名。</li><li>DSPy模块有可学习参数，涉及Prompt组件和LLM权重，实现处理输入并根据优化后的参数生成输出。</li><li>DSPy模块可组合成更大更复杂的模块。</li></ol><p>相关组件：</p><table><thead><tr><th>组件</th><th>用途</th></tr></thead><tbody><tr><td><a href="https://dspy.ai/api/modules/Predict/">dspy.Predict</a></td><td>处理输入输出字段，生成指令，构建指定类型的提示词模板</td></tr><tr><td><a href="https://dspy.ai/api/modules/ChainOfThought/">dspy.ChainOfThought(CoT)</a></td><td>继承自Predict，提供CoT模式相关提示词组装和大模型调用功能,通过推理步骤逐步逼近答案，可以使用Hint来提示LLM如何推理。</td></tr><tr><td><a href="https://dspy.org.cn/api/modules/ChainOfThoughtWithHint/">dspy.ChainofThoughtWithHint</a></td><td>使用Parallel模块并行处理dspy.Example实例列表, 增强了ChainOfThought，增加推理提示的选项</td></tr><tr><td><a href="https://dspy.ai/api/modules/MultiChainComparison/">dspy.MultiChainComparison</a></td><td>增加多重链比较功能</td></tr><tr><td><a href="https://dspy.ai/api/modules/ProgramOfThought/">dspy.ProgramOfThought(PoT)</a></td><td>根据输入字段生成并可执行Python代码，通过迭代细化产生单个输出字段</td></tr><tr><td>dspy.Retrieve</td><td>从检索模块检索信息, 根据query返回k个查询结果</td></tr><tr><td><a href="https://dspy.ai/api/modules/ReAct/">dspy.ReAct</a></td><td>提供React思维模式(思考、行动、观察)执行步骤，调整响应，适用于动态调整和多步骤推理。</td></tr></tbody></table><p><strong>优化器(Optimizer)</strong>:实现指标最大值的微调DSPy参数算法,由算法、指标(Metrics)、数据集(Dataset)、项目(Program)构成。<br>算法：用于迭代优化提示词参数的优化算法。<br>指标(Metrics):用于衡量算法优化效果的度量标准。<br>数据集(Dataset)：优化提示词的数据集，与签名输入输出内容定义一致的、数据实例对象构成的数据列表。<br>项目(Program)：优化实体，通常为模块(Module)实例。</p><p>原理：针对项目(Program)，利用优化算法，调整选择示例和参数，构成新的提示词，借助LLM获得预测结果，再利用指标(Metrics)计算预测结果判断是否符合预期，确定算法优化方向，最终不断迭代获得示例和最终参数。<br>相关优化器：</p><table><thead><tr><th>优化器</th><th>用途</th></tr></thead><tbody><tr><td><a href="https://dspy.ai/api/optimizers/LabeledFewShot/">dspy.LabeledFewShot</a></td><td>定义预测器使用k个样本的数量</td></tr><tr><td><a href="https://dspy.ai/api/optimizers/BootstrapFewShot/">dspy.BootstrapFewShot</a></td><td>引导式启动，利用少量示例来优化Prompt和权重。</td></tr><tr><td><a href="https://dspy.ai/api/optimizers/BootstrapFewShotWithRandomSearch/">dspy.BootstrapFewShotWithRandomSearch</a></td><td>在BootstrapFewShot的基础上增加了随机搜索的特性，生成不同示例集，评估每个示例集性能，最后选择最佳Prompt和模型权重。</td></tr><tr><td><a href="https://dspy.ai/api/optimizers/Ensemble/">dspy.Ensemble</a></td><td>将多个程序集成，统一不同的输出为单一结果</td></tr><tr><td><a href="https://dspy.ai/api/optimizers/BootstrapFinetune/">dspy.BootstrapFinetune</a></td><td>将提词器设定为BootstrapFewShot，专门用于编译过程中的微调</td></tr></tbody></table><h3 id="使用注意事项："><a href="#使用注意事项：" class="headerlink" title="使用注意事项："></a>使用注意事项：</h3><ol><li>在使用pip时，Windows安装DSPy前要先安装Microsoft C++ Build Tools，因为DSPy中的包madoka需要编译C&#x2F;C++的扩展，否则会报这个错误</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">error: Microsoft Visual C++ 14.0 or greater is required. Get it with &quot;Microsoft C++ Build Tools&quot;: https://visualstudio.microsoft.com/visual-cpp-build-tools/</span><br></pre></td></tr></table></figure><p>在linux下则没有这个问题，可以直接使用 pip install -U dspy命令正常安装。<br>2. 在linux中注意检查pip和Python是否在同一个环境，命令which python和which pip如果是相同路径，则为同一个环境。<br>3. dspy.OpenAI&#x2F;dspy.teleprompt.BootstrapFewShotWithOptuna从dspy 0.4开始已经移除，使用dspy.OpenAI需要使用dspy.configure()来设置配置模型，或者降级使用dspy版本。<br>4. dspy的官方文档设计并不友好，缺乏新手指引类的入门教程，学习和掌握难度比较大,建议先从样例开始学习。<br>5. 使用dspy.LM前需要注意调用大模型的API，例如export OPENAI_API_KEY等信息。</p><h3 id="参考样例-数学推理"><a href="#参考样例-数学推理" class="headerlink" title="参考样例-数学推理"></a>参考样例-数学推理</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"># 使用时需要注意在Conda中安装更新dspy、datasets，conda install -c conda-forge dspy/datasets</span><br><span class="line">import dspy</span><br><span class="line"># 检查版本</span><br><span class="line">print(dspy.__version__)</span><br><span class="line"></span><br><span class="line">gpt4o_mini = dspy.LM(&#x27;openai/gpt-4o-mini&#x27;, max_tokens=2000)</span><br><span class="line">gpt4o = dspy.LM(&#x27;openai/gpt-4o&#x27;, max_tokens=2000)</span><br><span class="line"># 除非另有说明，否则我们将使用 gpt-4o-mini 作为默认 LM </span><br><span class="line">dspy.configure(lm=gpt4o_mini) </span><br><span class="line"></span><br><span class="line"># 从dspy.datasets导入MATH数据集</span><br><span class="line">from dspy.datasets import MATH</span><br><span class="line">dataset = MATH(subset=&#x27;algebra&#x27;)</span><br><span class="line"># 输出350 350</span><br><span class="line">print(len(dataset.train), len(dataset.dev)) </span><br><span class="line"></span><br><span class="line"># 检查数据集中的示例</span><br><span class="line">example = dataset.train[0]</span><br><span class="line"># 输出结果如下：Question: The doctor has told Cal O&#x27;Ree that during his ten weeks of working out at the gym, he can expect each week&#x27;s weight loss to be $1\%$ of his weight at the end of the previous week. His weight at the beginning of the workouts is $244$ pounds. How many pounds does he expect to weigh at the end of the ten weeks? Express your answer to the nearest whole number.</span><br><span class="line">print(&quot;Question:&quot;, example.question)</span><br><span class="line"># 输出结果如下：Answer: 221</span><br><span class="line">print(&quot;Answer:&quot;, example.answer)</span><br><span class="line"></span><br><span class="line"># 定义模块 </span><br><span class="line">module = dspy.ChainOfThought(&quot;question -&gt; answer&quot;)</span><br><span class="line"># 需要在此之前确认调用大模型的API</span><br><span class="line">module(question=example.question)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 设置评估器并进行提示优化</span><br><span class="line">THREADS = 24</span><br><span class="line">kwargs = dict(num_threads=THREADS, display_progress=True, display_table=5)</span><br><span class="line">evaluate = dspy.Evaluate(devset=dataset.dev, metric=dataset.metric, **kwargs)</span><br><span class="line">evaluate(module)</span><br><span class="line"></span><br><span class="line"># 使用gpt4o作为教师模型少量调用(THREADS)，使用gpt4o_mini直接优化并最终生成优化后的模块</span><br><span class="line">kwargs = dict(num_threads=THREADS, teacher_settings=dict(lm=gpt4o), prompt_model=gpt4o_mini)</span><br><span class="line">optimizer = dspy.MIPROv2(metric=dataset.metric, auto=&quot;medium&quot;, **kwargs)</span><br><span class="line"># 提示中设置最大4个引导示例</span><br><span class="line">kwargs = dict(max_bootstrapped_demos=4, max_labeled_demos=4) </span><br><span class="line">optimized_module = optimizer.compile(module, trainset=dataset.train, **kwargs)</span><br><span class="line"># 评估优化后的模块，和前面对比评估结果做对比验证</span><br><span class="line">evaluate(optimized_module)</span><br><span class="line"></span><br><span class="line"># 可以查看优化后的提示词形式</span><br><span class="line">dspy.inspect_history()</span><br></pre></td></tr></table></figure><h3 id="日后计划"><a href="#日后计划" class="headerlink" title="日后计划"></a>日后计划</h3><p>研究PromptWizard框架，并和DSPY做对比，看看二者的优劣。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] <a href="https://www.ultrasev.com/blog/2024/dspy-101-0820">DSPy 从入门到劝退</a><br>[2] <a href="https://luxiangdong.com/2024/04/19/dspy1/">DSPY简易教程</a><br>[3] <a href="https://zhuanlan.zhihu.com/p/685171231">DSPy入门:再见提示，你好编程</a><br>[4] <a href="https://juejin.cn/post/7388091090321883162">DSPy实战：三十分钟无痛上手自动化Prompt框架</a><br>[5] <a href="https://docs.kanaries.net/zh/topics/AIGC/dspy">DSPy:开源大语言模型的革命</a><br>[6] <a href="https://blog.csdn.net/qq_41185868/article/details/138050523">LLMs之DSPy：DSPy(可优化RAG系统)的简介、安装和使用方法、案例应用之详细攻略</a><br>[7] <a href="https://www.53ai.com/news/tishicikuangjia/2024090107639.html">告别提示工程，未来属于DSPy（上）</a><br>[8] <a href="https://www.53ai.com/news/tishicikuangjia/2024090124159.html">告别提示工程，未来属于DSPy（下）</a><br>[9] <a href="https://dspy.ai/tutorials/math/">Tutorial: Math Reasoning</a><br>[10] <a href="https://towardsdatascience.com/">Towards Data Science</a><br>[11] <a href="https://towardsdatascience.com/intro-to-dspy-goodbye-prompting-hello-programming-4ca1c6ce3eb9/">Intro to DSPy: Goodbye Prompting, Hello Programming!</a><br>[12] <a href="https://github.com/langgptai/LangGPT">LangGPT —— 人人都可编写高质量 Prompt</a></p>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
            <tag> 算法 </tag>
            
            <tag> 应用 </tag>
            
            <tag> Prompt Engineering </tag>
            
            <tag> 框架 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GPU使用经验总结</title>
      <link href="/2025/09/14/GPU%E4%BD%BF%E7%94%A8%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/"/>
      <url>/2025/09/14/GPU%E4%BD%BF%E7%94%A8%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<p>GPU使用经验总结，主要总结在算法设计和项目开发过程中，对GPU使用过程方面，进行经验总结，为以后使用GPU等计算工具做准备。</p><h3 id="GPU使用基本流程"><a href="#GPU使用基本流程" class="headerlink" title="GPU使用基本流程"></a>GPU使用基本流程</h3><ol><li>基本流程五步骤：分析需求-评估资源-资源获取-环境准备-任务执行</li></ol><ul><li>分析需求：确定是否需要使用GPU(判断必要性)，梳理使用事情清单和实现目标，基于任务特点、任务阶段、使用环境判断任务类型是开发、训练还是推理。</li><li>评估资源：确定使用的AI平台、环境、GPU型号、使用时长和数量</li><li>资源获取：由于GPU资源受限，需要登记使用信息，进行GPU资源评审通过后，才能进行GPU资源申请和调配</li><li>环境准备：根据申请到GPU资源，在计划使用时间内，准备开发、训练、推理的生产和测试环境，并完成环境配置</li><li>任务执行：基于准备好的环境，下载加载调用模型，执行开发、训练、推理任务。</li></ul><h3 id="GPU分析需求"><a href="#GPU分析需求" class="headerlink" title="GPU分析需求"></a>GPU分析需求</h3><ol><li>根据模型的目的、开发阶段和需求，可以先从模型任务和模型选型开始。</li></ol><ul><li>模型任务类型：开发、训练、推理，涉及试算、开发和上线等IT环节。<ul><li>开发类型：主要在试算、开发、SIT阶段，主要在线上开发环境中执行，特点就要灵活快速修改代码，环境运行时需要一直占用资源，资源需要随时可支持，以获取运行结果。可以用于工具调度和代码调整过程中。</li><li>训练类型：主要在试算、开发、上线等阶段，主要用于线上训练作业，特点是代码基本固定，代码按排序依次执行，任务结束后就释放资源。可用于算法模型的微调，提升模型效果。</li><li>推理类型：主要在开发、上线等阶段，主要用于在线服务支持。特点就是代码已稳定，需要在服务器环境中执行运行供外部使用调用，资源持续占用。可用于算法模型的在线部署和服务调用，对外稳定提供服务。</li></ul></li></ul><ol start="2"><li>LLM模型的选型策略：模型配置(参数、token等)、模型评测效果</li></ol><ul><li>模型配置：<ul><li>参数：模型的权重、偏置等，是模型训练过程中的主要学习内容，影响模型的输出结果。参数越多一般效果越好，但训练消耗的资源也越多，而且模型的部署使用也会依赖更多的硬件资源支持，选择时要考虑可供使用的硬件配置。</li><li>token：模型理解和处理文本时的基本单元，使模型和语言解耦，模型的Max token就是在一次对话中基于上下文关系记忆的最大token数据量。</li></ul></li><li>模型评测效果：<ul><li>模型根据自身特点，有特定的适用领域，对模型在通用和特定任务、公开数据集下的运行结果做多维度测评，有利于在实际应用时，正确选择合适模型以达成效果。其中典型测评榜单有<a href="https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/">huggingface</a>和<a href="https://cevalbenchmark.com/static/leaderboard_zh.html">C-eval</a>,对于LLM评测综述可阅读<a href="https://zhuanlan.zhihu.com/p/642689101">首篇大语言模型评测的综述</a>一文。</li></ul></li></ul><h3 id="GPU评估资源"><a href="#GPU评估资源" class="headerlink" title="GPU评估资源"></a>GPU评估资源</h3><p>针对使用的模型，需要预估GPU资源数量。</p><ul><li>算力：指计算设备（GPU、CPU、NPU等）完成计算的能力大小，一般评价指标为在单位时间内完成的运算次数。GPU单卡可发挥的算力是指在理想条件下，一块GPU图形卡所能提供的最大计算性能。常用算力单位：FLOPS(每秒浮点运算次数)、TOPS(每秒万亿运算次数$10^{12}$),1T FLOPS就是每秒计算1万亿次浮点运算，等于$10^{12}$</li></ul><ol><li>基于运行时长的数据估算策略：GPU卡数影响训练时间<br>$$<br>开启激活重计算场景下的训练时间 &#x3D; \frac{8 * token数据量 * 模型参数量}{GPU卡数 * 单卡峰值运算性能FLOPS * GPU利用率}<br>$$<br>其中token数据量 * 模型参数量是模型训练所需的总计算量，GPU利用率一般在0.3−0.55之间，模型参数量中1B(1 Billion)代表10亿个参数(10^9).FLOPS每秒浮点运算次数,这个要考虑所选的数值精度，精度不同，相同GPU的单卡峰值运算性能也不同。</li><li>基于显存大小的数据估算策略：</li></ol><ul><li>工具：HuggingFace在线大模型显存消耗资源估算工具<a href="https://huggingface.co/spaces/hf-accelerate/model-memory-usage">Model Memory Calculator</a></li><li>工具使用：入参是模型对应HuggingFace地址、模型精度、library情况，输出为最大层(Largest Layer)、推理显存(Total Size)、Adam训练显存(Training using Adam)。可以先使用工具估算出不同精度下的训练和推理所需要的显存大小，再根据GPU单卡性能参数(显存大小)，估算所需单卡数量。</li></ul><ol start="3"><li>使用注意事项：</li></ol><ul><li>该估算工具可本地部署，部署方法可看链接：<a href="https://www.datalearner.com/blog/1051693562957225/https://www.wehelpwin.com/article/4255">https://www.datalearner.com/blog/1051693562957225/https://www.wehelpwin.com/article/4255</a></li><li>推理显存实际使用量应该是预估数量的1.2倍，原因来自EleutherAI的技术分析,这个是经验公式.详情请看链接：<a href="https://blog.eleuther.ai/transformer-math/">https://blog.eleuther.ai/transformer-math/</a></li><li>显存经验手工计算公式：<a href="https://blog.csdn.net/lov1993/article/details/136717491">大模型不同参数下的模型显存计算公式</a></li></ul><h3 id="GPU资源获取"><a href="#GPU资源获取" class="headerlink" title="GPU资源获取"></a>GPU资源获取</h3><ol><li>GPU资源评审考虑项：结合资源获取的公司流程，重点考虑任务类型、任务名、任务描述、资源需求、性能要求、使用时间、责任人等内容。注意点如下：</li></ol><ul><li>任务类型：模型任务是业务需求触发还是技术能力储备</li><li>任务名：可以根据任务特点简明扼要的名称做为任务名</li><li>任务描述：结合场景、目标和价值对任务做描述说明</li><li>资源需求：说明需要的资源，包括使用AI平台、任务类型、环境、GPU型号和数量</li></ul><h3 id="GPU环境准备："><a href="#GPU环境准备：" class="headerlink" title="GPU环境准备："></a>GPU环境准备：</h3><ol><li>环境准备依赖所使用的AI平台，具体使用要求需要结合平台规范进行。环境配置一般比较繁琐，但基本都一次性的。建议对环境准备过程做记录，以供其他人配置环境时，照此处理。</li></ol><h3 id="GPU任务执行"><a href="#GPU任务执行" class="headerlink" title="GPU任务执行"></a>GPU任务执行</h3><p>典型训练任务：模型微调</p><ol><li><p>目的：在特定数据集上进一步训练模型，提高模型在解决特定问题或任务时的性能。</p></li><li><p>时机：是否需要选择模型微调，主要考虑成本效益、领域专长、数据保护等方面。成本效益上看，微调预训练模型一般比从头训练大模型更经济，适应性也更好，也能更好解决Prompt Engineering限制问题。领域专长上看，可以利用企业多年积累的数据、为企业提供有针对性的定制化的模型服务，同时由于有了现成数据，利用监督学习的机制，也有利于模型在解决特定问题上性能提升。数据保护上看，由于使用的数据训练只在企业内部使用，也有利于数据保护，避免数据泄露和模型使用之间的两难困境。</p></li><li><p>方法：LoRA(新增低秩矩阵到原权限矩阵)、Prefix Tuning(前缀调整,输入增加可训练上下文前缀)、Prompt Tuning(提示调整,输入增加可训练的嵌入向量提示)、QLoRA(Quantized Low-Rank Adaptation,量化权重到4bit+LORA)、P-Tuning(使用可训练的LSTM模型生成嵌入向量用于输入)、P-TuningV2(P-Tuning结合多个N中输入)、Adapter Tuning(适配器调整,新增小神经网络).</p></li><li><p>方法适应性：</p><table><thead><tr><th>方法名</th><th>适应场景</th></tr></thead><tbody><tr><td>Adapter&#x2F;LoRA</td><td>资源有限，特别是计算和内存;中等到大型数据集上希望实现全参数微调效果;适用于分类或者问答等任务</td></tr><tr><td>Prompt Tuning&#x2F;P-Tuning</td><td>适用于自然语言生成型任务、数据量较少场景(10^3数据级);不深入模型架构，只希望用于快速原型设计、迭代调试场景。</td></tr></tbody></table></li><li><p>典型工具：</p><table><thead><tr><th>工具名</th><th>适用平台</th><th>工具用途</th></tr></thead><tbody><tr><td><a href="https://github.com/yangjianxin1/Firefly">Firefly</a></td><td>英伟达GPU</td><td>预训练和模型微调工具</td></tr><tr><td><a href="https://gitee.com/ascend/MindSpeed-LLM">MindSpeed LLM</a></td><td>华为昇腾NPU</td><td>预训练和模型微调工具</td></tr><tr><td><a href="https://hugging-face.cn/docs/peft/quicktour">Hugging Face PEFT库(Parameter-Efficient Fine-Tuning，参数高效微调)</a></td><td>英伟达GPU&#x2F;华为昇腾NPU</td><td>高效微调方法Python库,Hugging Face核心组件</td></tr></tbody></table></li><li><p>模型接口API设置: 可采用传统API格式、Openai Completions api格式(通用的自然语言生成接口)、Openai Chat api格式(专为生成对话和聊天场景而设计)</p></li></ol><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><p>[1] <a href="https://cloud.tencent.com/developer/article/2393847">模型运算量、显卡算力说明</a><br>[2] <a href="https://blog.csdn.net/qq_44815135/article/details/140663123">大模型参数量和占的显存怎么换算</a><br>[3] <a href="https://blog.csdn.net/qq_44193969/article/details/132246050">大模型训练时间估算</a><br>[4] <a href="https://zhuanlan.zhihu.com/p/671786293">通俗解读大模型微调Fine-Tuning</a><br>[5] <a href="https://zhuanlan.zhihu.com/p/673789772">大模型炼丹术：大模型微调总结及实现</a><br>[6] <a href="https://aws.amazon.com/cn/blogs/china/practical-series-on-fine-tuning-large-language-models-part-two/">炼石成丹：大语言模型微调实战系列（二）模型微调篇</a><br>[7] <a href="https://openai.xiniushu.com/docs/guides/fine-tuning">微调(Fine-tuning)|OpenAI官方帮助文档中文版</a><br>[8] <a href="https://pdf.dfcfw.com/pdf/H3_AP202501221642435170_1.pdf?1737543124000.pdf">深度解析Palantir</a><br>[9] <a href="https://cookbook.langchain.com.cn/docs/langchain-agents/#%E4%BB%A3%E7%90%86-agents-%E5%92%8C%E5%B7%A5%E5%85%B7">超能力对话代理 (Agents) 的超级 LLMs</a><br>[10]<a href="https://huggingface.co/Qwen/Qwen-7B-Chat/blob/main/examples/react_prompt.md?code=true">ReAct Prompting 技术命令千问使用工具</a><br>[11] <a href="https://www.wehelpwin.com/newslist">AI魔法学院</a><br>[12] <a href="https://zhuanlan.zhihu.com/p/660721012">通俗解读大模型主流微调方法：从Prefix Tuning、P-Tuning V1&#x2F;V2到LoRA、QLoRA</a><br>[13] <a href="https://zhuanlan.zhihu.com/p/1914674647611450675">主流大语言模型API参数详解</a><br>[14] <a href="https://blog.csdn.net/ybdesire/article/details/133828010">OpenAI接口Completion和ChatCompletion的区别与使用方法</a><br>[15] <a href="https://xiniushu.com/#test_tool">犀牛书</a><br>[16] <a href="https://zhuanlan.zhihu.com/p/666655216">OpenAI API 接口实战教程 #2 Chat类和Completion 类 自然语言生成模型</a></p>]]></content>
      
      
      <categories>
          
          <category> GPU </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GPU </tag>
            
            <tag> 经验总结 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Prompt Engineering(提示词工程)实施总结</title>
      <link href="/2025/09/14/Prompt%20Engineering(%E6%8F%90%E7%A4%BA%E8%AF%8D%E5%B7%A5%E7%A8%8B)%E5%AE%9E%E6%96%BD%E6%80%BB%E7%BB%93/"/>
      <url>/2025/09/14/Prompt%20Engineering(%E6%8F%90%E7%A4%BA%E8%AF%8D%E5%B7%A5%E7%A8%8B)%E5%AE%9E%E6%96%BD%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<h2 id="Prompt-Engineering-提示词工程-实施总结"><a href="#Prompt-Engineering-提示词工程-实施总结" class="headerlink" title="Prompt Engineering(提示词工程)实施总结"></a>Prompt Engineering(提示词工程)实施总结</h2><h3 id="Prompt简介"><a href="#Prompt简介" class="headerlink" title="Prompt简介"></a>Prompt简介</h3><p>定义：输入给LLM的指令性文本<br>本质：指令性文本，利用显式指令和隐性约束，通过语义编码触发LLM内部的Transformer层，进行注意力机制计算，进而将人类意图转化为模型可理解的概率分布空间。<br>特性：<br>　　LLM敏感性：LLM对输入提示词极其敏感，细节不同，结果不同。<br>　　LLM性能提升：改进提示词的设计，可以显著提高LLM完成任务的准确性，进而显著缩短AI生成内容的迭代周期。<br>意义：明确、具体的提示能显著提高模型输出和期望的契合度。</p><h3 id="基本Prompt结构"><a href="#基本Prompt结构" class="headerlink" title="基本Prompt结构"></a>基本Prompt结构</h3><p>基本Prompt要素构成：指令、上下文(背景)、输入数据、输出格式<br>　　<strong>指令(必须)</strong>：模型执行的特定任务或者指令，告诉模型应如何响应。<br>　　<strong>上下文(背景，非必须)</strong>：涉及参考的和任务相关的外部信息或者额外内容的上下文，用于引导LLM产生最佳回复，理解需求。<br>　　<strong>输入数据(必须)</strong>：提出的希望得到回复的输入或者问题，应清晰完整来确保模型生成满意结果，注意输入长度和类型。<br>　　<strong>输出格式(非必须)</strong>：要求或期望输出的内容所满足的格式、类型和结构。</p><h3 id="Prompt类型"><a href="#Prompt类型" class="headerlink" title="Prompt类型"></a>Prompt类型</h3><h4 id="System-Prompt-系统提示词"><a href="#System-Prompt-系统提示词" class="headerlink" title="System Prompt(系统提示词)"></a>System Prompt(系统提示词)</h4><p>定义：先预设背景角色、风格或者规则，然后由用户提出具体请求，模型按照所设角色身份特点回答问题。利用聊天模型的系统消息或者在提升中明确指定模型角色的Prompt方式。系统+用户组合提示，即角色扮演<br>应用场景：需要特定身份和专业领域知识场景。例如编程助手，智能问数助手等。<br>应用效果：角色提醒可以提升LLM的性能和准确性，并改善LLM的任务边界遵守，可持续用户的整个对话周期。</p><h4 id="User-Prompt-用户提示词"><a href="#User-Prompt-用户提示词" class="headerlink" title="User Prompt(用户提示词)"></a>User Prompt(用户提示词)</h4><p>定义：为了获取AI的具体回答或者完成特定任务，由用户输入的问题、请求和命令。<br>User Prompt(用户提示词)类型(按照提示方式分类)：<br><strong>陈述型提示(命令Prompt)</strong>：<br>定义：用陈述句或者命令式语句直接告诉模型执行某项任务、清晰描述期望输出的Prompt方式。<br>应用场景：定义明确、无歧义性、可有一定的结果预期的场景。<br>应用效果：适合模型直接产出所需结果，例如纪要总结、翻译等。<br><strong>问答型提示(询问Prompt)</strong>：<br>定义：用提问句或者疑问式语句向模型提问，不断尝试获取答案的Prompt方式。<br>应用场景：模拟问答、解释说明、咨询建议等场景。<br>应用效果：一句话的问题让模型直接回答要点。可以直接模拟问答场景，适合检索事实或者获取解释。</p><h4 id="Chain-of-Thought-思维链提示-CoT"><a href="#Chain-of-Thought-思维链提示-CoT" class="headerlink" title="Chain-of-Thought(思维链提示, CoT)"></a>Chain-of-Thought(思维链提示, CoT)</h4><p>定义：引导模型展示逐步推理过程的Prompt方式。例如加入“让我们一步一步思考”指令（零样本CoT），或者包含推理过程示例来实现（少样本CoT）<br>应用场景：数据解题过程呈现，逻辑推理过程呈现。<br>应用效果：适合需要复杂推理、逐步分析的任务或问题。由于模型会展示每一步，这样就容易理解模型为什么会得出这个结论，进而判断这个结论是否满足准确性和可解释性的要求。保留思考过程对于优化Prompt设计至关重要。<br>局限性：这对模型规模有要求，小模型输出的推理链往往不可靠，反而准确性下降，也增加了输出内容对提示微调的敏感度。<br>思维链提示类型(按样本分类)：<br><strong>零样本提示(zero-shot prompting)</strong>：<br>定义：在不提供示例的情况下，仅通过指令或者问题来让模型完成任务。优点是无许训练，快速验证效果，缺点就是受模型影响比较大，复杂任务的效果差。<br><strong>单样本提示(one-shot prompting)</strong>：<br>定义：提供一个示例（输入-输出对），让模型根据这个示例来类比处理新输入。优点是简单任务效果提升明显，缺点是可能会过拟合，示例质量对结果影响大。<br><strong>少样本提示(few-shot prompting)</strong>：<br>定义：提供多个示例（2-5对）展示任务的输入输出模式，让模型根据这个模式来类比处理新任务，归纳任务规律。优点是应对复杂任务泛化能力强，其在数学等推理任务上，相比零样本提示，能显著提升LLM的能力。缺点是对示例数量、质量要求比较高，消耗更多的token，成本比较高。<br>few-shot prompting格式化形式样例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Q: &lt;Question&gt;?</span><br><span class="line">A: &lt;Answer&gt;</span><br><span class="line"></span><br><span class="line">Q: &lt;Question&gt;?</span><br><span class="line">A: &lt;Answer&gt;</span><br><span class="line"></span><br><span class="line">Q: &lt;Question&gt;?</span><br><span class="line">A: &lt;Answer&gt;</span><br><span class="line"></span><br><span class="line">Q: &lt;Question&gt;?</span><br><span class="line">A: </span><br></pre></td></tr></table></figure><h3 id="Prompt构建技巧-以DeepSeek-R1为例"><a href="#Prompt构建技巧-以DeepSeek-R1为例" class="headerlink" title="Prompt构建技巧(以DeepSeek-R1为例)"></a>Prompt构建技巧(以DeepSeek-R1为例)</h3><h4 id="高质量提示词设计原则"><a href="#高质量提示词设计原则" class="headerlink" title="高质量提示词设计原则"></a>高质量提示词设计原则</h4><p><strong>澄清意图，指令清晰具体，毫不猜测（避免含糊不清）</strong>：使用明确、清晰、精准、不歧义的语言描述需求、意图，从而让LLM毫不猜测理解任务，错判意图。<br>　　技巧：对于明确性任务，可以设置字段名、表名、格式等，避免格式错乱。或者加入具体细节或限制性条件；提示词避免过于宽泛含糊或者措辞模棱两可，也不宜太过繁琐复杂。<br><strong>明确任务和预期输出</strong>：清楚地定义LLM要完成的任务，并指出期望的回答答复形式或者风格。<br>　　技巧：提示中指明期望的输出格式或者评价标准。<br><strong>给足信息、必要背景、数据或上下文信息</strong>：提示词中的背景信息会影响回复内容，原因在于LLM对最靠近的上下文信息非常敏感，所以要提供供任务完成所必须的背景、数据等细致详尽信息。可以先提出问题，再补充要求，最后在限定输出。<br>　　技巧：在提示词中补充必须信息，相关信息要全，不相关的信息一律不准出现。可以不添加额外的系统提示，所有指令可以直接都包含在用户提示中。对于少样本提示，应该避免提供示例，否则可能降低模型性能。只需要详细描述希望LLM解决的问题、执行的任务和输出格式。若必须使用，需要确保示例和任务高度一致。<br><strong>必要约束信息</strong>：在提示词中设置LLM一定边界，使得LLM聚焦输出内容，避免无关信息，在对回复内容有严格格式的场景更适用。<br>　　技巧：可就场景做示例举例，进而使任务的内容、要求更加明确，启发LLM正确思考，并实现限定输出形式。<br><strong>提示词结构清晰，避免一次堆叠过多指令内容</strong>：可分为角色扮演、条件指令、任务拆解、格式提示、结构化标记和示例。避免一次任务要求LLM同时完成多项任务或者设置多重限制约束条件。<br>　　技巧：<br>　　1. 角色扮演：指定角色，首先赋予LLM特定语境、立场、角色，有助于LLM更精确地理解任务，要求回答的内容风格要专业一致，这会影响LLM回答的专业度和口吻。例如，现在，你是*角色，正在做*事+对角色的要求。<br>　　2. 条件指令：一般是在交互式场景下的简单明了问题，可用户提示词方式提问。<br>　　3. 任务拆解（分步指引）：复杂逻辑分析是LLM的短板，可以将复杂任务或推理计算，按照分步思考、逐步推理的方式，将多个子任务拆解执行。分步完成替代一次完成，多次对话交互完成复杂任务细化。对于必要分支设置简化逻辑，预判岔路，预判岔路并指定处理方式，利用添加的细节描述，进而使任务要求明确。<br>　　4. 格式提示：设置期望的输出格式，例如常用的JSON、Markdown等，可以使用占位符或者示例格式。<br>　　5. 结构化标记和示例：利用分割符等标志，区分各种提示模块内容；利用指令&#x2F;上下文&#x2F;示例&#x2F;输入&#x2F;输出等设置醒目提示标签。明确输出的特征和质量。</p><h4 id="DeepSeek-R1的API调用"><a href="#DeepSeek-R1的API调用" class="headerlink" title="DeepSeek-R1的API调用"></a>DeepSeek-R1的API调用</h4><p>DeepSeek-R1的API调用<br>　　官方API调用平台：<a href="https://platform.deepseek.com/api_keys">https://platform.deepseek.com/api_keys</a><br>　　DeepSeek API文档：<a href="https://api-docs.deepseek.com/zh-cn/">https://api-docs.deepseek.com/zh-cn/</a><br>使用注意：</p><ol><li>API key 仅在创建时可见可复制，请妥善保存。不要与他人共享API key，或将其暴露在浏览器或其他客户端代码中。为了保护帐户安全，官方可能会自动禁用发现已公开泄露的API key。</li><li>DeepSeek API 使用与 OpenAI 兼容的 API 格式，可以使用 OpenAI SDK 来访问 DeepSeek API，或使用与 OpenAI API 兼容的软件。</li><li>OpenAI的ChatCompletion等对话模型支持系统提示词。</li><li>调用对话API样例脚本(Python)：</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># Please install OpenAI SDK first: `pip3 install openai`</span><br><span class="line"></span><br><span class="line">from openai import OpenAI</span><br><span class="line"></span><br><span class="line">client = OpenAI(api_key=&quot;&lt;DeepSeek API Key&gt;&quot;, base_url=&quot;https://api.deepseek.com&quot;)</span><br><span class="line"></span><br><span class="line">response = client.chat.completions.create(</span><br><span class="line">    model=&quot;deepseek-chat&quot;,</span><br><span class="line">    messages=[</span><br><span class="line">        &#123;&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant&quot;&#125;,</span><br><span class="line">        &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hello&quot;&#125;,</span><br><span class="line">    ],</span><br><span class="line">    stream=False</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">print(response.choices[0].message.content)</span><br></pre></td></tr></table></figure><h4 id="DeepSeek-R1调用方式-传统调用和Function-Calling"><a href="#DeepSeek-R1调用方式-传统调用和Function-Calling" class="headerlink" title="DeepSeek-R1调用方式:传统调用和Function Calling"></a>DeepSeek-R1调用方式:传统调用和Function Calling</h4><h5 id="传统调用"><a href="#传统调用" class="headerlink" title="传统调用"></a>传统调用</h5><p>使用对话API中样例式样，支持系统提示词。其模型参数如下：<br>温度(temperature):temperature 参数默认为 1.0，用于控制输出的随机性，较低值会让回复更准确、更一致，而较高值会增加创造性和多样性。0-0.2会确保结果稳定一致，0.7及其以上获取会更加多样性。<br>DeepSeek建议场景设置温度如下：</p><table><thead><tr><th align="left">场景</th><th>温度</th></tr></thead><tbody><tr><td align="left">代码生成&#x2F;数学解题</td><td>0.0</td></tr><tr><td align="left">数据抽取&#x2F;分析</td><td>1.0</td></tr><tr><td align="left">通用对话</td><td>1.3</td></tr><tr><td align="left">翻译</td><td>1.3</td></tr><tr><td align="left">创意类写作&#x2F;诗歌创作</td><td>1.5</td></tr></tbody></table><p>核采样(top_p): 核采样系数，用于控制输出的概率分布。<br>最大长度(max_tokens): 限制回答长度，可根据需要设置上限以避免无休止的长篇输出，降低成本（根据DeepSeek定价，每百万tokens输出12元）。<br>Token 用量计算：token 是模型用来表示自然语言文本的基本单位，DeepSeek计费单元，可以直观的理解为“字”或“词”；通常1个中文词语、1个英文单词、1个数字或1个符号计为1个token。一般情况下模型中token和字数的换算比例大致如下：1个英文字符 ≈ 0.3个token。1个中文字符 ≈ 0.6个token。</p><h5 id="工具调用-Function-Calling-函数调用"><a href="#工具调用-Function-Calling-函数调用" class="headerlink" title="工具调用(Function Calling, 函数调用)"></a>工具调用(Function Calling, 函数调用)</h5><p>定义：通过引入外部工具或者开发者提供的函数，使大模型可以以结构化方式调用外部工具或者函数信息进行回答，从而解决大模型原本无法解决的问题。由OpenAI在chat completions的API中添加的新能力。<br>场景：适合于工具性提示设计场景，提升LLM在工具或函数调用、数据提取场景下的使用可靠性问题，减少自由文本解析导致的麻烦。<br>OpenAI的Function Calling官方文档地址：<a href="https://platform.openai.com/docs/guides/function-calling#page-top">https://platform.openai.com/docs/guides/function-calling#page-top</a></p><h4 id="DeepSeek-R1下Prompt特点"><a href="#DeepSeek-R1下Prompt特点" class="headerlink" title="DeepSeek-R1下Prompt特点"></a>DeepSeek-R1下Prompt特点</h4><p>DeepSeek-R1擅长Prompt类型：复杂推理数学、代码调试；超长文档摘要、长对话跟踪；知识问答并解释。<br>DeepSeek-R1不擅长Prompt类型：格式严格输出；少样本提示学习；创意类、深度角色扮演；快速简单直白任务。<br>原因分析：通过对DeepSeek-R1内容指令遵循能力实现机制的分析，其MoE大规模混合专家中有专精于数据代码等专家，RL阶段的特别训练，进一步增强了在复杂推理数学方面的适应能力。其MLA多头潜变量注意力，降低显存的同时，增强了上下文长对话或文档的一次吞吐和全局一致性。</p><h4 id="Prompt使用适配要求-以DeepSeek-R1为例"><a href="#Prompt使用适配要求-以DeepSeek-R1为例" class="headerlink" title="Prompt使用适配要求(以DeepSeek-R1为例)"></a>Prompt使用适配要求(以DeepSeek-R1为例)</h4><p>单轮直接指令(Instruction + Context(必须) + 明确要求):以单轮问答形式工作，将上下文和指令内容合并一起，在一次用户提示中清晰给出，这样有助于LLM提取要求并简答。<br>明确角色并非系统提示词(角色设定 + 任务描述)：在用户提示内扮演角色做风格模仿引导，利用多轮会话每次重申角色设置。这样有助于LLM朝着所需风格方式生成文本。<br>结构化提示：适度结构化提示词有利于解析任务内容，明确指出格式规范。<br>避免过度复杂和示例过载：少样本提示或者过长的提升反而有可能让LLM性能下降，需要保持指令具体而且要聚集。</p><h3 id="Prompt模板"><a href="#Prompt模板" class="headerlink" title="Prompt模板"></a>Prompt模板</h3><h4 id="Prompt模板-1"><a href="#Prompt模板-1" class="headerlink" title="Prompt模板"></a>Prompt模板</h4><p>定义：给任务设计的模板或范式，结果形式中通常包含特定的入参位置和提示词。<br>Prompt模板四要素：立角色、述问题、定目标、补要求。<br>立角色：明确按上一个专家角色。<br>述问题：说清楚问题、背景和实际情况。<br>定目标：确定输出的内容是什么，要求LLM做什么。<br>补要求：补充特别注意事项。<br>注意：20%的世界用于撰写Prompt模板，补齐模板四要素，将80%的时间（大约两周）投入到Prompt模板的调试优化过程中，。</p><h4 id="Prompt模板设计注意事项"><a href="#Prompt模板设计注意事项" class="headerlink" title="Prompt模板设计注意事项"></a>Prompt模板设计注意事项</h4><ol><li>Prompt模板化，可以从简单开始，不断迭代优化。</li><li>要求等重要内容要放到Prompt后面。</li><li>需要显示明确说明使用或者参考的经验知识。</li><li>最好让模型输出CoT的思考过程，这个在模板反复迭代设计过程中很有帮助。同时要考虑提示的长度，思考细节要相关，对结果有贡献，太多不必要的细节不一定是好的方法。</li><li>Prompt的特殊格式和符合会对LLM的返回结果有影响，建议使用清晰的分隔符来分隔指令和上下文，例如“###”。</li><li>对于难以一次实现的复杂任务，可以通过拆解来实现。</li><li>设计提示时，避免说出什么不要做，而是要说出应该做什么。</li><li>学习Prompt模板可以从示例开始，可以利用官方的Prompt模板库，例如<a href="https://api-docs.deepseek.com/zh-cn/prompt-library/">DeepSeek API文档官方提示库</a></li><li>针对数据任务可以特殊处理。</li><li>设置合理的参数：温度(temperature)、核采样(top_p)设置要合理，以避免输出不相关或重复的内容。</li><li>补齐短板，善加引导。引导模型重复思考，慢慢捋清思路和步骤，并自检每一步的正确性。不要急于求成。</li></ol><h4 id="Prompt提示词库"><a href="#Prompt提示词库" class="headerlink" title="Prompt提示词库"></a>Prompt提示词库</h4><table><thead><tr><th align="left">典型Prompt库</th></tr></thead><tbody><tr><td align="left"><a href="https://platform.openai.com/docs/guides/prompting">OpenAI官方提示词指南</a></td></tr><tr><td align="left"><a href="https://prompts.fresns.cn/">ChatGPT分门别类的引导语大全</a></td></tr><tr><td align="left"><a href="https://docs.claude.com/zh-CN/docs/build-with-claude/prompt-engineering/overview">Claude 提示词工程指导文档</a></td></tr><tr><td align="left"><a href="https://github.com/NirDiamant/Prompt_Engineering">Nir Diamant发布的提示词工程技术库</a></td></tr><tr><td align="left"><a href="https://api-docs.deepseek.com/zh-cn/prompt-library/">DeepSeek API文档官方提示库</a></td></tr></tbody></table><h3 id="Prompt-Engineering-提示词工程-流程"><a href="#Prompt-Engineering-提示词工程-流程" class="headerlink" title="Prompt Engineering(提示词工程)流程"></a>Prompt Engineering(提示词工程)流程</h3><h4 id="Prompt-Engineering-提示词工程-流程-1"><a href="#Prompt-Engineering-提示词工程-流程-1" class="headerlink" title="Prompt Engineering(提示词工程)流程"></a>Prompt Engineering(提示词工程)流程</h4><p>流程:评测准备、模板细化、验证调优、Prompt自动优化</p><ol><li>评测准备：根据业务目标、场景技能和背景，设计评测指标和评测数据集，完成目标设定和评测数据准备。<br>注意：<br>　　2. 评测数据集应包含调试数据集和验证数据集，数据集内容应包括Prompt输入和预期输出。<br>　　3. 评测数据集在数量上要保证覆盖所有子场景，按照实际业务比例分布，调试和验证数据集内容不重复。在质量上，评测数据集要保证准确，选取最新的知识内容，最好是业务的真实案例或者符合业务实际的场景情况。对业务高频使用的数据一定要纳入到数据集中。<br>　　4. 评测指标可分为自动和人工专家评测指标，用自动化评测和人工专家评测方案获取评估结果。<br>　　5. 评测驱动开发：业务场景分析-&gt;设计评测指标和数据集-&gt;评测任务执行-&gt;评测结果分析。评测质量决定Prompt迭代优化。</li><li>模板细化：在Prompt模板四要素基础上进一步细化分解，利用内容的不断补全提升模型效果。<br>注意：<br>　　可以通过机构化的模板构造格式清晰的Prompt，明确角色的各项对应能力和背景，并针对性的对已有内容补充完善。</li><li>验证调优：根据评测数据集的内容，可分为调试验证和评测验证。<br>　　调试验证：重点关注生成结果的质量，可利用控制变量的方法不断迭代循环，利用得到的Badcase,借助<strong>Prompt模板调优策略</strong>对Prompt模板做优化，确保正向演进，目标是实现Prompt模板在调试数据集上综合最优。<br>　　评测验证：利用调试验证通过后的Prompt模板，同时关注结果的质量和效率。针对评测中出现的Badcase，需要构造类似的场景的测试数据到调试数据集中，通过调试数据集验证后，再进行评测验证。评测验证过程中应该拉业务一起测试。</li><li>Prompt自动优化：用AI的方法优化Prompt模板。根据APO技术（自动提示词优化），构建自动优化模块或使用平台优化能力，确定目标LLM输入调试和评测数据集，即可得到优化后的Prompt模板。</li></ol><h4 id="Prompt模板调优策略"><a href="#Prompt模板调优策略" class="headerlink" title="Prompt模板调优策略"></a>Prompt模板调优策略</h4><ol><li>添加示例。利用少样本提示(few-shot prompting)等方式，让LLM触类旁通。在Prompt模板设计中，在结构化提示中专门加入一个示例区域，示范期望的输入和输出格式。或者设置示例库，利用动态示例技术（Dynamic Few-Shot）,将用户输入和示例库中的示例动态匹配，动态选择相似度最高的示例作为用户输入的参考示例，进而提升LLM回答准确率。</li><li>添加专家背景。利用专家对信息或分类的详细业务描述，给提取的字段或分类下业务定义，让LLM更好理解意图。或者在背景中添加业务总结的内部业务规则，使LLM通过Prompt模板学习内容业务规则，并在输出时遵循这个规则来思考。优点是可以解决推理错误、信息提取和分类错误、LLM幻觉问题。</li><li>添加CoT(思维链)。在Prompt模板的背景知识中添加专门的区域，明确LLM应该执行思考过程，或者在Prompt模板中添加“一步一步思考”的指令，让LLM对问题逐步思考，让LLM学会推理。优点是可以解决推理错误、LLM幻觉问题。</li><li>Prompt模板拆分。将原来Prompt长模板拆分为多个Prompt短模板，各种完成部分任务，进而减少对LLM的内容输入，提升LLM单任务的准确性。优点是可以解决推理错误、信息提取和分类错误问题。</li><li>其他方法：Prompt模板添加“反复强调”内容或者对不清楚的问题回答不知道；要求模型不要做什么；在Prompt模板中将重要内容放在后面。</li></ol><h4 id="Prompt-Engineering-提示词工程-注意事项"><a href="#Prompt-Engineering-提示词工程-注意事项" class="headerlink" title="Prompt Engineering(提示词工程)注意事项"></a>Prompt Engineering(提示词工程)注意事项</h4><ol><li>必须科学地构建评测集，单靠case by case来发现问题，导致问题永远也找不完，影响上线验收的标准。</li><li>选择合适的LLM，要综合考虑资源和效果。在LLM更新或者升级时不要盲目乐观，有些精心设计的Prompt，升级模型后反而有可能出现准确率下降的问题。</li><li>注重业务专家的知识和经验，Prompt Engineering(提示词工程)的实际落地过程中往往比一开始的设想更复杂。</li></ol><h2 id="学术界解决方案"><a href="#学术界解决方案" class="headerlink" title="学术界解决方案"></a>学术界解决方案</h2><h3 id="Prompt论文及优化方案"><a href="#Prompt论文及优化方案" class="headerlink" title="Prompt论文及优化方案"></a>Prompt论文及优化方案</h3><table><thead><tr><th align="left">论文</th><th align="left">内容</th></tr></thead><tbody><tr><td align="left">[1] <a href="https://arxiv.org/abs/2411.10541">Does Prompt Formatting Have Any Impact on LLM Performance?</a></td><td align="left">2024年Jia He等人提出，本文探讨了不同提示模板对 LLM 性能的影响。实验表明，不存在适用于所有任务和模型的通用格式，鲁棒性和模型规模相关，Prompt格式变化对模型性能有统计学上的显著影响。</td></tr><tr><td align="left">[2] <a href="https://arxiv.org/abs/2409.14469">Rethinking Semantic Parsing for Large Language Models: Enhancing LLM Performance with Semantic Hints</a></td><td align="left">2024年Kaikai An等人提出，在prompt中嵌入语义提示能够持续提升LLM在各种任务上的性能。</td></tr><tr><td align="left">[3] <a href="https://arxiv.org/abs/2503.10084">Why Prompt Design Matters and Works: A Complexity Analysis of Prompt Search Space in LLMs</a></td><td align="left"><strong>重点</strong>，2025年Xiang Zhang等人提出，Prompt工程的理论基础。本论文提供了一个理论框架，解释了为什么有些提示成功而有些提示失败。我们表明，提示充当选择器，在 CoT 推理过程中从模型的完整隐藏状态中提取与任务相关的信息。每个提示都定义了一条穿过答案空间的独特轨迹，而轨迹的选择对于任务性能和未来在空间中的导航至关重要。Prompt之所以有效，是因为它能够科学地指导模型在复杂的推理链条中，每一步都抓重点。设计最优Prompt，就需要深入理解任务的计算需求，并确保提示能够引导LLM在每一步都准确地抓住并用好解决问题所需的核心信息。GitHub地址：<a href="https://github.com/juntaic7/CoT-with-Supervision">https://github.com/juntaic7/CoT-with-Supervision</a></td></tr><tr><td align="left">[4] <a href="https://arxiv.org/abs/2211.01910">Large language models are human-level prompt engineers</a></td><td align="left">2022年Yongchao Zhou等人提出，提出自动提示词工程 (Automatic Prompt Engineer，APE)，用于自动生成和选择指令，能显著改善零样本提示、少样本提示和思维链下LLM的表现。GitHub地址：<a href="https://github.com/keirp/automatic_prompt_engineer">https://github.com/keirp/automatic_prompt_engineer</a></td></tr><tr><td align="left">[5] <a href="https://arxiv.org/abs/2305.03495">Automatic Prompt Optimization with “Gradient Descent” and Beam Search</a></td><td align="left">2023年Reid Pryzant等人提出，Automatic Prompt Optimization (自动提示词优化，APO)，利用LLM在离散的token上做Prompt优化</td></tr><tr><td align="left">[6] <a href="https://arxiv.org/abs/2309.03409">LARGE LANGUAGE MODELS AS OPTIMIZERS</a></td><td align="left">2023年Chengrun Yang等人提出，Optimization by PROmpting (OPRO)方法，使用LLM本身做为优化器，通过不断迭代来寻找问题最佳的Prompt，GitHub地址：<a href="https://github.com/google-deepmind/opro">https://github.com/google-deepmind/opro</a></td></tr><tr><td align="left">[7] <a href="https://arxiv.org/abs/2309.08532">EvoPrompt: Connecting LLMs with Evolutionary Algorithms Yields Powerful Prompt Optimizers</a></td><td align="left">2023年Qingyan Guo等人提出，用于离散提示优化的全新框架 EvoPrompt, 将Prompt中离散的token视为进化算法(EA)中基因序列，通过选择、变异等手段，从而进化出最优的Prompt。GitHub地址：<a href="https://github.com/beeevita/EvoPrompt">https://github.com/beeevita/EvoPrompt</a></td></tr><tr><td align="left">[8] <a href="https://arxiv.org/abs/2310.16427">PromptAgent Strategic Planning with Language Models Enables Expert-level Prompt Optimization</a></td><td align="left">2023年Xinyuan Wang等人提出，PromptAgent一种优化方法，可以自主生成与专家手动生成的提示质量相当的提示。</td></tr><tr><td align="left">[9] <a href="https://arxiv.org/abs/2502.12215">Chain of Draft: Thinking Faster by Writing Less</a></td><td align="left">2025年Silei Xu等人提出草稿链(CoD)的新范式，在 CoD 中，LLM 在解决任务时生成简洁但信息丰富的中间推理输出。通过减少冗长程度并专注于关键洞察，CoD 在准确率上达到甚至超越 CoT。GitHub地址：<a href="https://github.com/sileix/chain-of-draft">https://github.com/sileix/chain-of-draft</a></td></tr><tr><td align="left">[10] <a href="https://arxiv.org/abs/2311.04155">Black-Box Prompt Optimization: Aligning Large Language Models without Model Training</a></td><td align="left">2023年Jiale Cheng等人提出， Black-Box Prompt Optimization (黑盒提示优化，BPO)，其理念是优化用户提示以适应 LLM 的输入理解，从而在不更新 LLM 参数的情况下最大限度地实现用户的意图。GitHub地址：<a href="https://github.com/thu-coai/BPO">https://github.com/thu-coai/BPO</a></td></tr><tr><td align="left">[11] <a href="https://arxiv.org/abs/2505.08303">Evaluating the Effectiveness of Black-Box Prompt Optimization as the Scale of LLMs Continues to Grow</a></td><td align="left">2025年Ziyu Zhou等人提出，验证不同的黑盒提示优化方法在越来越大的LLM的性能表现，发现黑盒优化方法的有效性随着模型尺寸的增加而降低。</td></tr><tr><td align="left">[12] <a href="https://arxiv.org/abs/2504.07357">Revisiting Prompt Optimization with Large Reasoning Models—A Case Study on Event Extraction</a></td><td align="left">2025年Saurabh Srivastava等人提出，在参数规模越来越大、性能越来越好的大型推理模型中，提示优化仍然可以给模型带来性能上的提升。</td></tr><tr><td align="left">[13] <a href="https://arxiv.org/abs/2311.10117">Automatic Engineering of Long Prompts</a></td><td align="left">2023年Cho-Jui Hsieh等人提出，提出新的自动长提示词工程算法，并证明该算法能有效提升准确率。</td></tr><tr><td align="left">[14] <a href="https://arxiv.org/abs/2101.06804">What Makes Good In-Context Examples for GPT-3</a></td><td align="left"><strong>重点</strong>。2021年Jiachang Liu等人提出，提出检索与测试样本语义相似的样本来构建其对应的提示词，基于检索的提示选择方法始终优于随机基准方法。</td></tr><tr><td align="left">[15] <a href="https://arxiv.org/abs/2201.11903">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</a></td><td align="left"><strong>重点</strong>。2022年Jason Wei等人提出，首次提出CoT(思维链)，通过让LLM逐步推理，将复杂问题分解为多个简单子问题来逐步推理。</td></tr><tr><td align="left">[16] <a href="https://arxiv.org/abs/2411.10541v1">Does Prompt Formatting Have Any Impact on LLM Performance</a></td><td align="left"><strong>重点</strong>。2024年Jia He等人提出，探讨了不同提示词模板对 LLM 性能的影响。</td></tr><tr><td align="left">[17] <a href="https://arxiv.org/abs/2310.11324">Quantifying Language Models’ Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting</a></td><td align="left">2023年Melanie Sclar等人提出.在少样本设置下，几种广泛使用的开源 LLM 对提示符格式的细微变化极为敏感。</td></tr></tbody></table><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] <a href="https://prompt-engineering.xiniushu.com/">面向开发者的 Prompt 工程</a><br>[2] <a href="https://www.promptingguide.ai/zh">Prompt Engineering Guide</a><br>[3] <a href="https://github.com/ChuxiJ/prompt-engineering-for-developers?tab=readme-ov-file">面向开发者的 LLM 入门课程</a><br>[4] <a href="https://prompt-guide.xiniushu.com/category/-basics">Prompt 学习指南</a><br>[5] <a href="https://github.com/yunwei37/Prompt-Engineering-Guide-zh-CN">提示词工程指南</a><br>[6] <a href="https://assh83.com/2024/06/23/%E3%80%8A%E9%9D%A2%E5%90%91%E6%99%AE%E9%80%9A%E4%BA%BA%E7%9A%84prompt%E5%B7%A5%E7%A8%8B%E6%93%8D%E4%BD%9C%E6%89%8B%E5%86%8C%E3%80%8B-%E5%9F%BA%E7%A1%80%E7%AF%87/">面向初学者的prompt工程保姆教程</a><br>[7] <a href="https://gptpmt.com/">什么是 Prompt Engineering？</a><br>[8] <a href="https://hub.baai.ac.cn/view/33671">OpenAI 官方 Prompt 工程指南</a><br>[9]<a href="https://solwen.ai/posts/what-is-prompt-engineering">Prompt Engineering 是什麼？提示工程指南：6 大關鍵原則！</a><br>[10] <a href="https://huggingface.co/blog/VirtualOasis/ai-prompt-engineering-and-agents">提示工程与AI智能体构建指南</a><br>[11] <a href="https://learningprompt.wiki/zh-Hans/">Learning Prompt</a><br>[12] <a href="https://blog.csdn.net/huyuyang6688/article/details/146217737">从新手到高手：全面解析 AI 时代的「魔法咒语」——Prompt</a><br>[13] <a href="https://learn.microsoft.com/zh-cn/azure/ai-foundry/openai/concepts/prompt-engineering?tabs=chat">微软提示工程技术</a><br>[14] <a href="https://www.rootstrap.com/blog/how-to-write-system-instructions-for-openais-gpt-4-chat-api">How to write “System” Instructions for OpenAI’s GPT-4 Chat API</a><br>[15] <a href="https://docs.claude.com/en/docs/build-with-claude/prompt-engineering/system-prompts">Giving Claude a role with a system prompt</a><br>[16] <a href="https://learnprompting.org/docs/introduction">Learn Prompting</a><br>[17] <a href="https://www.prompthub.us/blog/strategies-for-managing-prompt-sensitivity-and-model-consistency-">Strategies for Managing Prompt Sensitivity and Model Consistency</a><br>[18] <a href="https://api-docs.deepseek.com/zh-cn/">DeepSeek API 文档</a><br>[19] <a href="https://pub.towardsai.net/deepseek-r1-model-architecture-853fefac7050">DeepSeek-R1: Model Architecture</a><br>[20] <a href="https://www.lib.szu.edu.cn/sites/szulib/files/2025-03/%E5%8C%97%E4%BA%AC%E5%A4%A7%E5%AD%A6-DeepSeek%E7%B3%BB%E5%88%97-%E6%8F%90%E7%A4%BA%E8%AF%8D%E5%B7%A5%E7%A8%8B%E5%92%8C%E8%90%BD%E5%9C%B0%E5%9C%BA%E6%99%AF.pdf">DeepSeek提示词工程和落地场景</a><br>[21] <a href="https://docs.together.ai/intro">针对 DeepSeek R1 的提示工程指南</a><br>[22] <a href="https://blog.apiad.net/p/prompt-engineering">What you need to master Prompt Engineering</a><br>[23] <a href="https://davidmoore.io/how-to-talk-to-ai-part-2-good-prompt-bad-prompt/">How to talk to AI Part 2 – Good Prompt&#x2F;Bad Prompt</a><br>[24] <a href="https://help.aliyun.com/zh/model-studio/qwen-function-calling#ecbc6ae7f20im">Function Calling</a><br>[25] <a href="https://baoyu.io/blog/google-prompt-engineering-whitepaper">Google 官方提示工程 (Prompt Engineering)白皮书</a><br>[26]<a href="https://blog.langchain.com/dynamic-few-shot-examples-langsmith-datasets/">Dynamic few-shot examples with LangSmith datasets</a></p>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
            <tag> 算法 </tag>
            
            <tag> 应用 </tag>
            
            <tag> Prompt Engineering </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>算法文档编写要点</title>
      <link href="/2025/09/14/%E7%AE%97%E6%B3%95%E6%96%87%E6%A1%A3%E7%BC%96%E5%86%99%E8%A6%81%E7%82%B9/"/>
      <url>/2025/09/14/%E7%AE%97%E6%B3%95%E6%96%87%E6%A1%A3%E7%BC%96%E5%86%99%E8%A6%81%E7%82%B9/</url>
      
        <content type="html"><![CDATA[<p>算法文档编写要点，主要总结在算法文档编写过程中，文档内容中必须要有的关键事项，以方便日后开发人员按照文档开发。</p><ol><li><p>算法简单介绍：</p><p>编写要点：内容简明扼要，让使用人快速了解算法的用途和关键点，可以在算法中添加背景知识、历史知识和典型应用场景，方便快速理解。</p></li><li><p>算法原理详述：</p><p>介绍从算法背景知识开始，详述算法的涉及概念、核心思想、算法公式流程、算法实现方法、过程和结果等，阐明算法的工作机理，对于算法的优势劣势、性能限制局限性、适用场景等关键因素要特别关注和解释，以确保使用者快速判断算法适用情况。</p></li><li><p>算法功能说明：</p><p>3.1 功能举例：<br>3.1.1 功能说明：对功能简明扼要介绍<br>3.1.2 入参说明：表格形式说明参数名、数据类型、取值范围、默认值、备注、是否必须、数据内容样例等信息，并在备注等部分说明参数的作用。<br>3.1.3 调用示例：<br>是该算法的示例代码或伪代码，以方便使用者快速上手。示例代码要详细清晰，能重复展现算法的使用方法和功能。对于涉及多场景多应用的使用情况，需要提供每个场景下的示例，分门别类加以说明。<br>3.1.4 结果输出：<br>结果输出要给出样例，特别是涉及多场景情况下。输出要详细说明输出的内容（字段名）、类型、格式等信息，并说明其含义。<br>3.1.5 参考文献：<br>说明算法来源的文档、论文书籍等证明材料，涉及原创和专利要注意使用要求，涉及开源软件要求要参照开源协议使用。<br>3.2 其他功能近似：</p></li><li><p>使用开发指南和案例：</p><p>4.1 案例介绍：<br>重点介绍案例的使用场景<br>4.2 安装配置：<br>详细说明算法软件包的安装配置方法，包括环境准备、配置依赖、安装设置、使用检查、代码仓配置等信息，确保算法正确安装并能正常使用。<br>4.3 环境及数据准备：<br>在算法正式使用前，准备好环境配置和数据准备，特别是数据准备，数据的清洗、转换和预处理、结果可视化等动作要详细说明，确保数据符合算法入参要求，输出结果能正常呈现。涉及任务连续编排(Pipeline)功能的，最好图像化展示从头到尾的运行流程，说明Pipeline中的组件内容和运行结果。<br>4.3.1 模型训练和推理：<br>基于算法构建模型后，需要对模型先训练，后推理。文档说明中需要包括对训练数据的描述，模型训练和推理硬件软件配置、模型训练的配置等内容，对模型训练后保存的结果，是保存模型本身还是保存模型参数，保存位置等也要加以说明，模型命名和保存名要遵从模型开发和设计规范。<br>4.4 开发及使用样例：<br>代码开发要遵守开发规范，算法的样例代码和实际使用场景，要确保使用者正确理解和使用。<br>开发过程中要及时完成单元测试和质量检查，包括本地开发环境和线上测试环境代码运行通过，算法正常使用。算法设计和开发过程中涉及的需求变更，也要及时记录到文档中，用于事后可追溯。<br>4.5 备注和说明<br>针对使用过程中容易发生的典型情况和问题，给出样例、使用说明和处理方案、调试策略等，确保使用者快速解决使用过程中发现的典型问题。<br>算法设计和开发，一般要基于算法平台环境配置和操作，也需要在文档中对算法平台的行为做说明，包括环境组件配置、部署上线、权限实施、运维执行和执行监控等</p></li><li><p>版本规范说明：</p><p>按照架构设计和规范要求，说明并记录设计文档的版本号、变更说明、修改时间、变更人信息等。其中变更说明部分要详细说明变更内容和关联影响，方便使用者快速判断版本改进情况和对自身使用的影响。</p></li><li><p>评审结论：</p><p>算法设计和开发都需要经过严格评审，需要经过评审后给出评审记录(评审时间、评审人、评审结论)。</p></li></ol><h3 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h3><p>[1] <a href="https://blog.csdn.net/Linshaodan520/article/details/134759092">项目开发与算法开发基本流程</a><br>[2] <a href="https://cs.nju.edu.cn/lixuandong/guide">软件文档编写指南</a><br>[3] <a href="https://www.infoq.cn/article/o7076wcbpbkwewjbtbyz">谷歌软件工程师是怎样写设计文档的</a><br>[4] <a href="https://www.banlikanban.com/info/wiki/softwarepm/3498.html">什么是详细设计说明书？设计时需要考虑哪些关键要点</a></p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
            <tag> 经验总结 </tag>
            
            <tag> 文档管理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Git使用指南</title>
      <link href="/2025/09/13/Git%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/"/>
      <url>/2025/09/13/Git%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/</url>
      
        <content type="html"><![CDATA[<p>本人之前使用的版本控制工具是SVN，最近换成了Git,总结一下Git过程的使用经验，总结教训。</p><h3 id="版本控制工具历史"><a href="#版本控制工具历史" class="headerlink" title="版本控制工具历史"></a>版本控制工具历史</h3><ol><li>版本控制工具起源：diff和patch</li><li>最早的本地版本控制工具：RCS(Revision Control System)</li><li>集中式版本控制工具：</li></ol><ul><li>SVN(Subversion)：用于取代CVS(Concurrent Versions System)的更好用的版本控制系统。不适合跨地域的协作式开发，不适合对代码高质量追求和代码门禁，其操作依赖服务器，可以用于word这类二进制文件的版本控制，适合人数不多的项目。</li></ul><ol start="4"><li>分布式版本控制工具：</li></ol><ul><li>Git, 2005年诞生。Git不适合word这类二进制文件的版本控制，由于需要整体的读授权，其不能将读授权精细到目录级别。适合分布式开发和移动办公。Git和其他版本控制工具的主要差别在于Git对待数据的方式。其他版本控制工具记录的是差异，而Git记录的是快照，即把数据看做是对文件系统的一组快照。提交和保存项目，Git会对当时的全部文件制造快照并保存快照索引。Git在实际使用中，主要用于对代码仓代码进行管理。</li><li>TortoiseGit：Windows系统下的开源Git图形化操作工具。TortoiseGit支持多种语言, TortoiseGit下载链接:<a href="https://tortoisegit.org/download/#Language_Packs">https://tortoisegit.org/download/#Language_Packs</a></li></ul><h3 id="Git安装配置"><a href="#Git安装配置" class="headerlink" title="Git安装配置"></a>Git安装配置</h3><ol><li>Linux安装:两种安装方式(包管理器安装和源代码安装)</li></ol><ul><li>包管理器安装命令如下(以Ubuntu为例)：<br>sudo aptitude install git (必装软件包)<br>sudo aptitude install git-doc git-svn git-email gitk (依赖不同，需要单独安装)</li><li>Linux可以通过bash-completion软件包实现命令补齐功能，具体设置可以看<a href="https://blog.csdn.net/m0_60534883/article/details/146981814">Git安装及使用</a></li></ul><ol start="2"><li>Windows安装：Git官方网站下载安装和安装<a href="https://zhuanlan.zhihu.com/p/419092209">GitHub Desktop</a>(包含图形化和命令行版本的Git)</li><li>macOS安装： <a href="https://git-scm.com/downloads/mac">macOS Git安装程序</a><br>Git安装后一般还需要配置，才能和代码仓配合，用于代码的版本管理，该配置比较繁琐，配置时建议和有经验的人一起弄，配置好后一般不需要再改动，为防止遗忘，建议配置过程整理文档保存。</li></ol><h3 id="配置设置"><a href="#配置设置" class="headerlink" title="配置设置"></a>配置设置</h3><ol><li>配置分类：系统配置(–system)、用户配置(–global)和仓库配置(–local)，查看配置命令：git config</li><li>配置流程：配置个人身份(用户名和邮箱)-&gt;换行符文本配置(重点在于不同系统之间的差异)-&gt;文本编码配置(使用UTF-8编码)-&gt;服务器认证配置(一般是线上代码仓，协议认证方式和公钥)，配置详情请看<a href="https://blog.csdn.net/m0_60534883/article/details/146981814">Git安装及使用</a></li></ol><h3 id="Git概念"><a href="#Git概念" class="headerlink" title="Git概念"></a>Git概念</h3><table><thead><tr><th>区域和状态</th><th>中文名称</th><th>英文名称</th><th>用途</th></tr></thead><tbody><tr><td>工程区域</td><td>工作区</td><td>Working Directory</td><td>日常使用文件所在文件夹</td></tr><tr><td>工程区域</td><td>暂存区</td><td>stage</td><td>索引，在工程根目录.git&#x2F;index文件夹中</td></tr><tr><td>工程区域</td><td>版本库</td><td>Repository</td><td>本地仓库，工作区中隐藏的目录文件夹.git,用于存放工程所有版本数据</td></tr><tr><td>文件状态</td><td>已修改</td><td>modified</td><td>修改文件，文件未提交保存</td></tr><tr><td>文件状态</td><td>已暂存</td><td>staged</td><td>已修改文件已放在将要保存的清单中</td></tr><tr><td>文件状态</td><td>已提交</td><td>committed</td><td>文件已保存在本地数据库中</td></tr></tbody></table><h3 id="Git常用命令字典"><a href="#Git常用命令字典" class="headerlink" title="Git常用命令字典"></a>Git常用命令字典</h3><table><thead><tr><th>用途</th><th>命令</th><th>解释</th></tr></thead><tbody><tr><td>工程准备</td><td><a href="https://git-scm.com/docs/git-init/zh_HANS-CN">git init</a> 项目名或工程名</td><td>在本地目录下自动生成名为.git的目录，做为项目仓库。当前项目所在目录纳入Git管理,.git目录默认不可见</td></tr><tr><td>工程准备</td><td><a href="https://git-scm.com/docs/git-clone/zh_HANS-CN">git clone</a> url链接 &#x2F; git lfs clone url链接</td><td>将远端工程或项目克隆到本地磁盘，使用前提是要有该工程或项目的查看下载权限</td></tr><tr><td>工作区查看</td><td><a href="https://git-scm.com/docs/git-diff/zh_HANS-CN">git diff</a> 文件名 文件名</td><td>比较项目中任意两个节点&#x2F;分支&#x2F;索引的差异，在diff后添加–name-status参数，可以查看文件列表</td></tr><tr><td>工作区查看</td><td>git status</td><td>查看工作目录和暂存区状态，change to be committed(已暂存)、changes not staged for commit(未暂存)、untracked file(未被跟踪)</td></tr><tr><td>新增文件到暂存区</td><td><a href="https://git-scm.com/docs/git-add/zh_HANS-CN">git add</a>文件名</td><td>如果文件未被git跟踪，则需要先执行git add后，文件添加到暂存区后，再执行提交</td></tr><tr><td>删除在暂存区的文件</td><td><a href="https://git-scm.com/docs/git-rm/zh_HANS-CN">git rm</a> 文件名</td><td>将文件从当前分支的暂存区中删除，也可理解为从当前分支的下一步提交快照中删除，删除后文件将脱离git跟踪，不受git工程管理</td></tr><tr><td>文件移动和重命名</td><td><a href="https://git-scm.com/docs/git-mv/zh_HANS-CN">git mv</a> 文件名 新目录名&#x2F;新文件名</td><td>将文件从当前目录移动到新目录，或者将当前文件重新命名为新文件名</td></tr><tr><td>提交修改文件</td><td><a href="https://git-scm.com/docs/git-commit/zh_HANS-CN">git commit</a> 文件名 -m “提交描述信息”</td><td>暂存区文件改动提交到本地版本库中，提交的是本地动作，本地版本库记录改动，远端服务器不受影响</td></tr><tr><td>提交所有文件</td><td>git commit -am “提交描述信息”</td><td>一次提交所有暂存区改动文件到本地版本库中</td></tr><tr><td>查看日志</td><td><a href="https://git-scm.com/docs/git-log/zh_HANS-CN">git log</a></td><td>查看提交历史，可配置不同参数，按提交时间由近及远列出提交历史日志，包括提交ID、作者、提交时间、提交描述等</td></tr><tr><td>推送到远端仓库分支</td><td><a href="https://git-scm.com/docs/git-push/zh_HANS-CN">git push</a> origin 本地分支名:远端分支名</td><td>在git commit命令之后，将本地分支内容推送到远端分支上</td></tr><tr><td>撤销操作</td><td><a href="https://git-scm.com/docs/git-reset/zh_HANS-CN">git reset</a> commit_id</td><td>撤销工作区中的git add&#x2F;commit操作，将工作区内容回退到历史提交的commit_id节点，可配置参数</td></tr><tr><td>回退操作</td><td><a href="https://git-scm.com/docs/git-checkout/zh_HANS-CN">git checkout</a> . &#x2F; -文件名&#x2F; commit_id</td><td>回退本地所有修改而未提交的文件内容，取消所有本地工作区修改，使用-文件名，可回退某个单一文件的未提交改动,使用commit_id回退某个提交版本</td></tr></tbody></table><h3 id="Git分支管理"><a href="#Git分支管理" class="headerlink" title="Git分支管理"></a>Git分支管理</h3><ol><li>分支类型：<table><thead><tr><th>分支类型</th><th>特征名称</th><th>功能</th></tr></thead><tbody><tr><td>主分支</td><td>master&#x2F;main</td><td>常用，存储生产代码</td></tr><tr><td>开发分支</td><td>develop</td><td>常用，存储即将发布的代码，常用于开发人员开发使用</td></tr><tr><td>发布分支</td><td>release</td><td>常用，上线分支，准备发布上线使用</td></tr><tr><td>功能分支</td><td>feature</td><td>用于开发新功能</td></tr><tr><td>热修复分支</td><td>hotfix</td><td>用于紧急修复生产问题</td></tr></tbody></table></li><li>分支管理命令：<table><thead><tr><th>用途</th><th>命令</th></tr></thead><tbody><tr><td><a href="https://git-scm.com/docs/git-branch/zh_HANS-CN">git branch</a>  &#x2F;-r&#x2F;-a</td><td>查看本地工程的所有Git分支，-r查看远端服务器上的所有分支，-a查看远端+本地工程所有分支，”*”表示当前工作区所在分支</td></tr><tr><td>git branch 新分支名</td><td>基于当前分支节点创建新分支，不会切换到新分支，而使用git checkout -b 新分支名则可以切换到新分支</td></tr><tr><td>git branch -d&#x2F;-D 分支名</td><td>删除本地分支，-D表示强制删除情况，也可搭配参数删除远程分支</td></tr><tr><td><a href="https://git-scm.com/docs/git-checkout/zh_HANS-CN">git checkout</a> 分支名</td><td>切换检出分支，-f可以强制切换分支</td></tr><tr><td><a href="https://git-scm.com/docs/git-fetch/zh_HANS-CN">git fetch</a> origin 远端分支:本地分支</td><td>分支更新，从远端服务器获取远端分支后，对本地仓库里的本地分支进行更新</td></tr><tr><td><a href="https://git-scm.com/docs/git-merge/zh_HANS-CN">git merge</a>&#x2F;rebase 分支名</td><td>分支合并，从指定分支合并到当前分支</td></tr><tr><td><a href="https://git-scm.com/docs/git-pull/zh_HANS-CN">git pull</a> origin 远端分支:本地分支</td><td>分支合并，从远端服务器获取远端分支后，和本地分支进行自动合并</td></tr></tbody></table></li></ol><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><p>[1] <a href="https://git.p2hp.com/">Git中文网</a><br>[2] <a href="https://tortoisegit.org/">TortoiseGit官网</a><br>[3] <a href="https://blog.csdn.net/m0_60534883/article/details/146981814">Git安装及使用</a><br>[4] <a href="https://www.runoob.com/git/git-tutorial.html">Git教程</a></p>]]></content>
      
      
      <categories>
          
          <category> Git </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 基础 </tag>
            
            <tag> 项目管理 </tag>
            
            <tag> 版本控制 </tag>
            
            <tag> Git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python技巧-magic命令</title>
      <link href="/2025/09/13/Python%E6%8A%80%E5%B7%A7-magic%E5%91%BD%E4%BB%A4/"/>
      <url>/2025/09/13/Python%E6%8A%80%E5%B7%A7-magic%E5%91%BD%E4%BB%A4/</url>
      
        <content type="html"><![CDATA[<p>在IPython和Jupyter Notebook 高效magic命令。<br>magic命令：Jupyter Notebook中的特殊命令，使用%符号和要运行的命令一起使用。</p><h3 id="time"><a href="#time" class="headerlink" title="%%time"></a>%%time</h3><ol><li>功能：用于执行并反馈将代码运行一次后所花费的时间。输出CPU time(代码实际消耗的CPU运行时间)和Wall time(墙上挂钟时间, 代码从开始到结束所花费的，实际感受的运行时间)。</li><li>使用条件：在IPython和Jupyter Notebook中，必须放在测试单元代码的第一行，在%%time换行后，输入想要计时的一段或多段代码。</li><li>用途：</li></ol><ul><li>性能分析：提供代码运行时间信息。</li><li>代码执行时间评估：通过查看输出CPU时间和墙上时间，进而更全面的了解代码的运行时间。CPU 时间用于分析代码本身消耗的处理器资源，而墙上时间则反映了用户实际感受到的耗时。</li></ul><h3 id="time-1"><a href="#time-1" class="headerlink" title="%time"></a>%time</h3><ol><li>功能：将会给出当前行的代码运行一次后所花费的时间。输出Wall time(墙上时间, 代码从开始到结束所花费的实际时间)和代码运行结果。</li><li>使用条件：在IPython和Jupyter Notebook中，放在测试单元代码的第一行，在%time同行空格后，输入想要计时的一段或多段代码。</li><li>用途：检查代码运行时间并输出代码运行结果。</li><li>注意：也可以使用time库(import time),利用time.localtime(time.time())，返回当前时间的时间戳方法来计算时间间隔。</li></ol><h3 id="eit-r-R-n-N"><a href="#eit-r-R-n-N" class="headerlink" title="eit -r R -n N"></a>eit -r R -n N</h3><ol><li>功能：将执行代码语句运行R次，每次N遍，再对N * R 遍的运行结果取平均从而得到运行一遍代码的时间。</li><li>使用条件：在IPython和Jupyter Notebook中，必须放在测试单元代码的第一行，在%%timeit换行后，输入想要计时的一段或多段代码。如果放置在同行，则同行代码运行但不计时，从第二行才开始计时。</li><li>用途：检查代码运行时间并输出代码运行结果。</li><li>注意:%timeit的运行比%time执行时间快,在于%timeit内有额外的机制，用于防止系统调用(System calls)影响程序执行的时间结果。</li></ol><h3 id="prun"><a href="#prun" class="headerlink" title="%prun:"></a>%prun:</h3><ol><li>功能：宏观性能分析工具，用于计算函数或程序执行每个函数需要多长时间。</li><li>使用条件：在IPython和Jupyter Notebook中，放在测试单元代码的第一行，在%prun同行空格后，输入想要计时的一段或多段代码。结果返回列表，列表中是每个内部函数被调用次数、每次被调用时间、函数所有运行的累积时间。</li></ol><h3 id="lprun"><a href="#lprun" class="headerlink" title="%lprun:"></a>%lprun:</h3><ol><li>功能：微观性能分析工具，逐行计算程序性能，从而判断哪一行执行时间最长，进而优化那部分程序。</li><li>使用条件：需要安装第三方库: pip install line_profiler,在IPython中加载套件后，执行%lprun来计算函数的逐条性能。</li></ol><h3 id="mprun"><a href="#mprun" class="headerlink" title="%mprun:"></a>%mprun:</h3><ol><li>功能：使用单个语句执行的内存计算工具和代码一起执行。</li><li>使用条件：限制在独立模块上，不能应用在notebook上运行。</li></ol><h3 id="memit"><a href="#memit" class="headerlink" title="%memit:"></a>%memit:</h3><ol><li>功能：计时单个语句占用的内存空间。</li><li>使用条件：需要安装第三方库: pip install memory_profiler,在IPython中加载套件后，执行%memit来计算函数的逐条性能,从peak memory中。</li></ol><h3 id="who"><a href="#who" class="headerlink" title="%who"></a>%who</h3><ol><li>功能：在jupiter Notebook中显示所有可用变量。</li><li>使用条件：单独一行。</li></ol><h3 id="history-or-hist"><a href="#history-or-hist" class="headerlink" title="%history or %hist"></a>%history or %hist</h3><ol><li>功能：在jupiter Notebook中查看活动日志，并跟踪已经做过的内容。</li><li>使用条件：单独一行。</li></ol><h3 id="pinfo"><a href="#pinfo" class="headerlink" title="%pinfo"></a>%pinfo</h3><ol><li>功能：在jupiter Notebook中查看对象、包的详细信息（包括类型、长度、文件地址等）。</li><li>使用条件：单独一行。pinfo 包名或对象名。</li></ol><h3 id="writefile"><a href="#writefile" class="headerlink" title="%%writefile"></a>%%writefile</h3><ol><li>功能：在jupiter Notebook中用于在当前目录中，保存复用函数到Python文件中。</li><li>使用条件：单独一行,文件开头。%%writefile 文件名.py。</li></ol><h3 id="pycat"><a href="#pycat" class="headerlink" title="%pycat"></a>%pycat</h3><ol><li>功能：用于在当前目录中，读取Python文件中的复用函数到jupiter Notebook中，在新弹出窗口中显示文件中的所有代码。</li><li>使用条件：%pycat 文件名.py</li></ol><h3 id="quickref"><a href="#quickref" class="headerlink" title="%quickref"></a>%quickref</h3><ol><li>功能：在jupiter Notebook中，详细解释所有jupiter Notebook中存在的所有magic命令。</li><li>使用条件：单独一行 %quickref。</li></ol><h3 id="Jupiter-Notebook和Pycharm使用差异"><a href="#Jupiter-Notebook和Pycharm使用差异" class="headerlink" title="Jupiter Notebook和Pycharm使用差异:"></a>Jupiter Notebook和Pycharm使用差异:</h3><ol><li>命令中的数据集是否需要从网络中下载，还是直接运行本地数据集：</li></ol><ul><li>Jupiter Notebook：在本地浏览器环境中使用，网络(例如Github)数据集和本地数据集都可以正常使用。</li><li>Pycharm：除非设置好代理和打开防火墙，否则只能使用本地数据集，使用网络数据集会因为连接超时报错。</li></ul><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献:"></a>参考文献:</h3><p>[1] <a href="https://towardsdatascience.com/9-magic-command-to-enhance-your-jupyter-notebook-experience-101fb5f3a84/">9个可以提高Jupyter Notebook开发效率的魔术命令</a></p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> magic </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>专利基础知识整理</title>
      <link href="/2025/09/12/%E4%B8%93%E5%88%A9%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/"/>
      <url>/2025/09/12/%E4%B8%93%E5%88%A9%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/</url>
      
        <content type="html"><![CDATA[<h3 id="知识产权"><a href="#知识产权" class="headerlink" title="知识产权"></a>知识产权</h3><ol><li>概念：人对其智力劳动成果依法享有的占有、处分和收益的权利。</li><li>专利权概念：申请人就一项发明创造，即发明、实用新型或外观设计向国务院专利行政部门<strong>提出专利申请</strong>，经<strong>依法审查合格</strong>后，向专利申请人授予的<strong>在规定的时间内</strong>对该项发明创造享有的专有权利。</li></ol><h3 id="知识产权类型"><a href="#知识产权类型" class="headerlink" title="知识产权类型"></a>知识产权类型</h3><table><thead><tr><th>知识产权类型</th><th>保护期限</th></tr></thead><tbody><tr><td>专利权-发明</td><td>从申请日开始，保护期限20年</td></tr><tr><td>专利权-新型实用&#x2F;外观设计</td><td>从申请日开始，保护期限10年</td></tr><tr><td>著作权(版权)</td><td>自然人软件著作权保护器为自然人终生及其死后50年，法人或组织软件著作权保护期为首次发表后的50年</td></tr><tr><td>商标权</td><td>每次10年，可以不断续注</td></tr><tr><td>商业秘密</td><td>永久</td></tr></tbody></table><h3 id="专利权特点"><a href="#专利权特点" class="headerlink" title="专利权特点"></a>专利权特点</h3><table><thead><tr><th>特点</th><th>解释</th></tr></thead><tbody><tr><td>时间性</td><td>专利保护有时间限制，例如发明的保护期限是20年</td></tr><tr><td>地域性</td><td>专利只在申请的国家或地区有效，可以借助PCT专利合作公约申请多国保护</td></tr><tr><td>排他性</td><td>赋予专利人一定时间内的垄断权利</td></tr></tbody></table><h3 id="授予专利的条件-《专利法》第二十二条"><a href="#授予专利的条件-《专利法》第二十二条" class="headerlink" title="授予专利的条件(《专利法》第二十二条)"></a>授予专利的条件(《专利法》第二十二条)</h3><table><thead><tr><th>条件</th><th>解释</th></tr></thead><tbody><tr><td>新颖性</td><td>是指该发明或者实用新型不属于现有技术；也没有任何单位或者个人就<strong>同样的</strong>发明或者实用新型在申请日以前向国务院专利行政部门提出过申请，并记载在申请日以后公布的专利申请文件或者公告的专利文件中</td></tr><tr><td>实用性</td><td>可以<strong>复现</strong>，是指该发明或者实用新型能够制造或者使用，并且能够产生<strong>积极</strong>效果</td></tr><tr><td>创造性</td><td>是指与现有技术相比，该发明具有<strong>突出的实质性特点和显著</strong>的进步，该实用新型具有实质性特点和进步</td></tr></tbody></table><h3 id="好想法提炼条件"><a href="#好想法提炼条件" class="headerlink" title="好想法提炼条件"></a>好想法提炼条件</h3><p>掌握现有技术，发现现有技术缺陷并能转换为能解决的技术问题，找到解决问题的技术方案。</p><h3 id="挖掘专利技巧"><a href="#挖掘专利技巧" class="headerlink" title="挖掘专利技巧"></a>挖掘专利技巧</h3><ol><li>切记完美主义，申请专利的技术方案不需要现在就实现，或者已经实现产品落地成形，只需要能看到有后续的市场价值即可。要克服低估自身技术成果的错误思想。</li><li>针对现有技术，找到和现有技术特征的差别、改进现有技术方案的问题或者提升现有技术方案性能。</li><li>针对现实需求，找到解决问题全部的解决方案，并以方案的每个步骤的具体可操作性为方向。<br>对特定场景进行挖掘，从中换位思考。</li><li>看技术方案：可以是实现新功能的架构和方法、解决技术问题的新方法或者改造已有方法中的某个步骤。</li></ol><h3 id="专利价值判断依据"><a href="#专利价值判断依据" class="headerlink" title="专利价值判断依据"></a>专利价值判断依据</h3><ol><li>创造性：看专利是否是原创性、重大改进还现有技术的极小改进。</li><li>市场应用前景：看专利对应的市场空间是否明确或规模比较大。</li><li>侵权证据可获得性：看专利是否<strong>很容易</strong>判断对方是否侵权，侵权证据获取成本是否比较高。</li><li>专利可规避程度：看专利是否可以采用其他技术方案达成相同的目的，或者付出较小的代码或性能也能实现相同的目标。</li><li>标准相关性：看专利是否已纳入正式国家、国际、行业等技术标准，纳入技术标准的专利一般是基本专利，否则为商用特性专利。基本专利一般受标准组织政策约束。</li></ol><h3 id="侵权判断标准-等同原则四要素："><a href="#侵权判断标准-等同原则四要素：" class="headerlink" title="侵权判断标准-等同原则四要素："></a>侵权判断标准-等同原则四要素：</h3><p>以基本相同的手段，实现基本相同的功能，产生基本相同的效果，而且本领域普通技术人员在被诉侵权行为发生时无需经过创造性劳动就能够联想到的特征。<br>其中：<br>     - 基本相同的手段，是指被诉侵权技术方案中的技术特征与权利要求对应技术特征在技术内容上并无实质性差异。<br>     - 基本相同的功能，是指被诉侵权技术方案中的技术特征与权利要求对应技术特征在各自技术方案中所起的作用基本相同。被诉侵权技术方案中的技术特征与权利要求对应技术特征相比还有其他作用的，不予考虑。<br>     - 基本相同的效果，是指被诉侵权技术方案中的技术特征与权利要求对应技术特征在各自技术方案中所达到的技术效果基本相当。被诉侵权技术方案中的技术特征与权利要求对应技术特征相比还有其他技术效果的，不予考虑。<br>     - 无需经过创造性劳动就能够想到，是指对于本领域普通技术人员而言，被诉侵权技术方案中的技术特征与权利要求对应技术特征相互替换是容易想到的。<br>等同原则构建的初衷是为了避免被诉侵权人通过改变一些细微的、非实质性的技术特征来逃避专利侵权的法律责任，从而给予专利权人以有效的救济。等同原则的实质是扩展权利要求文字表达的保护范围，使专利的保护范围不仅涵盖与专利相同的东西，也涵盖与专利相似的东西。    </p><h3 id="编写专利经验"><a href="#编写专利经验" class="headerlink" title="编写专利经验"></a>编写专利经验</h3><ol><li>纯算法类不适合写专利，但是算法类和工具能够外化呈现的可以写专利。能解决需求、提升性能、方案创新和缺陷改进的适合写专利。</li><li>评审创造性时，技术问题、解决方案和实施效果是作为评价创造性的整理来考虑的。而新方案和新问题是保证创造性的关键。</li></ol><h3 id="常用专利检索网站："><a href="#常用专利检索网站：" class="headerlink" title="常用专利检索网站："></a>常用专利检索网站：</h3><table><thead><tr><th>专利网站</th><th>是否付费</th></tr></thead><tbody><tr><td><a href="http://www.innojoy.com/search/index.shtml">innojoy系统</a></td><td>付费</td></tr><tr><td><a href="https://www.baiten.cn/">佰腾网</a></td><td>付费</td></tr><tr><td><a href="http://www.uspto.gov/patft/index.html">美国专利商标局</a></td><td>免费</td></tr><tr><td><a href="https://www.cnipa.gov.cn/">中国国家知识产权局</a></td><td>免费</td></tr><tr><td><a href="https://www.drugfuture.com/cnpat/cn_patent.asp">中国专利全文打包下载</a></td><td>免费</td></tr><tr><td><a href="https://www.drugfuture.com/uspat/us_patent.asp">美国专利全文打包下载</a></td><td>免费</td></tr><tr><td><a href="https://www.patent9.com/">专利在线Patent.com</a></td><td>免费</td></tr><tr><td><a href="https://ipc.incopat.com/index">IPC分类查询</a></td><td>免费</td></tr><tr><td><a href="https://patents.google.com/">谷歌Patents</a></td><td>免费</td></tr><tr><td><a href="https://www.incopat.com/">incoPat</a></td><td>免费</td></tr></tbody></table><p>专利检索有技巧，需要单独学习并掌握。</p><h3 id="参考文献："><a href="#参考文献：" class="headerlink" title="参考文献："></a>参考文献：</h3><p>[1]  <a href="https://book.douban.com/subject/36746756/">专利审查指南</a><br>[2]  <a href="https://book.douban.com/subject/6082864/">中国专利法详解</a><br>[3]  <a href="https://book.douban.com/subject/26796545/">专利分析</a><br>[4]  <a href="https://www.cnipa.gov.cn/art/2020/11/23/art_97_155167.html">专利法</a><br>[5]  <a href="https://book.douban.com/subject/30682427/">以案说法——专利复审、无效典型案例汇编</a><br>[6]  <a href="https://ggfw-dlsks.cnipa.gov.cn/">专利代理师考试</a><br>[7]  <a href="https://blog.csdn.net/aiyanzielf/article/details/124519315?ops_request_misc=%257B%2522request%255Fid%2522%253A%25222f5aea95e35e65d31d08a1a7411972af%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=2f5aea95e35e65d31d08a1a7411972af&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-124519315-null-null.142%5Ev102%5Epc_search_result_base8&utm_term=%E4%B8%93%E5%88%A9&spm=1018.2226.3001.4187">从零开始写专利</a><br>[8]  <a href="https://www.bjcourt.gov.cn/article/newsDetail.htm?NId=150002896&channel=100014003">专利侵权判定指南(2017)</a>,北京市高级人民法院著<br>[9]<a href="https://www.cnipa.gov.cn/art/2024/12/31/art_66_196988.html">《人工智能相关发明专利申请指引（试行）》</a></p>]]></content>
      
      
      <categories>
          
          <category> 专利 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 基础 </tag>
            
            <tag> 知识产权 </tag>
            
            <tag> 专利 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>项目管理十大要点</title>
      <link href="/2025/09/11/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86%E5%8D%81%E5%A4%A7%E8%A6%81%E7%82%B9/"/>
      <url>/2025/09/11/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86%E5%8D%81%E5%A4%A7%E8%A6%81%E7%82%B9/</url>
      
        <content type="html"><![CDATA[<p>懂得项目管理，你其实当“军长”都够用了，华为的项目经理就是CEO,我就是华为最大的项目经理。–老板</p><p>项目管理的要点有哪些？即：一个使命、两个概念、三重约束、四类方法、五个过程组、六字方针、七计成败、八个绩效域、九方关系、十大知识领域。</p><h3 id="一个使命"><a href="#一个使命" class="headerlink" title="一个使命"></a>一个使命</h3><ul><li>一个使命：Make Ideas A Reality</li><li>“Make Ideas A Reality”指出了项目最本质的使命就是：把一个个想法最终变为现实。项目管理首先把产生的idea变成可行的方案，然后再把可行的方案变成现实。这不仅明确了项目的核心使命，更是对项目管理过程中每一个环节的精准概括。</li></ul><h3 id="二个概念"><a href="#二个概念" class="headerlink" title="二个概念"></a>二个概念</h3><ul><li>二个概念：项目和项目管理</li><li>项目管理中涉及到两个核心概念：项目和项目管理。<ul><li>项目：为创造独特的产品、服务或成果而进行的临时性的工作。项目的成功往往取决于有效的规划、执行和监控。</li><li>项目管理：将知识、技能、工具与技术应用于项目活动，以满足项目的要求。项目管理不仅要求项目经理具备扎实的管理知识和技能，还需要他们拥有出色的领导能力、沟通能力和团队合作精神。通过项目管理，可以确保项目的资源得到合理分配，风险得到有效控制，团队的工作高效协同，最终实现项目的预期目标。</li></ul></li></ul><h3 id="三重约束"><a href="#三重约束" class="headerlink" title="三重约束"></a>三重约束</h3><ul><li>三重约束：范围、时间、成本<ul><li>任何一个项目都会受到范围、时间、成本的约束，项目管理就是要做好这三者之间的平衡。</li><li>项目的三重约束可以互动，如范围不变的前提下，要缩短时间，就需要增加成本，或者范围不变，要减少成本，就需要延长时间。在平衡范围、时间和成本的过程中，项目经理需要考虑如何在有限的时间内，有限的预算下，完成规定范围内的任务并达成项目目标，还要使得客户满意。</li></ul></li></ul><h3 id="四类方法"><a href="#四类方法" class="headerlink" title="四类方法"></a>四类方法</h3><ul><li>四类方法：预测型方法、迭代型方法、增量型方法、敏捷型方法</li><li>如果把项目的技术和需求两个要素分别作为横轴和纵轴，就会构成四个象限，从左到右代表技术从确定到不确定，从下往上代表需求的确定到不确定，则每个象限代表了不同类别的项目，对于不同类别的项目需要使用对应的项目管理方法，方能正确解锁项目。<ul><li>第一象限项目：技术确定，需求不确定。这类项目往往已经具备成熟的技术基础，但面临的需求却较为模糊或易变。在这种情况下，使用增量型方法是非常合适的。通过分阶段交付成果，逐步明确和细化需求，同时保持技术的稳定性，可以确保项目在满足不断变化的需求的同时，也能保证技术的有效实施。</li><li>第二象限项目：技术不太确定，需求也不太确定。这是极具挑战性的项目类型，既需要面对技术上的未知，又要应对需求的不确定性。对于这类项目，我们需要采用敏捷的项目管理方法，强调快速迭代和适应性调整。通过频繁的反馈和协作，及时发现问题并进行调整，确保项目能够在不断变化的环境中保持灵活性和创新性。</li><li>第三象限项目：技术确定，需求确定。这类项目通常处于较为稳定的状态，技术基础坚实，需求也清晰明确。针对这样的项目，使用预测型方法将是一个高效的选择。预测型方法强调计划的重要性和执行的精确性，通过详细的项目计划和时间线，确保项目按照既定的路径稳步推进。</li><li>第四象限项目：技术不确定，但需求确定。面对这种情况，使用迭代型方法是非常明智的选择。迭代型方法强调在不确定的技术环境下，通过多次的迭代和反馈，逐步逼近和满足确定的需求。</li></ul></li></ul><h3 id="五个过程组"><a href="#五个过程组" class="headerlink" title="五个过程组"></a>五个过程组</h3><ul><li>五个过程组：启动、规划、执行、监控、收尾</li><li>项目管理一共五个过程组：启动、规划、执行、监控、收尾。这五个过程组相互衔接，共同确保项目的顺利进行和成功完成。<ul><li>启动：定义一个新项目或现有项目的一个新阶段，授权开始该项目或阶段的一组过程。</li><li>规划：明确项目范围，完善目标，为实现目标制定行动方案的一组过程。</li><li>执行：完成项目管理计划中确定的工作，以满足项目需求的一组过程。</li><li>监控：跟踪、审查和调整项目进展与绩效的一组过程，该过程识别任何计划需要变更的领域，并启动相应变更。</li><li>收尾：正式完成或结束项目、阶段或合同时所执行的过程。</li></ul></li></ul><h3 id="六字方针"><a href="#六字方针" class="headerlink" title="六字方针"></a>六字方针</h3><ul><li>六字方针：目标、价值、客户、团队、计划、控制</li><li>项目管理一共有6字方针，分别是：以目标为导向、以价值为牵引、以客户为中心、以团队为核心、以计划为龙头、以控制为手段。<ul><li>以目标为导向：任何项目都需要设定清晰的目标，目标应该是Smart化的产品、服务或成果；</li><li>以价值为牵引：项目还需要实现项目目标所对应的价值；</li><li>以客户为中心：要时刻关注客户以及客户的关注点；</li><li>以团队为核心：要紧密团结项目团队成员（包括客户以及伙伴等），形成一个核心团队；</li><li>以计划为龙头：没有计划就是打乱仗，有了计划才能协调好人财物，高效搞定项目；</li><li>以控制为手段：监控项目目标、价值、计划执行等，一旦有所偏离，要及时采取纠偏措施。</li></ul></li></ul><h3 id="七计成败"><a href="#七计成败" class="headerlink" title="七计成败"></a>七计成败</h3><ul><li>七计成败：主、将、天地、法令、兵众、士卒、赏罚</li><li>交付项目的过程中，要以孙子兵法的“七计”来判断项目成败的概率，一旦发现薄弱环节需要及时补救。<ul><li>主孰有道：指项目的价值&#x2F;目标是否科学合理，是否获得各方干系人的认同？</li><li>将孰有能：指项目经理是否具备足够的资质和能力，能否胜任？</li><li>天地孰得：指是否主孰有道能获得项目关键干系人如投资方和使用方的支持？</li><li>法令孰行：指项目组是否有清晰的项目管理规章制度？</li><li>兵众孰强：指项目团队成员是否胜任，尤其各核心团队成员？</li><li>士卒孰练：指项目团队成员是否经过同类场景的训战，是否具备足够的经验？</li><li>赏罚孰明：指项目组是否有清晰的奖惩制度，能否严格按制度执行？</li></ul></li></ul><h3 id="八个绩效域"><a href="#八个绩效域" class="headerlink" title="八个绩效域"></a>八个绩效域</h3><ul><li>项目管理一共有八个绩效域：干系人、团队、开发方法与生命周期、规划、项目工作、交付、测量、不确定性。<ul><li>干系人：与干系人相关的活动和功能；</li><li>团队：与负责生成项目可交付物以实现商业成果相关的人员活动和功能；</li><li>开发方法与生命周期: 与项目的开发方法、节奏和生命周期阶段相关的活动和功能；</li><li>规划：为交付项目可交付物和项目成果所需的与初始、持续进行和演变的组织和协调相关的活动和功能；</li><li>项目工作：与建立项目过程、管理实物资源和营造学习环境相关的活动和功能；</li><li>交付：与交付项目要实现的范围和质量相关的活动和功能；</li><li>测量：与评估项目绩效和采取适当行动维持可接受绩效相关的活动和功能；</li><li>不确定性: 与风险和不确定性相关的活动和功能。</li></ul></li></ul><h3 id="九方关系"><a href="#九方关系" class="headerlink" title="九方关系"></a>九方关系</h3><ul><li>九方关系：三方 X 三层 &#x3D; 九方关系<ul><li>三方：投资方、建设方和使用方</li><li>三层：指导层、管理层和执行层</li></ul></li><li>项目管理的九方关系是一个复杂而精细的体系，它涵盖了项目涉及的各个层面和利益相关方。这一体系的核心在于三方与三层的交织，共同构建了一个包含九个关键干系方的九宫格。当三方与三层相结合时，就构成了九方关系。这九个关键干系方在项目中各自扮演着重要的角色，他们的支持和配合对于项目的成功至关重要。因此，作为项目经理，需要对这九个关键干系方进行有效管理。这包括了解他们的需求和期望，建立有效的沟通机制，协调他们之间的利益关系，以获得他们的支持和合作。</li></ul><h3 id="十大知识领域"><a href="#十大知识领域" class="headerlink" title="十大知识领域"></a>十大知识领域</h3><ul><li>项目管理涉及十大知识领域：干系人管理、范围管理、进度管理、成本管理、质量管理、资源管理、采购管理、沟通管理、风险管理、整合管理，这也对应了“项目管理十问”。<ul><li>第一问是：为谁做？也就是“干系人管理”，因而要知晓为什么要做此项目；</li><li>第二问是：做什么？也就是“范围管理”，是基于干系人的需求，以决定要做什么；</li><li>第三问是：花多长时间做？也就是“进度管理”，要考虑计划花多长时间，过程中是否有偏差，如何矫正偏差；</li><li>第四问是：用多少成本做？也就是“成本管理”，要考虑计划花多少钱，是否有超支等；</li><li>第五问是：按什么标准做？也就是“质量管理”，要考虑质量过程是否可控；</li><li>第六问是：内部谁来做？也就是“人力资源管理”，要考虑如何构建团队、角色和职责的分配，以及激励和领导团队等内容；</li><li>第七问是：外部谁来做？也就是“采购管理”，包括采购规划、供应商选择、合同管理以及采购收尾；</li><li>第八问是：人员如何协作？也就是“沟通管理”，为了确保及时、准确和适当地收集、整理、发布、存储和使用项目信息；</li><li>第九问是：有哪些风险导致无法达成目标？也就是“风险管理”，包括风险识别、风险评估、风险应对计划和风险监控；</li><li>第十问是：如何实现整体最优？也就是“整合管理”，包括项目起始、项目计划制定、项目执行以及项目结束。</li></ul></li></ul><h3 id="参考文献："><a href="#参考文献：" class="headerlink" title="参考文献："></a>参考文献：</h3><p>[1] <a href="https://news.qq.com/rain/a/20240904A06FMA00?uid%5B0%5D=&uid%5B1%5D=&suid=&media_id=">华为项目管理十大要点，太有用了，必须收藏！</a><br>[2] <a href="https://zhuanlan.zhihu.com/p/662502101">大厂项目管理’43210’法：人人都需要的项目管理课</a></p>]]></content>
      
      
      <categories>
          
          <category> 项目管理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 项目管理 </tag>
            
            <tag> 管理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch基础教程及注意事项-基础篇</title>
      <link href="/2025/08/25/PyTorch%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B%E5%8F%8A%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9-%E5%9F%BA%E7%A1%80%E7%AF%87/"/>
      <url>/2025/08/25/PyTorch%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B%E5%8F%8A%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9-%E5%9F%BA%E7%A1%80%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h2 id="PyTorch安装和环境准备"><a href="#PyTorch安装和环境准备" class="headerlink" title="PyTorch安装和环境准备"></a>PyTorch安装和环境准备</h2><p>PyTorch下载地址：<a href="https://pytorch.org/">https://PyTorch.org/</a></p><ol><li>PyTorch可以同时安装GPU（CUDA）和CPU版本，使用时需要注意，不同版本PyTorch不能安装在同一个Python解释器下，否则会报错。可以尝试一个安装在本地Python解释器下，另一个使用Anaconda Prompt安装在Anaconda环境中。</li><li>如果发现安装的cuda版本在PyTorch中不存在预构建二进制文件，可以手动先安装低版本，例如电脑安装的是12.2版本cuda，PyTorch可以先安装11.8版本，最后使用PyTorch命令检查按照按照的PyTorch版本和cuda版本。</li></ol><h2 id="Anaconda专门环境配置"><a href="#Anaconda专门环境配置" class="headerlink" title="Anaconda专门环境配置"></a><a href="https://anaconda.org.cn/">Anaconda</a>专门环境配置</h2><p>介绍：数据科学和机器学习软件套装<br>环境管理功能：使用cnnda包管理器，在管理软件包的同时，可以创建管理不同的Python环境<br><a href="https://docs.conda.org.cn/projects/conda/en/stable/user-guide/tasks/manage-environments.html">常用环境管理命令</a>：</p><table><thead><tr><th>方法</th><th>conda命令</th></tr></thead><tbody><tr><td>创建新环境</td><td>conda create –name 环境名称</td></tr><tr><td>创建指定版本环境</td><td>conda create –name 环境名称 Python &#x3D; 版本号</td></tr><tr><td>激活环境</td><td>conda activate 环境名称</td></tr><tr><td>退出当前环境</td><td>deactivate</td></tr><tr><td>查看所有已创建环境</td><td>conda env list</td></tr><tr><td>复制环境</td><td>conda create –name 复制后环境新名 –clone 环境名</td></tr><tr><td>删除环境</td><td>conda env remove –name 环境名</td></tr><tr><td>查看帮助</td><td>conda –help</td></tr></tbody></table><p><a href="https://docs.conda.org.cn/projects/conda/en/stable/user-guide/tasks/manage-pkgs.html">常用包管理命令</a>：</p><table><thead><tr><th>方法</th><th>conda命令</th></tr></thead><tbody><tr><td>安装包</td><td>conda install 包名</td></tr><tr><td>安装指定版本包</td><td>conda install 包名 &#x3D; 版本号</td></tr><tr><td>更新包</td><td>conda update 包名</td></tr><tr><td>卸载包</td><td>conda remove 包名</td></tr><tr><td>查看已安装包</td><td>conda list</td></tr><tr><td>搜索包</td><td>conda search 包名</td></tr><tr><td>清理conda缓存，删除不需要使用的包</td><td>conda clean –all</td></tr><tr><td>查看conda版本</td><td>conda –version</td></tr></tbody></table><h2 id="PyTorch"><a href="#PyTorch" class="headerlink" title="PyTorch"></a>PyTorch</h2><p>介绍：动态计算图、可自动微分、张量计算、多语言多设备支持的开源Python深度学习框架，基于torch库，底层由C++实现<br>结构（从上到下）：  </p><ol><li>PyTorch生态系统（专业库）：  <ul><li><a href="https://docs.pytorch.org/vision/stable/index.html">torchvision</a>:用于计算机视觉的数据集和模型  </li><li><a href="https://docs.pytorch.org/text/stable/index.html">torchtext</a>:用于自然语言处理的数据集和模型  </li><li><a href="https://docs.pytorch.org/audio/stable/index.html">torchaudio</a>:用于音频处理的数据集和模型</li></ul></li><li>PyTorch核心：  <ul><li><a href="https://docs.pytorch.org/docs/stable/PyTorch-api.html">PyTorch API</a>(顶层):开发者直接调用接口<br> torch: 张量核心计算<br> torch.nn:构建神经网络<br> torch.autograd: 自动微分，反向传播  </li><li><a href="https://docs.pytorch.org/cppdocs/">C++核心</a>（中层）：高性能计算，沟通Python代码和底层硬件<br> ATen: 基础张量和数学运算核心库<br> JIT: 即时编译优化模型，编译器和解释器的接口<br> Autograd引擎：自动微分计算引擎，增强ATen库  </li><li>基础层（底层）：直接操作硬件，实现高速优化<br> TN&#x2F;THNN: C&#x2F;C++实现的基础张量和神经网络操作库，非常底层<br> THC&#x2F;THCUNN: 对应模块的CUDA实现</li></ul></li><li>PyTorch运算流程：python代码-&gt;Python API接口-&gt;C++核心计算-&gt;底层CUDA&#x2F;C库加速计算-&gt;返回结果。</li></ol><h2 id="张量（Tensor）"><a href="#张量（Tensor）" class="headerlink" title="张量（Tensor）"></a>张量（Tensor）</h2><p>定义：数据核心表示形式，类似于NumPy多维数组，数据可存储在CPU&#x2F;GPU等计算设备<br>组成：  </p><ul><li>维度(Dimensionality)：定义张量的多维数组结构  </li><li>形状(shape)：定义张量每个维度的大小  </li><li>数据类型(Dtype)：定义张量上每个元素存储所需的内存大小和解释方式，包括整理、浮点型和布尔型。<br>张量属性&#x2F;方法工具如下：</li></ul><table><thead><tr><th>方法属性</th><th>说明</th></tr></thead><tbody><tr><td>.shape&#x2F;.size()</td><td>获取张量形状</td></tr><tr><td>.dtype</td><td>获取张量数据类型</td></tr><tr><td>.device</td><td>查看张量所在计算设备(CPU&#x2F;GPU)</td></tr><tr><td>.dim()</td><td>获取张量的维度数</td></tr><tr><td>.requires_grad</td><td>判断张量是否使用梯度计算</td></tr><tr><td>.numel()</td><td>获取张量元素总数</td></tr><tr><td>.is_cuda</td><td>判断张量是否在GPU上</td></tr><tr><td>.T</td><td>获取张量转置（二维及以下张量）</td></tr><tr><td>.item()</td><td>获取单元素张量值</td></tr><tr><td>.is_contiguous()</td><td>检查张量是否连续存储</td></tr><tr><td>.view(shape)&#x2F;.reshape(shape)</td><td>在不改变数据的情况下，改变张量形状</td></tr><tr><td>.unsqueeze(dim)</td><td>在指定维度添加一个维度</td></tr><tr><td>.squeeze(dim)</td><td>去掉指定维度为1的维度</td></tr><tr><td>.numpy()</td><td>将张量转换为Numpy数组,仅限CPU张量， 数组和张量共享内存，修改数据互相影响</td></tr><tr><td>.clone()</td><td>数据克隆，深复制，可存放在新内存地址中</td></tr><tr><td>.flatten()</td><td>张量展平，构成一维向量</td></tr><tr><td>Tensor创建</td><td></td></tr><tr><td>torch.tensor(data)</td><td>从Python列表和numpy数组中创建张量</td></tr><tr><td>torch.as_tensor(data)</td><td>数据转换为张量（共享内存）</td></tr><tr><td>torch.zeros(size)</td><td>创建全为零的张量</td></tr><tr><td>torch.ones(size)</td><td>创建全为1的张量</td></tr><tr><td>torch.empty(size)</td><td>创建未初始化的张量</td></tr><tr><td>torch.eye(size)</td><td>创建单位矩阵</td></tr><tr><td>torch.full(size, fill_value)</td><td>创建填充指定值的张量</td></tr><tr><td>torch.manual_seed(seed)</td><td>设置随机数种子</td></tr><tr><td>torch.initial_seed()</td><td>返回当前随机种子</td></tr><tr><td>torch.rand(size)</td><td>创建服从均匀分布的随机张量，值为[0, 1]</td></tr><tr><td>torch.randn(size)</td><td>创建服从标准正态分布的随机张量</td></tr><tr><td>torch.randint(low, high, size)</td><td>创建整数随机张量</td></tr><tr><td>torch.randperm(n)</td><td>创建0到n-1的随机配列</td></tr><tr><td>torch.arange(start, end, step)</td><td>创建一维序列张量，类似Python的range函数</td></tr><tr><td>torch.linspace(start, end, steps)</td><td>创建指定范围内等间隔序列张量</td></tr><tr><td>torch.logspace(start, end, steps)</td><td>创建对数间隔序列张量</td></tr><tr><td>torch.form_numpy(ndarray)</td><td>Numpy数组转换为张量，数组和张量共享内存，修改数据互相影响</td></tr><tr><td>Tensor操作</td><td></td></tr><tr><td>torch.stack()</td><td>沿着新维度堆叠张量</td></tr><tr><td>torch.cat((x, y), dim)</td><td>指定维度连接多个张量</td></tr><tr><td>torch.matmul(x, y)&#x2F;torch.mm(input, mat2)</td><td>矩阵乘法</td></tr><tr><td>torch.bmm(input, mat2)</td><td>批量矩阵乘法</td></tr><tr><td>torch.eig(input)</td><td>计算矩阵特征值和特征向量</td></tr><tr><td>torch.svd(input)</td><td>计算矩阵的奇异值分解</td></tr><tr><td>torch.inverse(input)</td><td>计算矩阵的逆</td></tr><tr><td>torch.det(input)</td><td>计算矩阵的行列式</td></tr><tr><td>torch.trance(input)</td><td>计算矩阵的迹</td></tr><tr><td>torch.dot(x, y)</td><td>向量点积（仅适用于一维张量）</td></tr><tr><td>torch.abs(x)</td><td>求绝对值</td></tr><tr><td>torch.sqrt(x)</td><td>求平方根</td></tr><tr><td>torch.pow(x)</td><td>求幂运算</td></tr><tr><td>torch.exp(x)</td><td>求指数函数</td></tr><tr><td>torch.log(x)</td><td>求自然对数</td></tr><tr><td>torch.sum(x)</td><td>求和</td></tr><tr><td>torch.mean(x)</td><td>求均值</td></tr><tr><td>torch.max(x)</td><td>求最大值</td></tr><tr><td>torch.min(x)</td><td>求最小值</td></tr><tr><td>torch.clamp(input, min, max)</td><td>张量限制在指定范围内</td></tr><tr><td>torch.round(input)</td><td>近似，四舍五入</td></tr><tr><td>torch.floor(input)</td><td>向下取整</td></tr><tr><td>torch.ceil(input)</td><td>向上取整</td></tr><tr><td>torch.argmax(x, dim)</td><td>返回指定维度下的最大值对应索引</td></tr><tr><td>torch.softmax(x, dim)</td><td>计算指定维度下的softmax</td></tr><tr><td>torch.meshgrid()</td><td>生成网络，可用于生成坐标</td></tr></tbody></table><p>注意：  </p><ol><li>共享内存的使用，典型如张量和Numpy数组之间的转化存在共享内存，张量形状的改变也存在共享内存，共享内存存在数据互相影响。  </li><li>张量和Numpy有转换，是因为二者有相似的内存结构，所以有内置方法直接转换。如果PyTorch中Tensor和pandas里的DataFrame进行转换，需要通过Numpy作为中间步骤实现。</li></ol><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] <a href="https://study.163.com/course/introduction/1208894818.htm">深度学习与PyTorch入门实战</a><br>[2] Zhang, A., Lipton, Z. C., Li, M., &amp; Smola, A. J. (2023). <a href="https://zh.d2l.ai/index.html">Dive into Deep Learning</a>. Cambridge University Press. URL: <a href="https://d2l.ai/">https://D2L.ai</a><br>[3] <a href="https://github.com/dragen1860/Deep-Learning-with-PyTorch-book">PyTorch深度学习</a><br>[4] <a href="https://www.runoob.com/PyTorch/PyTorch-tutorial.html">菜鸟教程</a><br>[5] <a href="https://datawhalechina.github.io/thorough-PyTorch/index.html#">深入浅出PyTorch</a></p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
            <tag> 算法 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> PyTorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch基础教程及注意事项-数据和模型篇</title>
      <link href="/2025/08/25/PyTorch%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B%E5%8F%8A%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9-%E6%95%B0%E6%8D%AE%E5%92%8C%E6%A8%A1%E5%9E%8B%E7%AF%87/"/>
      <url>/2025/08/25/PyTorch%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B%E5%8F%8A%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9-%E6%95%B0%E6%8D%AE%E5%92%8C%E6%A8%A1%E5%9E%8B%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h2 id="数据ETL-提取、转换、加载"><a href="#数据ETL-提取、转换、加载" class="headerlink" title="数据ETL(提取、转换、加载)"></a>数据ETL(提取、转换、加载)</h2><p>数据集：  </p><ol><li>内置数据集：<br>  定义： PyTorch生态系统（专业库）中内置的常用数据集<br><a href="https://docs.pytorch.org/vision/stable/index.html">torchvision</a>:用于计算机视觉常用数据集<br>torchvision.datasets 内部常用数据集  </li><li>典型数据集：</li></ol><table><thead><tr><th>数据集</th><th>名称</th><th>下载加载命令</th><th>数据量</th><th>用途</th></tr></thead><tbody><tr><td>MNIST</td><td>手写数字图像数据集</td><td>torchvision.datasets.MNIST(root, train, download, transform)</td><td>7万张手写数字0-9的灰度图像，其中，6万张用于训练，1万张用于测试。每张图像的大小为28×28像素</td><td>图像分类</td></tr><tr><td>CIFAR-10</td><td>彩色图像数据集</td><td>torchvision.datasets.CIFAR10(root, train, download, transform)</td><td>10个类别、每个类别有6000张图像，总共有5万张训练图像和1万张测试图像，6万张32×32像素彩色图像</td><td>图像分类</td></tr><tr><td>COCO</td><td>通用物体检测、分割、关键点检测数据集</td><td>torchvision.datasets.CocoCaptions(root, anaFile, transform)</td><td>33 万张图像、150 万目标实例、80 个目标类、91 个物品类以及 25 万关键点人物</td><td>图像分割</td></tr><tr><td>ImageNet</td><td>经典图像数据集</td><td>torchvision.datasets.ImageNet(root, split, transform, loader)</td><td>120万张训练图像，5万张验证图像和10万张测试图像</td><td>图像分类和物体检测</td></tr><tr><td>STL-10</td><td>彩色图像数据集</td><td>torchvision.datasets.STL10(root, split, download, transform)</td><td>10个类组成，总共约6000+张96*96像素图像</td><td>图像识别</td></tr><tr><td>Cityscapes</td><td>城市街道场景图像</td><td>torchvision.datasets.Cityscapes(root, split, mode, transform)</td><td>50 个不同城市街景中记录的视频序列，其包含 20000 个弱注释帧和 5000 帧的高质量像素级</td><td>城市街景语义理解</td></tr></tbody></table><ol start="3"><li>自定义数据集：</li></ol><p>工具：torch.utils.data.Dataset抽象类,从自己的数据源创建自定义数据集。<br>  用法：需要继承该抽象类，并实现如下方法：<strong>len</strong>(self)(返回数据集中的样本数量)，<strong>getitem</strong>(self, idx)(通过索引返回样本)。</p><ol start="4"><li>外置数据集：一般是外部数据库，可以使用<a href="https://www.psycopg.org/docs/">psycopg2</a>等模块。</li></ol><table><thead><tr><th>数据库</th><th>模块工具</th><th>数据库驱动包</th></tr></thead><tbody><tr><td>GaussDB(DWS)</td><td><a href="https://support.huaweicloud.com/mgtg-dws/dws_01_0120.html">psycopg2</a>&#x2F;<a href="https://support.huaweicloud.com/mgtg-dws/dws_01_0171.html">PyGreSQL</a>&#x2F;psycopg2-binary</td><td>可使用开源驱动JDBC&#x2F;ODBC</td></tr><tr><td>GaussDB</td><td>psycopg2</td><td><a href="https://support.huaweicloud.com/distributed-devg-v8-gaussdb/gaussdb-12-0155.html">Psycopg</a></td></tr><tr><td>Opengauss</td><td>psycopg2</td><td><a href="https://docs.opengauss.org/zh/docs/6.0.0/docs/DeveloperGuide/%E5%8A%A0%E8%BD%BD%E9%A9%B1%E5%8A%A8_Psycopg.html">Psycopg</a></td></tr><tr><td>PostgreSQL</td><td>psycopg2&#x2F;psycopg3</td><td>Python模块工具安装后，不需要再单独在pg库侧安装驱动包</td></tr></tbody></table><ol start="5"><li>使用注意：</li></ol><ul><li>规格不同按需适配|使用时需要注意模块、模块接口、Python和数据库版本限制。  </li><li>使用时需要注意防火墙等限制，防火墙即可以阻止下载，也可以阻止连接，一般会导致连接超时报错。  </li><li>账号密码和数据库配置等信息一般是单独放置，加密保存，可以使用Python模块Crypto、cryptodome或者<a href="https://www.cnblogs.com/zzkkk1h/p/18117606">pycryptodomex</a>库中的加解密方法，获取使用信息。  </li><li>除了使用连接模块psycopg2的psycopg2.connect()连接数据库外，还可以使用<a href="https://blog.csdn.net/xc_zhou/article/details/80893289">DBUtils.PooledDB</a>包（管理数据库连接池）连接数据库，DBUtils.PooledDB包使用的时候，需要和psycopg2或者importlib一起配合使用。</li></ul><h2 id="数据加载、处理和转换"><a href="#数据加载、处理和转换" class="headerlink" title="数据加载、处理和转换"></a>数据加载、处理和转换</h2><ol><li>数据加载器：<a href="https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader">torch.utils.data.DataLoader</a>，从数据集中按批次加载数据，支持多线程加载加速和数据打乱。</li></ol><ul><li>关键参数：torch.utils.data.DataLoader(dataset，batch_size, shuffle，num_workers，drop_last) <ul><li>batch_size: int类型的数据， 每次加载样本的数量，如默认设置为1，那就是一行一行的喂数据给模型，效率比较慢。  </li><li>shuffle: bool数据类型， 是否需要对数据进行洗牌（通常用于训练时将数据打乱使用），如果数据有规律特征（顺序或倒序），则不应该设置为True。  </li><li>num_workers：int类型的数据，默认为0，表示使用主进程导入数据，非负数表示使用多少子进程导入数据。  </li><li>drop_last:bool数据类型, 默认为False，和batch_size配合使用，可用于数据集中不能被batch_size整除中，确认是否丢弃最后一批数据。</li></ul></li></ul><ol start="2"><li>多数据源加载：<a href="https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader">torch.utils.data.ConcatDataset</a>，自定义加载多数据源，可以将多数据源合并为一个数据集。</li></ol><h2 id="数据转换"><a href="#数据转换" class="headerlink" title="数据转换"></a>数据转换</h2><ol><li>目的：将原始数据转换为适合模型训练的数据格式  </li><li>组成：</li></ol><ul><li>数据预处理：数据归一化，调整数据格式、大小和数据范围，使其适合模型输入。  </li><li>数据增强：在训练时对数据做变换，例如随机剪裁&#x2F;翻转等，提高模型泛化能力，避免过拟合。</li></ul><ol start="3"><li>一般工具：torchvision视觉库，torchvision.transfroms工具，以下是典型数据转换和增强操作：<br> 基础数据变换操作</li></ol><table><thead><tr><th>函数</th><th>用途</th></tr></thead><tbody><tr><td>torchvision.transfroms.Compose()</td><td>多变换操作组合，按照顺序依次执行</td></tr><tr><td>torchvision.transfroms.Resize()</td><td>调整图像大小，保证输入到网络的图像大小一致</td></tr><tr><td>torchvision.transfroms.ToTensor()</td><td>图像转化为Tensor张量，像素数值归一化为[0,1]范围</td></tr><tr><td>torchvision.transfroms.Normalize()</td><td>图像数据标准化，使数据符合特定均值和标准差</td></tr><tr><td>torchvision.transfroms.CenterCrop()</td><td>从图像中心剪裁指定大小区域</td></tr></tbody></table><p>数据增强操作</p><table><thead><tr><th>函数</th><th>用途</th></tr></thead><tbody><tr><td>torchvision.transfroms.RandomHorizontalFlip()</td><td>随机水平翻转图像</td></tr><tr><td>torchvision.transfroms.RandomRotation()</td><td>随机旋转图像一定角度</td></tr><tr><td>torchvision.transfroms.ColorJitter()</td><td>调整图像亮度、对比度、饱和度和色调</td></tr><tr><td>torchvision.transfroms.RandomCrop()</td><td>随机裁剪指定大小的区域</td></tr><tr><td>torchvision.transfroms.RandomResizeCrop()</td><td>随机裁剪图像并调整到指定大小</td></tr></tbody></table><h2 id="模型保存，加载和部署"><a href="#模型保存，加载和部署" class="headerlink" title="模型保存，加载和部署"></a>模型保存，加载和部署</h2><ol><li>功能：训练中断时恢复训练；在不同的训练阶段比较模型性能；方便模型部署和成员之间的模型共享；可用于迁移学习。  </li><li>类型：</li></ol><ul><li>保存整个模型：torch.save(model, path), 保存模型架构和所有参数。<br>优势：完整保留模型结构。<br>劣势：文件体积大，对模型类的定义有依赖。  </li><li>保存模型参数：torch.save(model.dict(), path), 只保留模型的状态字典信息。<br>优势：文件小，可加载到不同模型架构中，兼容性好。<br>缺点：过程繁琐，使用前需要每次先创建相同架构的模型。</li></ul><ol start="3"><li>注意事项：</li></ol><ul><li>保存命名：模型和参数的保存要按照架构要求和命名规范进行，要有意义。  </li><li>设置定期保存命令，最好是每隔几轮训练迭代就保存一次检查点，防止训练出现问题。  </li><li>在保存完成后，需要测试加载能力，确保保存的模型能正常加载使用。  </li><li>训练和保存过程中，由于比较耗时，可以在此期间将模型架构、训练参数等信息做文档保存，以备查阅。  </li><li>模型文件等材料放入Git&#x2F;svn&#x2F;Dbox&#x2F;代码仓的版本管理系统中，做版本管理。  </li><li>如果无法加载保存旧的版本模型，可以查阅文档，确定当时使用的PyTorch版本后再加载，或者转换模型的格式。</li></ul><h3 id="模型加载"><a href="#模型加载" class="headerlink" title="模型加载"></a>模型加载</h3><p>跨设备模型加载：  </p><ol><li>CPU或GPU加载：先保存模型参数到指定路径中，设定模型使用设备（torch.device(“cpu)）,再将模型参数加载到模型中（torch.load(参数路径，map_location &#x3D; ‘cpu)）,最后将模型转移到指定的模型使用设备上。  </li><li>多GPU模型加载：</li></ol><ul><li>目的：调度GPU，实现计算加速， GPU 并行计算（DataParallel 或 torch.distributed）  </li><li>GPU方法和函数清单：</li></ul><table><thead><tr><th>方法函数</th><th>说明</th></tr></thead><tbody><tr><td>torch.cuda.is_available</td><td>判断GPU是否可用</td></tr><tr><td>torch.device()</td><td>创建设备对象，可用于张量设置在GPU上</td></tr><tr><td>torch.to(device)</td><td>将张量移动到指定设备上</td></tr><tr><td>torch.device(‘cuda’ if torch.cuda.is_available() else ‘cpu’)</td><td>优先使用GPU计算，没有GPU情况下用CPU计算</td></tr><tr><td>注意事项：</td><td></td></tr></tbody></table><ol><li>在设定本次计算使用的工具时，使用torch.device(‘cpu’),cpu需要小写。  </li><li>在使用GPU运算时，要求模型和数据都要在GPU或者CPU同一个设备上。  </li><li>模型和输入数据使用的设备确定时，可以在模型之前设定，也可以在使用过程中设置，张量和模型必须在同一个设备上可以使用torch.to（）工具来转移。  </li><li>GPU中专用GPU内存和共享GPU内存的差别:专用GPU内存我们通常称为“显存”，就是显卡上独立专门的物理内存。而共享GPU内存是指从系统内存中单独划出，供GPU使用的内存，是显存的补充。可以在CUDA中显式管理共享内存。</li></ol><h3 id="分布式模型训练框架和工具："><a href="#分布式模型训练框架和工具：" class="headerlink" title="分布式模型训练框架和工具："></a>分布式模型训练框架和工具：</h3><ol><li>PyTorch原生分布式能力：<a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html">torch.DistributedDataParallel</a>、<a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.DataParallel.html">torch.DataParallel</a>、<a href="https://docs.pytorch.ac.cn/docs/stable/distributed.html">torch.distributed</a>，torch.DistributedDataParallel效率更高。</li></ol><ul><li>数据并行模式支持：PyTorch支持数据并行模式，不支持流水线、Tensor、混合和自动并行模式，其1.9.0以上版本支持ZeRO，可以用于torch.DistributedDataParallel的communicationhook调用。torch.DistributedDataParallel支持通讯优化。  </li><li>计算加速支持：可以使用torch.amp实现低精度训练（1.6以上版本，FP16精度）  </li><li>内存优化支持：可以使用torch.utils.checkpointing进行重计算</li></ul><ol start="2"><li><a href="https://docs.nvidia.com/nemo-framework/user-guide/24.07/nemotoolkit/nlp/megatron.html">NeMo- Megatron</a>:英伟达开发的分布式训练模型开源框架，支持数据并行和模型并行。  </li><li><a href="https://blog.csdn.net/zwqjoy/article/details/130732601">DeepSpeed</a>:微软开发的分布式训练模型开源框架，可以和NeMo- Megatron兼容，是目前主要使用的分布式训练工具。<br> 大模型分布式训练并行技术：<a href="https://zhuanlan.zhihu.com/p/598714869">https://zhuanlan.zhihu.com/p/598714869</a></li></ol><h3 id="模型转换"><a href="#模型转换" class="headerlink" title="模型转换"></a>模型转换</h3><ol><li>版本兼容性：在<a href="https://docs.pytorch.org/docs/stable/generated/torch.save.html">torch.save()</a>中使用_use_new_zipfile_serialization来确保好的兼容性,可以确保仍然能使用1.6版本之前的旧格式。  </li><li>格式转换：可以使用torch.jit.scipt()将模型转换为TorchScript格式，再使用torch.jit.save()和torch.jit.load()加载模型。  可以解决旧版本模型加载失败的问题。  </li><li>导出格式：</li></ol><table><thead><tr><th>格式</th><th>使用命令</th><th>特点</th><th>适用场景</th></tr></thead><tbody><tr><td><a href="https://docs.pytorch.ac.cn/docs/stable/jit.html">TorchScript</a></td><td>torch.jit.trace() torch.jit.script()</td><td>Pytorch原生格式，保持动态图特性</td><td>Pytorch生态内部</td></tr><tr><td><a href="https://docs.pytorch.ac.cn/docs/stable/onnx.html">ONNX</a></td><td>torch.onnx.export()</td><td>开发标准，跨框架兼容</td><td>多框架协同环境</td></tr><tr><td><a href="https://developer.nvidia.com/zh-cn/blog/accelerating-inference-up-to-6x-faster-in-pytorch-with-torch-tensorrt/">Torch-TensorRT</a></td><td><a href="https://docs.pytorch.ac.cn/TensorRT/">import torch_tensorrt</a></td><td>nvidia优化格式</td><td>GPU推理加速</td></tr></tbody></table><ol start="4"><li>注意事项：</li></ol><ul><li>TorchScript导出前要使用model.eval()调用模型，以转换模型模式状态为评估。</li></ul><h3 id="模型部署"><a href="#模型部署" class="headerlink" title="模型部署"></a>模型部署</h3><ol><li>目的：使用AI平台(<a href="https://support.huaweicloud.com/modelarts/index.html">ModelArts</a>)，通过API调用的方式，将AI能力整合到Web等系统中，供调用使用。  </li><li>一般部署流程：训练模型-》模型优化-》格式转换-》部署环境选择-》服务封装-》性能监控  </li><li>部署方式：</li></ol><ul><li>本地部署：ONNX部署，使用onnxruntime模块(ONNX Runtime 是由微软维护的一个跨平台机器学习推理加速器)，使用onnxruntime.InferenceSession()加载模型，创建推理会话，再使用onnxruntime.InferenceSession().run()执行推理  </li><li>云端部署（优先）：使用<a href="https://fastapi.tiangolo.com/zh/tutorial/#_1">fastapi</a>模块构建REST API，通过API接口调用。  <a href="https://www.runoob.com/fastapi/fastapi-install.html">fastapi</a>工具使用。</li></ul><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] <a href="https://study.163.com/course/introduction/1208894818.htm">深度学习与PyTorch入门实战</a><br>[2] Zhang, A., Lipton, Z. C., Li, M., &amp; Smola, A. J. (2023). <a href="https://zh.d2l.ai/index.html">Dive into Deep Learning</a>. Cambridge University Press. URL: <a href="https://d2l.ai/">https://D2L.ai</a><br>[3] <a href="https://github.com/dragen1860/Deep-Learning-with-PyTorch-book">PyTorch深度学习</a><br>[4] <a href="https://www.runoob.com/PyTorch/PyTorch-tutorial.html">菜鸟教程</a><br>[5] <a href="https://datawhalechina.github.io/thorough-PyTorch/index.html#">深入浅出PyTorch</a></p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
            <tag> 算法 </tag>
            
            <tag> 数据 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> PyTorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch基础教程及注意事项-神经网络篇</title>
      <link href="/2025/08/25/PyTorch%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B%E5%8F%8A%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AF%87/"/>
      <url>/2025/08/25/PyTorch%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B%E5%8F%8A%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h2 id="神经网络模型"><a href="#神经网络模型" class="headerlink" title="神经网络模型"></a>神经网络模型</h2><p>定义：模仿人脑的计算模型，由连接节点（神经元）组成，节点按照层次排列。</p><ul><li>神经元（Neuron）:<ul><li>定义：神经网络基本单元，由输入（Input）、’权重（weights）、偏置(bias)、求和函数(summation function)、阈值(activation potential)、激活函数(activation function)、输出（output）构成。</li><li>用途：接收输入数据(起始点，特征向量)，将输入数据加权求和与偏置相加，通过阈值和激活函数处理后，产生输出数据（终点，预测结果）。</li></ul></li><li>网络层（Layer）:<ul><li>定义：由多个神经元组成，神经元之间的连接密度和类型构造网络配置。</li><li>组成（最简）：输入层(Input Layer，接收输入数据)、隐藏层(Hidden Layer, 数据处理)、输出层(Output Layer, 产生输出结果)。</li></ul></li></ul><h3 id="前馈神经网络-Feedforward-Neural-Networks-FNN"><a href="#前馈神经网络-Feedforward-Neural-Networks-FNN" class="headerlink" title="前馈神经网络(Feedforward Neural Networks, FNN)"></a>前馈神经网络(Feedforward Neural Networks, FNN)</h3><ol><li>定义：基础神经网络，数据从输入到输出单向流动。</li><li>特点：数据从输入层开始，经过隐藏层计算，最后到达输出层输出预测结果，数据单向流动，全过程没有反馈和循环等方向操作。</li><li>结构：输入层（网络数据入口，每个节点代表一种输入特征）、隐藏层（每层由多个神经元构成，每个神经元通过激活函数体现其非线性能力，用于获取数据的非线性特征）、输出层（网络预测结果出口，其节点个数和问题有关）。</li></ol><h3 id="卷积神经网络-Convolutional-Neural-Networks-CNN"><a href="#卷积神经网络-Convolutional-Neural-Networks-CNN" class="headerlink" title="卷积神经网络(Convolutional Neural Networks, CNN)"></a>卷积神经网络(Convolutional Neural Networks, CNN)</h3><ol><li>定义：专门处理网络拓扑结构数据的神经网络模型，是机器视觉核心技术。可以使用卷积层提取空间特征。</li><li>结构：</li></ol><ul><li>输入层（Input Layer）：原始数据图像入口，用于接收类似于图像的三维数组（图像高度、宽度和颜色通道）。</li><li>卷积层(Convolutional Layer)：根据卷积公式，用卷积核(Kernel)提取局部特征，生成特征图。</li><li>非线性激活函数层(Activation Function)：引入非线性特征，增强网络适应性。</li><li>池化层(Pooling Layer)：在卷积层后，在保留最重要特征信息情况下，通过池化技术(最大池化-区域最大值&#x2F;平均池化-区域平均值)降低特征图空间维度，减少计算和参数数量，生成池化特征图。</li><li>归一化层（Normalization Layer, 可选）：采用归一化技术（局部响应归一化&#x2F;批归一化），使多维特征图转换为一维向量，加速训练，提高模型稳定性。</li><li>正则化（Regularization, 可选）：用于防止模型过拟合。</li><li>损失函数层(Loss Function)：衡量模型预测和实际结果之间的差异。</li><li>优化器(Optimizer)：根据损失函数对参数的梯度更新模型参数。</li><li>全连接层(Fully Connected Layer)：用于综合所有提取的特征映射到输出，并进行最后的分类或回归。</li><li>输出层(Output Layer)：模型预测结果出口。</li></ul><h3 id="循环神经网络-Recurrent-Neural-Networks-RNN"><a href="#循环神经网络-Recurrent-Neural-Networks-RNN" class="headerlink" title="循环神经网络(Recurrent Neural Networks, RNN)"></a>循环神经网络(Recurrent Neural Networks, RNN)</h3><ol><li>定义：允许信息反馈循环，适用于序列数据（时间序列、语音识别、自然语言）。</li><li>特点：“记忆能力”，使用隐状态(hidden state)，在隐藏层可以保留以前时间调用的信息，进而捕获数据中的时间等前后依赖关系。</li><li>典型RNN模块：</li></ol><ul><li>torch.nn.RNN: 基本RNN单元。</li><li>torch.nn.LSTM: 长短期记忆网络(Long short-Term Memory, LSTM)，RNN的变种，能学习长期依赖关系。</li><li>torch.nn.GRU: 门控循环单元，LSTM简化版本。</li></ul><h2 id="神经网络模型训练过程简介（Training-Process）"><a href="#神经网络模型训练过程简介（Training-Process）" class="headerlink" title="神经网络模型训练过程简介（Training Process）"></a>神经网络模型训练过程简介（Training Process）</h2><ol><li>数据ETL：</li></ol><ul><li>收集和处理数据，包括数据清洗、标准化和归一化。</li><li>数据分割，包括训练集、验证集和测试集。</li></ul><ol start="2"><li>定义网络模型：</li></ol><ul><li>设计模型架构，在业务需求、设备支持、项目交付、行管规则等要求下，选择合适的模型，定义网络层、前向传播过程、激活函数等。</li><li>设置初始化模型参数（权重和偏置）。</li><li>选择损失函数：根据问题特点选择合适的损失函数（分类或回归等）。</li><li>选择优化器：根据需要选择优化算法，更新模型参数。</li></ul><ol start="3"><li>前向传播（Forward Propagation）：</li></ol><ul><li>在每次迭代中，根据输入数据通过模型传递，计算预测输出。</li><li>开启训练前需要清除梯度，调整模型进入训练模式（model.train()）。</li></ul><ol start="4"><li>计算损失(Calulate Loss)：</li></ol><ul><li>使用损失函数计算评估预测输出和实际输出之间的差异。</li></ul><ol start="5"><li>反向传播(Backpropagation)：</li></ol><ul><li>利用自动求导计算损失函数相对于模型参数（权重和偏置）的梯度。</li><li>一般调度torch.nn.MSEloss().backward()等计算。</li></ul><ol start="6"><li>参数更新(Parameter Update)：</li></ol><ul><li>调用优化器，根据计算出的梯度和优化器策略，更新模型参数。</li><li>一般通过optim.SGD().step()等更新参数。</li></ul><ol start="7"><li>迭代优化(Iteration)：</li></ol><ul><li>重新循环上述步骤，直到模型在验证集上是性能不能再提升，或者迭代达到预定次数。</li><li>此过程也是试算过程的开始，考虑到数据、方案、环境等因素的不完善，模型方案不一定有好的结果，需不断校验检查。</li></ul><ol start="8"><li>测试评估：</li></ol><ul><li>利用测试集评估模型性能，确保模型没有过拟合或者欠拟合。</li><li>计算准确率(Accuracy):计算正确预测比例（分类问题）。</li><li>测试评估前需要调整模型进入评估模式（model.eval()）,并且在评估过程中要禁用梯度计算。（torch.no_grad(), 减少不必要的计算和内存开销）,以确保模型能正确推理。</li></ul><ol start="9"><li>模型调优：</li></ol><ul><li>根据模型在测试集上的表现调参，优化模型各项配置和参数。</li></ul><ol start="10"><li>部署模型：</li></ol><ul><li>将训练好的模型，根据部署平台的使用要求，部署到生产环境中，用于实际工作。</li></ul><h2 id="PyTorch神经网络工具字典"><a href="#PyTorch神经网络工具字典" class="headerlink" title="PyTorch神经网络工具字典"></a>PyTorch神经网络工具字典</h2><p>  关键模块：torch.nn（网络模块）、torch.optim（优化器模块）、 torch.autograd(自动微分)</p><ol><li><a href="https://docs.pytorch.ac.cn/docs/stable/nn.html#module-torch.nn.parallel">torch.nn</a>模块组成：</li></ol><ul><li>关键类：torch.nn.Module类，是所有神经网络模块的基类，可以用来从这个基类派生出自己的模型类，并定义其中的网络层结构和前向传播过程。自定义神经网络模型时，需要定义这两部分：<strong>init</strong>()(定义网络层)、forward()(定义数据前向传播过程)。</li><li>预定义层（Modules）:包含各种层组件，如卷积层、线性层、池化层、归一化层、循环神经网络层、嵌入层、Dropout层、非线性激活函数（Activation Function，决定神经元是否应该被激活）等。</li><li>容器类(Containers)：由模块（torch.nn.Module）、序列(torch.nn.Sequential)、模块列表(torch.nn.ModuleList)、模块字典(torch.nn.ModuleDict)、参数列表(torch.nn.ParameterList)、参数字典(torch.nn.ParameterDict)组成。</li><li>损失函数（Loss Function）:衡量模型预测值和真实值之间的差异。</li><li>实用函数（Functional Interface）：torch.nn.functional(作用于张量上的实现和层对象相同功能的函数)。</li><li>初始化方法：torch.nn.init(权重初始化策略)。</li></ul><ol start="2"><li>torch.nn常用组件：</li></ol><ul><li>卷积层</li></ul><table><thead><tr><th>组件</th><th>描述和特点</th></tr></thead><tbody><tr><td>torch.nn.Conv2d()</td><td>2D卷积层，常用于图像</td></tr></tbody></table><ul><li>线性层</li></ul><table><thead><tr><th>组件</th><th>描述和特点</th></tr></thead><tbody><tr><td>torch.nn.Linear(in_features，out_features)</td><td>输入in_features个特征，输出out_features个特征</td></tr></tbody></table><ul><li>池化层</li></ul><table><thead><tr><th>组件</th><th>描述和特点</th></tr></thead><tbody><tr><td>torch.nn.MaxPool2d()</td><td>2D最大池化层，常用于降维</td></tr></tbody></table><ul><li>激活函数</li></ul><table><thead><tr><th>组件</th><th>描述和特点</th></tr></thead><tbody><tr><td>torch.nn.functional.relu()</td><td>定义为f(x) &#x3D; max(0, x)，常用于隐藏层</td></tr><tr><td>torch.nn.functional.sigmoid()</td><td>输出值为0和1之间，适合二分类问题</td></tr><tr><td>torch.nn.functional.tanh()</td><td>输出值在-1和1之间，适合输出层使用</td></tr><tr><td>torch.nn.functional.softmax</td><td>将输出转换为概览分布，适用多分类的输出层</td></tr></tbody></table><ul><li>损失函数</li></ul><table><thead><tr><th>组件</th><th>描述和特点</th><th>适配场景</th></tr></thead><tbody><tr><td>torch.nn.MSELoss()</td><td>均方误差（L2范数），计算输出和目标值之间的平方差</td><td>回归问题</td></tr><tr><td>torch.nn.CrossEntropyLoss()</td><td>计算输出和目标值之间的交叉熵</td><td>分类问题</td></tr><tr><td>torch.nn.BCEWithLogitsLoss()</td><td>计算Sigmoid激活和二元交叉熵的损失</td><td>二分类问题</td></tr></tbody></table><ol start="3"><li><a href="https://docs.pytorch.ac.cn/docs/stable/optim.html">torch.optim</a></li></ol><ul><li>功能：根据损失函数的梯度，在训练过程中自动化更新网络模型参数（权重和偏置），在避免局部最优的情况下，参数加速收敛到最优解，进而使模型预测结果逐步优化逼近目标值。</li><li>优化器（Optimizer）选择判断：数据是否稀疏-&gt;是否需要快速收敛</li><li>常用优化器类型：</li></ul><table><thead><tr><th>优化器名称</th><th>中文名称</th><th>调度方法</th><th>收敛速度</th><th>内存占用</th><th>超参数敏感度</th><th>特点</th><th>适用场景</th></tr></thead><tbody><tr><td>SGD</td><td>随机梯度下降</td><td>torch.optim.SGD(params, lr &#x3D; 0.01, momentum &#x3D; 0, weight_decay &#x3D; 0)</td><td>慢</td><td>低</td><td>高</td><td>简单，可添加动量加速收敛，适合和基准比较，使用梯度的移动平均值（一阶矩）</td><td>基础简单模型</td></tr><tr><td>Adam</td><td>自适用矩估计</td><td>torch.optim.Adam(params, lr &#x3D; 0.001, betas &#x3D; (0.9, 0.999), eips &#x3D; 1e-08, amsgrad &#x3D; False)</td><td>快</td><td>中</td><td>低</td><td>可自适应学习率，可计算每个参数的学习率，结合SGD和RMSprop的特点</td><td>绝大多数的深度学习任务</td></tr><tr><td>RMSprop</td><td>均方根传递</td><td>torch.optim.RMSprop(params, lr &#x3D; 0.01, alpha &#x3D; 0.99)</td><td>快</td><td>中</td><td>中</td><td>适应学习率，使用平方梯度的移动平均值来缩放梯度（二阶矩）</td><td>RNN网络</td></tr><tr><td>Adagrad</td><td>自适应学习率应梯度下降</td><td>torch.optim.Adagrad(params, lr &#x3D; 0.01, initial_accumulator_value &#x3D; 0)</td><td>先快后慢</td><td>中</td><td>高</td><td>参数独立学习率，学习率随时间减小</td><td>稀疏数据</td></tr></tbody></table><p>4 <a href="https://docs.pytorch.ac.cn/docs/stable/autograd.html">torch.autograd</a></p><ul><li>功能：各种类和函数对任意标量函数计算数学函数的导数，主要用来自动计算梯度。深度学习自动求导主要用于在神经网络计算梯度和反向传播算法实现。</li></ul><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] <a href="https://study.163.com/course/introduction/1208894818.htm">深度学习与PyTorch入门实战</a><br>[2] Zhang, A., Lipton, Z. C., Li, M., &amp; Smola, A. J. (2023). <a href="https://zh.d2l.ai/index.html">Dive into Deep Learning</a>. Cambridge University Press. URL: <a href="https://d2l.ai/">https://D2L.ai</a><br>[3] <a href="https://github.com/dragen1860/Deep-Learning-with-PyTorch-book">PyTorch深度学习</a><br>[4] <a href="https://www.runoob.com/PyTorch/PyTorch-tutorial.html">菜鸟教程</a><br>[5] <a href="https://datawhalechina.github.io/thorough-PyTorch/index.html#">深入浅出PyTorch</a></p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
            <tag> 算法 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> PyTorch </tag>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>昇思MindSpore基础教程</title>
      <link href="/2025/08/18/%E6%98%87%E6%80%9DMindSpore%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B/"/>
      <url>/2025/08/18/%E6%98%87%E6%80%9DMindSpore%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>全场景、深度学习计算框架，可支持CPU、GPU、华为AI昇腾系列处理器等硬件,可使用Linux、windows、MacOS系统。<br>教程目的：熟悉框架语法结构，熟悉从构建数据集到训练的全流程</p><h2 id="MindSpore安装"><a href="#MindSpore安装" class="headerlink" title="MindSpore安装"></a>MindSpore安装</h2><p><a href="https://www.mindspore.cn/install/#guide%5C">MindSpore安装</a><br>验证安装成功会显示如下信息：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">MindSpore version: 版本号</span><br><span class="line">The result of multiplication calculation is correct, MindSpore has been installed on platform [CPU] successfully!</span><br></pre></td></tr></table></figure><p>注意：<br>     - 版本选择2.7.0-rc1,选择2.6.0的时候会导致下载失败。<br>     - 下载请在管理员下的命令提示符下进行。</p><h2 id="Tensor-创建-属性-操作-NumPy"><a href="#Tensor-创建-属性-操作-NumPy" class="headerlink" title="Tensor: 创建-属性-操作-NumPy"></a>Tensor: 创建-属性-操作-NumPy</h2><p>张量（Tensor）是MindSpore中的基本数据结构，存储多维数组的数据结构。</p><ol><li>张量属性：形状(shape)和数据类型(dtype)<br> 形状：定义张量的维度，是tuple类型，使用张量.shape查看<br> 数据类型：使用张量.dtype查看，定义张量的数据类型和大小，包括基础数据类型、其他类型、类型转换规则，必须使用<a href="https://www.mindspore.cn/docs/zh-CN/r2.7.0rc1/api_python/mindspore/mindspore.dtype.html">mindspore.dtype</a>中定义的类型。</li><li>张量初始化：<ul><li>直接创建：mindspore.Tensor()。<a href="https://www.mindspore.cn/docs/zh-CN/r2.7.0rc1/api_python/mindspore/mindspore.Tensor.html">Tensor参考API文档</a></li><li>使用numpy创建：mindspore.Tensor(numpy.array())</li><li>从旧张量中创建新张量：mindspore.ops.OnesLike(Tensor())</li></ul></li><li>张量运算：涉及算术、线性代数、矩阵运算等。可使用类numpy的索引和切片使用方式，存在和pandas类似的连接(cancat)和合并使用方式.</li><li>与NumPy的数据转换：<ul><li>NumPy转换为Tensor: mindspore.Tensor(numpy.array())</li><li>Tensor转转换为NumPy：mindspore.Tensor.asnumpy(),Tensor和ndarray会共享内存地址。</li></ul></li><li>注意：<ul><li><p>使用前需要设置环境上下文，设置模式和目标设备类型，例如：</p></li><li><pre><code>from mindspore import contextcontext.set_context(mode=context.GRAPH_MODE, device_target=&quot;CPU&quot;)</code></pre><p>模式包括GRAPH_MODE（静态图模式）和PYNATIVE_MODE（动态图模式）两种。<br>目标设备类型包括CPU、GPU、Ascend(昇腾)</p></li><li><p><a href="https://www.mindspore.cn/docs/zh-CN/r2.7.0rc1/api_python/mindspore.ops.html#parameter%E6%93%8D%E4%BD%9C%E5%87%BD%E6%95%B0">mindspore.obs</a>提供function接口，涉及Tensor操作函数、数学运算函数、神经网络函数、微分函数、调试函数等。</p></li><li><p>使用时需要注意有些API函数中存在支持平台的限制，例如Ascend(昇腾)、GPU、CPU，有些函数只支持CPU或者GPU,有些实验性质的API只支持Ascend。</p></li><li><p>当使用 init 参数来初始化 Tensor 时，通常需要使用 Tensor.init_data 来加载 Tensor 的数据</p></li></ul></li></ol><h2 id="数据：创建-自定义-数据处理"><a href="#数据：创建-自定义-数据处理" class="headerlink" title="数据：创建-自定义-数据处理"></a>数据：创建-自定义-数据处理</h2><ol><li><a href="https://www.mindspore.cn/docs/zh-CN/r2.7.0rc1/api_python/mindspore.dataset.loading.html#%E9%87%87%E6%A0%B7%E5%99%A8-1">mindspore.dataset</a>：<br>性质：核心数据加载模块，基于Pipeline的数据引擎。<br>作用：提供数据采样、增强变换、数据批处理等功能，通过使用不同的数据加载方式，和数据集加载API配合使用。<br>数据加载方式三种：开源数据集加载、标准格式数据集加载、自定义数据集加载<br>开源数据集加载：视觉数据集、文本数据集、音频数据集<br>标准格式数据集加载：加载业界标准数据格式的数据集文件，例如TFRecord、MindRecord等格式。<br>自定义数据集加载（mindspore.dataset.GeneratorDataset）：自定义Python数据源，自定义数据读取和处理逻辑。</li><li>数据处理步骤：加载数据集、数据集操作、批处理、迭代器、数据增强（数据集样本变换）</li></ol><ul><li>数据加载：</li><li>加载数据集（自定义、开源和标准格式数据集）<ul><li>加载数据集(开源和标准格式)：先用模块提供的API加载和处理数据集import mindspore.dataset as ds。再通过使用jupyter notebook中运行linux命令下载数据集并解压，存放到指定路径中，注意数据集的目录结构。使用时需要注意使用的数据集文件所在的根目录路径，有些加载必须使用该路径才能使用数据。</li><li>加载自定义数据集：使用<a href="https://www.mindspore.cn/docs/zh-CN/r2.7.0rc1/api_python/dataset/mindspore.dataset.GeneratorDataset.html#mindspore.dataset.GeneratorDataset">GeneratorDataset接口</a>自定义数据集加载。可以先自定义数据集类(DatasetGenerator)，包含数据初始化操作__init__, 数据访问方法__getitem__, 数据量统计方法__len__等，再通过GeneratorDataset接口加载自定义数据集类访问数据集中的数据样本。</li></ul></li><li>数据集操作、批处理、迭代器、数据增强（数据集样本变换）<ul><li>数据集操作（filter&#x2F; skip）：通过使用对象方法  .filter &#x2F; .skip&#x2F; .shuffle来实现数据集的进一步过滤、跳过、混洗等操作。注意：.shuffle等方法是实例方法，使用时需要先创建实例，再调度数据集操作方法。</li><li>批处理(mindspore.dataset.Dataset.batch):将数据集中的多条数据组合为一个批数据。</li><li>迭代器(mindspore.dataset.Dataset.create_dict_iterator):创建数据集迭代器，返回字典样式的样本数据，可实现预处理过程中的数据循环输出。</li><li>数据增强：数据集样本变换（mindspore.dataset.transforms)，一种通用数据增强方法，特别是在数据量过小或在样本单一等问题场景下影响模型训练效果时，通过使用数据增强操作进一步扩展样本多样性和操作，从而提升模型的泛化能力。</li></ul></li></ul><h2 id="模型：定义-模型结构-模型层-模型参数"><a href="#模型：定义-模型结构-模型层-模型参数" class="headerlink" title="模型：定义-模型结构-模型层-模型参数"></a>模型：定义-模型结构-模型层-模型参数</h2><p>以神经网络模型的构建为例</p><ol><li>关键模块：<br> <a href="https://www.mindspore.cn/docs/zh-CN/r2.7.0rc1/api_python/mindspore.nn.html#%E5%9F%BA%E6%9C%AC%E6%9E%84%E6%88%90%E5%8D%95%E5%85%83">mindspore.nn</a>：神经网络模块，用于构建神经网络中的预定义构建块或者计算单元。<br> 其中包括基本构成单元、容器（构造神经元）、封装层、卷积神经网络层、循环神经网络层、Transformer层、嵌入层、非线性激活函数层、线性层、Dropout层（随机丢弃层）、归一化层、池化层、填充层、损失函数、优化器、动态学习率、图像处理层、公共层、工具等部分组成。</li><li>关键类：<br> mindspore.nn.Cell:MindSpore中神经网络的基本构成单元，是所有网络的基类，所有模型或神经网络层应当继承该基类，并重新其中的__init__方法和construct方法。Cell(神经元)在GRAPH_MODE（静态图模式）下将编译为一张计算图，在PYNATIVE_MODE（动态图模式）下作为神经网络的基础模块。</li></ol><ul><li>construct:定义要执行的计算逻辑，所有子类必须重写此方法。</li><li>parameters_and_names：返回当前Cell或所有子Cell的参数名称和参数本身</li></ul><ol start="3"><li>关键成员函数：</li></ol><ul><li>二维卷积神经网络层（mindspore.nn.Conv2d）：根据输入Tensor张量计算二维卷积。先根据参数建立卷积神经网络模型，再根据输入Tensor，输出Tensor.可用于在神经网络中提取特征。</li><li>全连接线性层（mindspore.nn.Dense）：根据输入Tensor，计算出线性变换后的输出Tensor， 可用于对输入张量做线性变换。使用的激活函数可以指定具体的激活函数名，例如mindspore.nn.ReLU</li><li>非线性激活函数层（mindspore.nn.ReLU）:逐元素计算修正线性单元激活函数，逐位输出各位中数值和零的最大值。输入和输出都是Tensor，在网络中添加非线性激活函数，可用于神经网络学习复杂特征。</li><li>池化层（mindspore.nn.MaxPool2d）：在输入Tensor上应用2D最大池化运算，组成2D平面，可用于数组降采样。</li><li>公共层（mindspore.nn.Flatten）：对输入的Tensor按照输入维到输出维进行展平。</li></ul><h2 id="自动微分：求导-梯度缩放-停止计算-梯度"><a href="#自动微分：求导-梯度缩放-停止计算-梯度" class="headerlink" title="自动微分：求导-梯度缩放-停止计算-梯度"></a>自动微分：求导-梯度缩放-停止计算-梯度</h2><ol><li>参数设置:<br> mindspore.Parameter：Tensor子类，当被绑定为Cell属性时，可以同Cell的方法获取。<br> mindspore.ParameterTuple：用于管理多个mindspore.Parameter，实现将网络参数存储到参数元祖集合中。</li><li>梯度计算：<br> mindspore.ops.matmul:计算输入的两个Tensor乘积，输入的两个Tensor数据类型必须一致。<br> mindspore.ops.GradOperation (get_all&#x3D;False, get_by_list&#x3D;False, sens_param&#x3D;False)，一阶导数方法，来自mindspore.ops.primitive的框架算子，一个高阶函数，为输入函数生成对应的梯度函数。可实现对输入求导（返回第一个输入的梯度和所有输入的梯度）、对参数求导和同时对输入和参数求导。其中</li></ol><ul><li>get_all为False时，只会对第一个输入求导，为True时，会对所有输入求导</li><li>get_by_list为False时，不会对权重求导，为True时，会对权重求导</li><li>sens_param对网络的输出值做缩放以改变最终梯度，配合缩放指数，确保缩放指数的维度和输出维度保持一致。</li><li>mindspore.ops.stop_gradient:用于消除某个值对梯度的影响，例如截断来自于函数输出的梯度传播，可以用来禁止网络内的算子对梯度的影响。</li></ul><ol start="3"><li>求导步骤：</li></ol><ul><li>定义使用算子定义网络结构、定义求导网络、根据具体输入值计算计算求导值。</li><li>对权重求一阶导，需要将mindspore.ops.GradOperation中的get_by_list设置为Trure.如果对某些权重不进行求导，则在定义网络结构中，对相应权重参数值mindspore.Parameter中的requires_grad设置为False.</li><li>对梯度值做缩放，需要将mindspore.ops.GradOperation中的sens_param设置为Trure，并在求导网络中定义缩放指数self.grad_wrt_output，再将该指数用于求导函数中。</li></ul><h2 id="优化模型：损失函数-优化器"><a href="#优化模型：损失函数-优化器" class="headerlink" title="优化模型：损失函数-优化器"></a>优化模型：损失函数-优化器</h2><ol><li>超参：可调整参数，用于控制模型训练优化过程，影响模型训练和收敛速度等。<br> 一般超参如下：</li></ol><ul><li>训练轮次（epoch）:训练时遍历数据集的次数，一般用于训练mindspore.train.Model.train方法中</li><li>批次大小（batch size）:用于构造训练数据集，数据集进行分批次读取训练，并设定每个批次数据大小。</li><li>学习率（learning rate）:用于优化器参数设置中，学习率偏小会导致收敛速度偏慢，过大则导致训练不收敛等不可预测问题。</li></ul><ol start="2"><li>损失函数：评价模型的预测值和真实值不一样的程度，损失函数可以直接使用mindspore.nn.L1Loss（计算预测值和目标值之间的平均绝对误差）这类损失函数定义。</li><li>优化器：用于计算和更新梯度，优化器的选择直接影响最终模型的性能。优化器在模块mindspore.nn中，mindspore的所有优化逻辑都封装在Optimizer对象中，要使用优化器，需要构建Optimizer对象，要能够保持参数状态并基于计算得到的梯度能进行参数更新。构造Optimizer对象前，需要先确定包含所有需要优化参数的迭代器，如网络中需要训练的参数parameter等。<br> 优化器可以直接使用mindspore.nn.SGD（随机梯度下降的实现）这类优化器函数定义。</li><li><a href="https://www.mindspore.cn/docs/zh-CN/r2.7.0rc1/api_python/train/mindspore.train.Model.html#mindspore.train.Model.train">mindspore.train.Model</a>：模型训练和推理高阶接口，根据用户的传入参数、损失函数、优化器，封装可训练或推理的实例。<br> train方法：模型训练接口，可根据训练执行轮次epoch，训练数据集train_dataset，开始模型训练。</li><li>模型训练四步骤：</li></ol><ul><li>定义神经网络</li><li>构建数据集</li><li>定义超参、损失函数和优化器</li><li>输入训练轮次和数据集开始模型训练。</li></ul><h2 id="保持加载：保存模型-加载权重-导出IR"><a href="#保持加载：保存模型-加载权重-导出IR" class="headerlink" title="保持加载：保存模型-加载权重-导出IR"></a>保持加载：保存模型-加载权重-导出IR</h2><h3 id="保存模型："><a href="#保存模型：" class="headerlink" title="保存模型："></a>保存模型：</h3><ol><li>主要使用Callback机制，使用API函数如下：</li></ol><ul><li><a href="https://www.mindspore.cn/docs/zh-CN/r2.7.0rc1/api_python/train/mindspore.train.ModelCheckpoint.html">mindspore.train.ModelCheckpoint</a>：checkpoint回调函数，用于在训练过程中，保存网络参数。注意：在分布式训练场景下，每个训练进程都需要指定不同的目录，保存checkpoint文件，否则可能训练失败。</li><li><a href="https://www.mindspore.cn/docs/zh-CN/r2.7.0rc1/api_python/train/mindspore.train.CheckpointConfig.html#mindspore.train.CheckpointConfig">mindspore.train.CheckpointConfig</a>：保存checkpoint配置策略</li></ul><ol start="2"><li>注意：</li></ol><ul><li>保存模型步骤：先设置checkpoint配置策略，再创建ModelCheckpoint对象，在将ModelCheckpoint对象传递给model.train训练方法，开始训练。</li><li>MindSpore为方便用户区分每次生成的CheckPoint文件，会在用户定义的前缀后添加”_”和数字加以区分。如果想要删除.ckpt文件时，请同步删除.meta 文件。</li></ul><h3 id="加载模型："><a href="#加载模型：" class="headerlink" title="加载模型："></a>加载模型：</h3><ol><li><a href="https://www.mindspore.cn/docs/zh-CN/r2.7.0rc1/api_python/mindspore/mindspore.load_checkpoint.html#mindspore.load_checkpoint">mindspore.load_checkpoint</a>:加载checkpoint文件，返回值是字典。主要用于将参数文件中的网络参数存入自定义的参数字典中，配合模型实例，将参数加载入网络中。</li><li><a href="https://www.mindspore.cn/tutorials/zh-CN/r1.3/save_load_model.html">mindspore.load_param_into_net</a>:将自定义参数字典中的参数加载到网络或优化器中，加载后，网络中的参数就是之前CheckPoint保存的参数，返回网络中没有被加载的参数列表。</li></ol><h3 id="验证模型："><a href="#验证模型：" class="headerlink" title="验证模型："></a>验证模型：</h3><ol><li>推理场景：<br> <a href="https://www.mindspore.cn/docs/zh-CN/r2.7.0rc1/api_python/train/mindspore.train.Model.html#mindspore.train.Model.train">mindspore.train.Model</a>：模型训练或推理高阶接口。 根据用户传入的参数，封装可训练或推理的实例。</li></ol><ul><li>eval方法：模型评估接口。</li><li>train方法：模型训练接口。</li><li>验证步骤：先定义验证数据集，再利用数据集，调度eval方法进行推理验证。</li></ul><ol start="2"><li>任务中断再训练或者微调场景：</li></ol><ul><li>验证步骤：先定义训练轮次和训练数据集，再利用数据集，调度train方法进行训练。可用于迁移学习。</li></ul><h3 id="导出模型："><a href="#导出模型：" class="headerlink" title="导出模型："></a>导出模型：</h3><p>通过网络和CheckPoint格式文件生成对应模型格式文件，实现在不同硬件平台上的推理。</p><ol><li><a href="https://www.mindspore.cn/docs/zh-CN/r2.7.0rc1/api_python/mindspore/mindspore.export.html#mindspore.export">mindspore.export</a>:将MindSpore网络模型导出为指定格式的文件。可导出ONNX&#x2F;AIR&#x2F;MINDIR三种格式文件。当导出文件格式为AIR、ONNX时，单个Tensor的大小不能超过2GB。</li><li>文件导出格式：</li></ol><ul><li>ONNX(Open Neural Network eXchange):一种针对机器学习所设计的开放式的文件格式，由微软提出，与环境和平台无关的标准格式，2.2 用于在不同深度学习框架共享和交换已训练好的模型。推荐的输出文件后缀是”.onnx”.</li><li>AIR(Ascend Intermediate Representation):一种Ascend模型的中间表示格式。类似ONNX，由华为提出，能更好适配昇腾AI处理器。推荐的输出文件后缀是”.air”.</li><li>MINDIR(MindSpore Native Intermediate Representation for Anf):一种MindSpore模型的中间表示格式。推荐的输出文件后缀是”.mindir”.</li></ul><ol start="3"><li>格式导出：</li></ol><ul><li><p>ONNX格式：可在第三方硬件平台上推理。导出文件名称会自动添加“.onnx”后缀。</p><p><code>export(..., file_format = &#39;ONNX&#39;)</code></p></li><li><p>AIR格式：可在昇腾AI处理器上推理。导出文件名称会自动添加“.air”后缀。<br><code>export(..., file_format = &#39;AIR&#39;)</code></p></li><li><p>MINDIR格式：可在MindSpore端侧（GPU&#x2F;CPU&#x2F;Ascend）上推理。导出文件名称会自动添加“.mindir”后缀。<br><code>export(..., file_format = &#39;MINDIR&#39;)</code></p></li></ul><h2 id="端侧推理：模型加载-植入APP-推理"><a href="#端侧推理：模型加载-植入APP-推理" class="headerlink" title="端侧推理：模型加载-植入APP-推理"></a>端侧推理：模型加载-植入APP-推理</h2><p>不同推理设备有不同的推理方法，模型可以在昇腾Ascend处理器和移动设备上进行推理，由于昇腾Ascend处理器推理配置复杂，本文主要介绍移动设备推理。</p><ol><li><a href="https://www.mindspore.cn/lite/docs/zh-CN/r2.7.0rc1/index.html">MindSpore Lite</a>:AI引擎，支持GPU&#x2F;CPU&#x2F;NPU异构调度，支持模型轻量化、全场景(IOS&#x2F;安卓&#x2F;Huawei LiteOS系统)部署，支持MindSpore、TensorFlow Lite、Caffe和ONNX 4类AI框架（不支持Pytorch）。</li><li>工作流程和步骤：</li></ol><ul><li>选择模型：可使用预置的终端模型，或者使用自己已训练好的模型。</li><li>模型转换：主要是格式转换<br>转换工具下载：按照linux和windows系统的不同，选择不同的转换工具压缩包。<br>转换工具使用：不同系统转换工具不同，执行转换命令时，–fmk参数表示输入模型的原始格式，例如MINDIR;–modeFile表示输入模型路径；–outputFile表示模型输出路径，转换后的模型会自动添加.ms后缀，转换为.ms格式。</li><li>构建环境和运行：不同系统编译和推理有差异。<br>编译构建：windows系统要比linux麻烦一点，需要下载库和模型。<br>执行推理：编译完成后，进入模型目录，调用.ms格式的模型文件</li></ul><p>基于公开来源资料mindspore教程1.3版本，详情请看<a href="https://www.mindspore.cn/tutorials/zh-CN/r1.3/custom.html#">MindSpore官网</a></p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
            <tag> 算法 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> MindSpore </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GaussDB(DWS)数据库-巡检运维篇</title>
      <link href="/2025/08/04/GaussDB(DWS)%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%B7%A1%E6%A3%80%E8%BF%90%E7%BB%B4%E7%AF%87/"/>
      <url>/2025/08/04/GaussDB(DWS)%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%B7%A1%E6%A3%80%E8%BF%90%E7%BB%B4%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h3 id="基本运维动作"><a href="#基本运维动作" class="headerlink" title="基本运维动作"></a>基本运维动作</h3><ol><li><p>常用运维命令：</p><ul><li>查看集群状态或单个主机的状态：cm_ctl query -Cv；Normal:表示集群可用，主备关系正常；Degraded:表示集群可用，但是数据没有冗余备份；Unavailable:表示集群不可用；Catchup:表示集群的DN备实例正在追赶主实例的日志信息。</li><li>切换集群主备实例、实现AZ之间的相互切换：cm_ctl switchover -a</li><li>启停实例：<ul><li>注意事项<ul><li>适用场景：主机发生故障状态异常，需要停止所有实例；或单台服务器、硬盘故障，需要停止对应的实例进行检修更换等</li><li>前提条件：停止节点&#x2F;实例后，需要保证集群内至少有一个正常的CN节点、cm_server节点、gtm节点；涉及cn节点、实例后，如果没有负载均衡，需要将业务迁移至其他CN；若集群磁盘水位处于75%以上，停止节点、实例后需要重点关注磁盘水位变化；停止实例前，需要检查集群状态，保证对应实例状态正常，无catchup；停止实例前，连接集群执行checkpoint ；启停集群服务、单个主机上的所有实例或者单独启停某个实例进程</li></ul></li><li>停止模式有如下几种：<ul><li>smart: 等待用户业务结束后，集群退出</li><li>fash:不等待用户业务结束，集群退出</li><li>immediate: 不等待用户业务结束，强制集群退出。</li></ul></li><li>修复节点：<ul><li>修复前提：适用场景：DN异常、cn处于delete或者down状态</li><li>前提条件：集群安装成功，并且处于已启动状态；DN环只能损坏一个实例；集群内CMServer、CMAgent、Coordinator至少存在一个正常实例；GTM故障时，另一个GTM必须处于最高可用状态</li><li>注意实现：正常节点执行修复操作；修复涉及cn需要保障被修复的cn没有ddl业务将发生故障的主机（实例）替换为正常主机（实例）</li></ul></li></ul></li></ul></li><li><p>常用运维SQL：</p><ul><li>活跃作业语句查询，使用场景：查询此时在数据库运行的作业信息，用于确定当前作业运行的SQL、耗时、运行状态，排队状态和等待状态等信息，pgxc_stat_activity视图</li><li>作业等待查询，使用场景：查询此时在数据库运行作业的等待信息，用于确定当前作业运行过程中，在各个DN实例等待事件和阻塞状态，是在等待IO还是在等待锁等，pgxc_thread_wait_status视图</li><li>锁等待查询，使用场景：查询此时在数据库运行的作业中等锁的信息，包括等锁的语句、用户和持有锁的语句、用户，用于确定当前作业系统中调度不合理的场景，比如当一个表在进行增加表字段时，其他插入或者查询作业会阻塞。pgxc_lock_conflicts视图。</li><li>单表倾斜：通过查看表数据在各个dn的分布情况判断是否存在数据倾斜。<ul><li>倾斜表重分布处理步骤：<ul><li>创建一张新的表，并选取合适的分布列，判断一列是否可以作为分布列，可通过下面的语句查询该列的值分布情况：<code> bash select attr, cont(*) from schema.table group by atttr;</code> </li><li>将数据从旧表导入到新表：   <code> bash insert into new_table select * from old_table;</code> </li><li>删除旧表：   <code> bash drop table old_table;</code> </li><li>将新表重命名为旧表：   <code> bash alter table new_table rename to old_table;</code></li></ul></li></ul></li><li>全库查倾斜表：查询全库数据表的倾斜情况，在集群表较多时可以指定条件统计倾斜率超过10%和表大小大于100G的表倾斜信息，针对每个倾斜大表，需要按照单表的倾斜处理办法进行处理。运维建议：针对业务表建议每周例行全库运维处理倾斜表，各个节点磁盘使用率差别不超过5%。</li><li>判断需要analyze的表：识别从未做过analyze的表；识别insert、update&#x2F;delete总量超过上次统计数据量20%的表。需要固化到业务中，立即做analyze的场景：每次truncate表后执行insert数据的场景。每天插入新数据后并且业务立即只查询刚插入的数据场景。</li><li>脏页回收：<ul><li>dws用户表数据在经过频繁插入、更新、删除后，会产生脏页，脏页会占用空间。在脏页率达到一定程度时需要使用vacuum full analyze命令清理，建议每月进行一次维护</li><li>使用方法：登录需要统计的数据库，执行上面的SQL语句，找到需要进行清理的表。执行SQL命令；vacuum full analyze 业务表：</li><li>调优建议：当脏页率&gt;30%或者脏数据行数&gt;1w时，对表做脏页回收。</li></ul></li><li>内存使用：查询当前数据库节点的内存使用情况，单位MB.查询当前节点内存使用情况：   <code>bash select * from pv_total_memory_detail;  </code></li></ul></li><li><p>运维日志：</p><ul><li>数据库日志：数据库日志记录了DWS数据库服务端启动，运行或停止时出现的问题，当数据库在启动，运行或者停止的过程中出现问题时，数据库用户可以通过运行日志，快速分析问题的产生原因。</li><li>管控面日志</li><li>操作系统日志：操作系统日志可以记录系统的运行状态和异常情况，并且用于故障排除和性能分析。</li></ul></li><li><p>产品架构形态：dws数据库主要有如下部署方式：线下物理机部署ESL和云化部署HCS</p><ul><li>线下部署：<ul><li>DWS的ESL版本使用fusioninsight manager管理平台提供集群状态监控，告警管理，监控采集等功能。集群安装完成后，登录管理平台即可查看集群的状态监控指标.</li><li>数据库级别的监控指标：服务对于CPU、内存，物理读写与IO等资源的消耗趋势，反应了数据库的业务压力，需要指出的是这里的内存使用大小指的时候各个数据库实例在监控时间点消耗的内存总量。</li><li>节点级别的监控指标：提供主机级别的CPU&#x2F;内存以及磁盘使用情况的趋势图。</li><li>登录节点执行运维命令均需要在“omm”用户下执行，并且需要source环境变量。</li></ul></li><li>云化部署：<ul><li>DWS的HCS形态完成集群创建后，即可在集群管理页面看到创建的集群信息，选择集群操作选项中的监控面板功能，查看监控信息。</li><li>节点级别监控：CPU&#x2F;IO&#x2F;磁盘使用率，内存，网络等；</li><li>集群概览：集群状态，整体资源消耗利率，实例状态</li><li>实时查询：活跃会话数，活跃应用数，活跃查询数。</li></ul></li></ul></li></ol><h3 id="巡检工具及运维工具"><a href="#巡检工具及运维工具" class="headerlink" title="巡检工具及运维工具"></a>巡检工具及运维工具</h3><ol><li>巡检工具：<ul><li>不同巡检任务：</li><li>日常巡检：<ul><li>使用场景：用于集群日常维护，获取集群的健康状态，发现集群的潜在风险问题。</li><li>TOP巡检项：集群状态，负载均衡状态，CPU使用率，磁盘性能和使用率，日志空间大小，内存泄露，数据倾斜，透明大页，周期性备份等</li><li>使用规范：每单周或双周执行一次</li></ul></li><li>升级前巡检：<ul><li>使用场景：用于集群版本升级前，提前发现可能会影响集群升级的问题。</li><li>TOP巡检项：集群状态，文件系统占用率，磁盘空间空间，防火墙关闭，xid回卷，系统表是否损坏。</li><li>使用规范：升级版本前5天内执行</li></ul></li><li>扩容前巡检：<ul><li>使用场景：用于扩容操作前，提交发现可能会影响集群扩容的风险问题。</li><li>TOP巡检项:集群状态，节点间互信，磁盘使用率，磁盘的inodes使用率，数据倾斜，SCTP模块是否安装等。</li><li>使用规范：扩容操作前5天内执行</li></ul></li><li>温备前巡检（仅HCS形态支持）：<ul><li>使用场景：用于温备操作前，提交发现可能会影响温备操作的风险问题</li><li>TOP巡检项：全部11项</li><li>使用规范：温备操作前5天内执行</li></ul></li><li>深度巡检：<ul><li>使用场景：用于GaussDB集群日常维护，获取集群更深度的健康状态，发现集群的潜在风险，深度巡检必须停止业务后执行。</li><li>TOP巡检项：磁盘使用率，僵尸进程，内存泄露，残留临时表，是否有psort索引，负载均衡状态，分布键顺序，开启analyze，guc参数符合调优条件等</li><li>使用规范:每年执行一次。</li></ul></li></ul></li><li>ESL形态巡检工具：<ul><li>工具介绍：FusionInsight Tool DWS Prober是为工程师提供的一套健康检查工具，能够检查集群相关节点，服务的健康状态，提前发现集群中潜在的问题，并生成健康检查报告。由两部分组成：FusionCare和SysChecker。其中FusionCare提供对租户面节点巡检功能，SysChecker提供对管控面FusionInsight的巡检功能。</li><li>适用场景：适用于集群安装后，对集群的服务状态，节点硬件状态，操作系统配置等进行检查。</li><li>常用巡检功能：日常巡检、升级前巡检、扩容前巡检、深度巡检</li></ul></li><li>HCS形态巡检工具：<ul><li>工具介绍：Inspect插件，是为了技术支持和维护工程师提供的HCS集群界面化健康检查工具，能够检查集群相关节点，服务的健康状态，提前发现集群中的潜在问题，并生成健康检查报告。同时该插件与集群版本解耦，使用前可直接升级到最新版本。</li><li>使用场景：适合工程师在集群日常维护、升级、扩容、温备等操作前快速对集群软件、硬件、配置等进行健康检查。</li><li>日常巡检功能： 日常巡检、升级前巡检、扩容前巡检、温备前巡检、深度巡检。</li></ul></li><li>运维工具：概述：DWS提供运维工具包，针对现网高频问题和应急场景，汇总成工具包，目标是通过工具一键收集定位定界信息，用于加快问题处理和提升运维效率。<ul><li>gs_dbmonitor:<ul><li>功能描述：周期采集集群SQL级的运行状态：DDL探针，活跃语句，SQL排队，作业等待，内存使用，主备同步等信息</li><li>使用场景：实时监控系统运行状态：实时查询集群的SQL作业运行情况，实时查看异常指标，便于进一步分析集群运行情况盘点：汇总历史变化信息并转化为图表形式来分析业务和负载变化趋势，可用于系统健康度评估和业务变化分析</li></ul></li><li>gs_ccnqueue:<ul><li>功能描述：排查ccn排队作业是否有异常，输出结果包括排队的作业数，可用内存数，剩余内存数，正在执行的作业的估算内存大小，执行时间，执行用户，执行的SQL等信息</li><li>使用场景：有大量的作业在waiting in ccn queue, 查看正在运行的作业和排队作业的资源消耗情况</li></ul></li><li>gs_cpuwatcher:<ul><li>功能描述：查找集群中引起cpu高的业务SQL</li><li>使用场景：系统CPU使用率高，使用本工具抓取占用cpu高的业务SQL</li></ul></li><li>gs_memwatcher:<ul><li>功能描述：对集群内的CN&#x2F;DN的实例，进行单实例的内存监控</li><li>使用场景：系统内存&#x2F;动态内存使用率高，使用本工具抓取占用内存高的业务SQL报内存错误memory is temporarily unavailable时排查内存占用情况。监控运行过程中的内存使用率。</li></ul></li><li>gs_iowatcher:<ul><li>功能描述：监控单CN&#x2F;DN上业务SQL的IO使用情况，如果需要监控多个，可以起多个线程。</li><li>使用场景：系统IO使用率高，使用本工具抓取占用IO高的业务SQL</li></ul></li><li>gsar:<ul><li>功能描述：对指定网卡流量，重传，丢包等指标监测，以便快速定位网络问题。</li><li>使用场景：通过报错等问题，定位到可能有网络故障时，使用此工具对网络进行排查。</li></ul></li><li>gs_oscoreconfig,gs_coreanalyze<ul><li>功能描述：单节点配置os core,关闭Bbox core；解析gaussdb产生的core文件，并打印出语句，执行用户等相关信息；展示当前目录下的所有已解析的core文件结果；压缩存放解析结果的core文件夹。</li><li>使用场景：当集群内gaussdb进程产生core文件后，使用该工具分析core文件堆栈，用于分析进程异常退出的原因</li></ul></li><li>gs_diskusedcheck<ul><li>功能描述：对集群内所有磁盘的使用率进行筛选，识别超过阈值的磁盘。对单DN或单磁盘下所有DN进行磁盘使用率检查，覆盖以下场景：专有目录的大小，大文件检测</li><li>使用场景:磁盘告警或集群只读时，可使用此工具进行排查。当磁盘使用率出现倾斜时，可使用此工具进行排查。业务和负载变化，可用于系统健康度评估的参考。</li></ul></li><li>gs_tablescan:<ul><li>功能描述：快速校验集群内表文件是否有损坏或者表查询异常。</li><li>使用场景：发现存在表数据文件损坏后，使用此工具进行全库排查（磁盘故障，全局排查，数据校验）</li></ul></li></ul></li><li>TOPSQL:<ul><li>概述：TopSQL是DWS数据库内置的一款功能十分强大的性能分析工具。在生产环境中，难免会出现一些突发情况，导致查询语句出现异常中断、阻塞时间长等情况，如果当时没能记录下来，那么事后就要投入更多的人力以及时间成本，去对错位进行定位和解决，有时还往往定位不到错误的地方。为了解决这样的窘迫的情况，DWS开发了TopSQL功能，对运行中的语句记录（实时TopSQL），对运行完成的语句进行记录（历史TopSQL）</li><li>实时TopSQL：<ul><li>前提条件：<ul><li>guc参数enable_resource_track为on（默认为on）</li><li>guc参数resource_track_level为query&#x2F;perf或者operator(默认为query)</li><li>监控作业的类型为：优化估算的执行代价大于或等于resource_track_cost取值的作业。</li><li>Cgroups功能正常加载，可通过gs_cgroup -P查看控制组信息。</li><li>GUC参数enable_track_record_subsql控制是否记录存储过程、匿名块内部语句。</li><li>在上述条件中，enable_resource_track为系统级参数，用于设置是否开启资源监控功能。resource_track_level为session级参数，可以对某个session的资源监控级别进行灵活设置。</li></ul></li></ul></li><li>实时TopSQL常用视图：<ul><li>gs_session_cpu_statistics:查询实时CPU信息</li><li>gs_session_memory_statistics:查询实时memory信息</li><li>gs_wlm_session_statistics:查询当前cn的实时资源</li><li>pgxc_wlm_session_statistics：查询所有cn的实时资源</li><li>gs_wlm_operator_statistics：查询当前CN作业算子执行实时资源信息</li><li>pgxc_wlm_operator_statistics：查询所有CN作业算子执行实时资源信息</li><li>pg_session_wlmstat：查询当前用户执行作业正在运行时的负载管理信息。</li><li>pgxc_wlm_workload_records:动态负载功能开启，enable_dynamic_workload 为on时，该视图有效，查询当前用户在每个CN上，作业执行时的状态信息。</li></ul></li><li>历史TopSQL：<ul><li>前提条件：<ul><li>guc参数enable_resource_track为on（默认为on）</li><li>guc参数resource_track_level为query，perf或operator（默认为query）</li><li>guc参数enable_resource_record为on（默认为on）</li><li>guc参数resource_track_duration小于作业执行时间（默认为60s）</li><li>guc参数enable_track_record_subsql控制是否记录存储过程、匿名块内部语句（默认为on）</li><li>guc参数resource_track_subsql_duration小于存储过程中内部语句的执行时间（默认为180s）</li><li>监控作业类型为:资源监控实时视图中，记录的作业结束时的执行时间大于或等于resource_track_duration的作业</li><li>Cgroups功能正常加载，可通过gs_cgroup -P查看控制组的信息。</li></ul></li></ul></li><li>历史TopSQL常用视图：<ul><li>gs_wlm_session_history:查询当前cn最近执行作业结束后的负载记录</li><li>pgxc_wlm_session_history：查询所有cn最近执行作业结束后的负载记录</li><li>gs_wlm_session_info:数据表，查询当前cn作业执行结束后的负载记录。要查到历史记录，必须保证enable_resouce_record为on</li><li>pgxc_get_wlm_session_info_bytime:函数，对视图pgxc_wlm_session_info进行筛选查询，要查到历史记录，必须保证enable_resouce_record为on。在统计数据量很大的场景中，建议使用该函数进行查询。</li><li>gs_wlm_operator_histroy:查询当前cn作业算子最近执行资源信息。要查到记录，必须保证resource_track_level 为operator</li><li>pgxc_wlm_operator_history:查询所有cn作业算子最近执行资源信息。要查到记录，必须保证resouce_track_level为operator</li><li>gs_wlm_operator_info：数据表，查询当前cn作业算子历史执行资源信息。要查到记录，必须保证resource_track_level为operator和enable_resource_record为on</li><li>pgxc_wlm_operator_info:查询所有CN作业算子历史执行资源信息。要查到记录，必须保证resource_track_level为operator和enable_resource_record为on。</li></ul></li></ul></li></ol><h3 id="运维监控"><a href="#运维监控" class="headerlink" title="运维监控"></a>运维监控</h3><ol><li>华为云stack DWS 微服务组件：<ul><li>Controller：整个DWS的后台组件</li><li>Monitor:ECF公共组件，主要功能：集群实例的状态监控，告警&#x2F;事件上报</li><li>Event:ECF公共组件，主要功能：ECF事件&#x2F;告警管理中心，支持向SMN，OC，CTS发送事件和告警。</li><li>ECFAgent:部署在集群节点上的代理，主要功能：接收告警和事件，监控集群状态</li><li>DMSAgent:部署在集群节点上的代理，主要功能：采集数据库的资源监控信息和数据库所在节点的系统资源信息。</li></ul></li><li>告警：<ul><li>告警配置：DWS提供告警配置功能，用于提前发现集群潜在问题和故障，告警内容涵盖集群故障、资源过载、性能降级等多种故障场景，建议客户根据业务场景配置合理的告警阈值和规则，建议对紧急告警，进行短信和电话配置，便于及时关注集群告警。</li></ul></li><li>监控：<ul><li>监控上报：集群侧节点上有定时任务采集，数据仓库服务节点监控信息每隔1分钟采集一次，数据仓库整集群监控信息每隔4分钟采集一次，会在目录&#x2F;uploadtocessrc下生成*.json文件，上报成功后会将监控文件存放到uploadtocesbak进行备份，文件备份周期为2天。</li><li>异步的数据XX进程，会将数据给ces服务，OC运维监控平台从ces上获取数据进行处理展示。</li><li>监控主要用于性能问题维护，异步上报监控，监控超过阈值会上报告警，通过监控趋势提前了解集群是否需要扩容，以及潜在的性能问题风险，当前DWS已有的性能监控指标粒度比较粗，无法精确到节点上具体的性能指标监控详情，新开发的DMS功能会有比较细粒度的监控后续的监控功能会以新开发的DMS为主，会进一步完善和丰富DMS的监控功能，DWS在630就已有的性能监控会保持现状，不会有大的需求改动。</li></ul></li></ol><h3 id="业务应急"><a href="#业务应急" class="headerlink" title="业务应急"></a>业务应急</h3><ol><li>常见故障场景和应急手段：<ul><li>整体性能慢：通过应急“三板斧”，快速恢复集群性能</li><li>CPU使用率高：找到CPU占比高的语句，对相关业务进行应急查杀或者资源限制，事后进行SQL优化。</li><li>IO使用率高：找到IO占比高的语句，对相关业务进行应急查杀，事后进行SQL优化</li><li>内存报错：找到内存占比高的语句，对相关业务进行应急查杀或资源限制</li><li>锁冲突报错：找到持锁语句，应急查杀并将锁冲突业务，错峰执行。</li><li>集群只读：找到空间占比高的表或者语句，清理空间。</li></ul></li><li>整体性能慢：<ul><li>概念：数据库系统的性能管理在整个业务系统中起着很重要的作用，集群性能管理不当或在硬件、OS等故障后，容易出现集群性能降级，需要及时介入处理避免长时间对业务造成影响</li><li>问题现象：集群整体出现卡慢，业务常规查询，建表等语句劣化，性能探针出现劣化</li><li>问题影响：集群整体性能降级，造成跑批延迟，影响业务时效性</li><li>处理套路：判断性能瓶颈点（硬件&#x2F;资源&#x2F;内部等待事件等）,针对性能瓶颈点快速恢复。</li></ul></li><li>CPU使用率高：<ul><li>概念：CPU指标表示当前集群计算资源的使用情况，一般建议CPU使用率维持在60%以下，防止在主备切换后出现CPU瓶颈；当CPU使用率超过80%后，不同业务之间会有比较严重的CPU争抢，此时建议通过错峰或扩容等手段降低CPU负载。当出现CPU使用率异常突增时，会导致集群整体性能劣化，需要及时处理。</li><li>问题现象：集群CPU使用率突增，或CPU水位长期维持在80%以上，出现CPU过载</li><li>问题影响：集群整体性能降级，造成跑批延迟，影响业务时效性</li><li>处理套路：找到占用CPU高的语句或用户，针对该语句或用户处理。</li></ul></li><li>IO使用率高：<ul><li>概念：IO指标表示当前集群读写性能，对于机械硬盘应重点关注该指标，当机械盘IO使用率超过90%后，业务可能会有大量wait io出现，频繁IO等待导致集群整体性能降级。</li><li>问题现象：集群IO使用率突增，或IO使用率长期在90%以上，出现IO过载</li><li>问题影响：集群整体性能降级，造成跑批延迟，影响业务时效性</li><li>处理套路：找到占用IO高的语句或用户，同步排查硬件故障情况，针对IO占用高的语句或用户进行处理</li></ul></li><li>内存报错：<ul><li>概念：业务语句在执行过程中，大部分操作都是在动态内存中完成的，当SQL中间结果集过大或当前并发过高时，会导致集群动态可用内存不足，出现memory is temporarily unavailable报错</li><li>问题现象：集群动态内存使用率突增，出现动态内存不足告警，部分业务出现内存不足报错。</li><li>问题影响：部分业务报错</li><li>处理套路：找到占用内存占用高的语句或用户，针对内存占用高的语句或用户进行处理。</li></ul></li><li>锁冲突报错：<ul><li>概念：当对表进行查询&#x2F;DDL&#x2F;DML等任何操作时，数据库会对表进行加锁操作，在事务结束时释放。常规锁按照粒度可以分为8个等级，各个操作对应不同的锁级别，级别不同，阻塞程度不同。当互相冲突的语句执行时，后执行的语句会进入锁等待队列，表现为语句被阻塞。例如，对表进行长查询时，truncate语句会被阻塞，进入锁等待队列，表现为truncate语句执行卡住。</li><li>问题现象：表相关的业务阻塞，执行慢或出现锁等待超时报错</li><li>问题影响：部分业务报错或该表相关的业务被阻塞</li><li>处理套路：找到持锁语句，应急查杀或停用持锁业务。</li></ul></li><li>集群只读：<ul><li>概念：当集群某个磁盘完全写满，达到100%时，此时该盘对应的实例进程无法进行数据写入和xlog日志写入，并且可能导致对应的备机实例也写满，此时集群会出现不可用状态。为了避免集群不可用，当集群磁盘使用率到达90%时，会触发集群只读保护，此时只能进行查询，无法进行写入，这种情况下需要及时清理磁盘空间，将磁盘空间使用率降低到安全水位（建议80%）以下</li><li>问题现象：集群进入只读模式，写入相关的业务出现read only报错</li><li>问题影响：增删改业务报错</li><li>处理套路：找到触发只读的目录，根据对应的目录内容，找到大表或触发语句。</li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> 数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GaussDB(DWS) </tag>
            
            <tag> 数据库 </tag>
            
            <tag> 数据仓库 </tag>
            
            <tag> 数据 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GaussDB(DWS)数据库-性能调优和开发实践篇</title>
      <link href="/2025/08/04/GaussDB(DWS)%E6%95%B0%E6%8D%AE%E5%BA%93-%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98%E5%92%8C%E5%BC%80%E5%8F%91%E5%AE%9E%E8%B7%B5%E7%AF%87/"/>
      <url>/2025/08/04/GaussDB(DWS)%E6%95%B0%E6%8D%AE%E5%BA%93-%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98%E5%92%8C%E5%BC%80%E5%8F%91%E5%AE%9E%E8%B7%B5%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h3 id="性能调优"><a href="#性能调优" class="headerlink" title="性能调优"></a>性能调优</h3><ol><li>定义：通过优化数据库系统的配置及SQL查询，以提高数据库性能和效率的过程。目的是消除性能瓶颈、减少响应时间、提高系统吞吐量和资源利用率，降低业务成本，从而提高系统稳定性，给用户带来更大的价值。</li><li>SQL执行计划解析, 执行计划命令：<ul><li>EXPLAIN VERBOSE + SQL语句(select&#x2F;update&#x2F;insert&#x2F;delete&#x2F;merge into&#x2F;create table as):获取详细执行信息，SQL语句不会真正执行 verbose</li><li>EXPLAIN PERFORMANCE+ SQL语句(select&#x2F;update&#x2F;insert&#x2F;delete&#x2F;merge into&#x2F;create table as):获取详细执行信息。performance</li></ul></li><li>verbose选项下打印详细计划信息中，plan information信息包括：<ul><li>E-rows:算子估算输出行数</li><li>E-distinct:单DN上算子distinct估计值</li><li>E-memory:DN上每个算子估算的内存使用量，只有DN上执行的算子才会显示。某些场景会在估算的内存使用量后面使用括号，表示该算子在内存资源充足下的自动扩展内存上限。</li><li>E-width:每个算子输出元祖的估算宽度</li><li>E-costs:每个算子估算的执行代价</li></ul></li><li>performance选项下打印执行信息中，和verbose相比，新增的相关信息：<ul><li>A-time：算子实际执行时间，在DN上输出由[]括起来的，由逗号分割的两个值，分别表示算子在不同DN上执行的最短时间和最长时间。</li><li>A-row：算子实际输出元祖数，是各个DN上算子输出元祖数总和。</li><li>Peak memory：算子消耗内存峰值，在DN上输出由[]括起来的，由逗号分割的两个值，分别表示算子在不同DN上执行的最小内存消耗和最大内存消耗。</li><li>A-width：算子每行元祖实际宽度，仅涉及重内存使用算子。</li></ul></li><li>performance选项打印执行中的相关信息：<ul><li>plan information:以表格形式显示整个执行过程中每个算子的执行概要信息。</li><li>SQL Diagnostic information:SQL自诊断信息<ul><li>verbose可诊断：统计信息未收集、分区不剪枝、SQL不下推</li><li>performance可诊断：统计信息未收集、分区不剪枝、SQL不下推、HashJoin中大表做内表、大表等值连接使用Nestloop、大表Broadcast、数据倾斜、索引不合理。</li></ul></li><li>predicate information:算子计算信息，如scan的filter条件，join的join条件</li><li>Datanode information:算子在每个DN上执行的详细信息，包括执行时间、CPU、Buffer的使用情况。<ul><li>执行时间（actual time）:如果这个值在各DN上存在较大差异，可初步判断存在计算倾斜（各DN上承担计算量差异过大）</li><li>输出元祖数（rows）:结合执行时间进一步佐证是否存在计算倾斜。</li><li>CPU执行cycle:在算子执行期间，执行所消耗的CPU cycle</li><li>Buffer命中率（hit）:针对Scan算子做数据扫描时。从性能角度看，buffer命中率越高越好，这需要增大集群的shared_buffers(行存)、cstore_buffers(列存)的配置参数。</li></ul></li><li>User Define information:性能profile信息，算子执行过程中的，关键动作性能打桩信息。</li><li>Memory information:算子执行过程中的内存消耗信息。包括内存信息和下盘信息。</li><li>Query Summary:query执行的概要信息。这部分打印总的执行时间和网络流量，包括各DN上初始化和结束阶段的最大最小执行时间、CN上的初始化、执行、结束阶段的时间、以及当前语句执行时系统可用内存、语句估算内存等信息。</li></ul></li><li>性能分析主要关注Plan information&#x2F;SQL diagnostic ioformation&#x2F;predicate information&#x2F;Datanode information这4部分信息。在大集群下，对复杂SQL，建议使用explain analyze 打印概要的实际执行信息，避免打印各个节点信息太多导致执行计划过长</li><li>执行计划类型：3种<ul><li>FQS计划（fast query shipping）<ul><li>CN直接将原语句下发的DN,各个DN单独执行，并将执行结果在CN上汇总。</li></ul></li><li>Stream计划：<ul><li>算子形态：GATHER&#x2F;REDISTRIBUTE&#x2F;BROADCAST</li><li>CN根据原语句先生成执行计划，再将计划下发到DN执行，各DN执行过程中使用Stream算子进行数据交互。</li></ul></li><li>Remote Query计划<ul><li>CN生成执行计划后，将部分原语句下发到DN,各DN单独执行，执行后将结果发送给CN，CN执行剩余计划。</li></ul></li><li>Explain执行计划:<ul><li>ANALYZE | ANALYSE :显示实际运行时间和其他统计数据</li><li>VERBOSE:显示有关计划的额外信息，例如输出列信息。</li><li>COST: 包括每个规划节点的估计总成本，以及估计的行数和每行的宽度。</li><li>CPU: 打印CPU的使用情况信息。</li><li>DETAIL:打印DN上的信息。</li><li>NODES:打印query执行的节点信息</li><li>NUM_NODES:打印执行中的节点个数信息</li><li>BUFFERS:包括缓冲区的使用情况信息</li><li>TIMING:包括实际的启动时间和花费在输出节点上的时间信息。</li><li>PLAN:是否将执行计划存储在plan_table中。</li><li>FORMAT:指定输出格式</li><li>GENERIC:显示将语句中的常数替换为参数后生成的generic计划</li></ul></li></ul></li><li>TopSQL：<ul><li>定义：将SQL的排队信息、运行信息（耗时、CPU、内存、IO、网络、空间）记录到一张系统表中，即作业级监控。</li><li>功能：<ul><li>确定影响数据库性能资源最密集的SQL查询</li><li>监控和跟踪SQL查询，随时间推演的性能变化</li><li>分析查询执行计划，以确定潜在的优化</li></ul></li><li>分类:实时&#x2F;历史, 当前CN&#x2F;全部CN,级别都是query<ul><li>实时当前CN:GS_WLM_SESSION_STATISTCS</li><li>实时全部CN:PGXC_WLM_SESSIOIN_STATISTICS</li><li>历史当前CN:GS_WLM_SESSION_INFO</li><li>历史全部CN:PGXC_WLM_SESSION_INFO</li></ul></li></ul></li><li>历史TopSQL：<ul><li>Topsql主要是通过视图进行承载，按照级别分为query&#x2F;perf&#x2F;operator</li><li>query:SQL语句的计划信息，类似于explain输出信息，记录到Topsql中。</li><li>perf:包含实际执行时间和执行行数的计划信息，类似于explain analyze输出信息，记录到Topsql中。</li><li>operator:不仅会把包含实际执行时间和执行行数的信息记录到TopSql中，还会把算子级别执行信息，记录到Topsql中.</li></ul></li><li>历史TopSQL视图记录了作业运行结束时的资源使用情况、运行状态信息和性能告警信息。<ul><li>分类：级别（query&#x2F;perf+operator）+当前CN&#x2F;全部CN</li><li>query&#x2F;perf级别当前CN: GS_WLM_SESSION_INFO</li><li>query&#x2F;perf级别全部CN: PGXC_WLM_SESSION_INFO</li><li>operator级别当前CN:GS_WLM_OPERATOR_INFO</li><li>operator级别全部CN:PGXC_WLM_OPERATOR_INFO</li></ul></li><li>TopSQL配置GUC参数：<ul><li>实时TopSQL参数：运行中的语句记录<ul><li>enable_resource_track(ON):资源实时监控开启，实时TopSQL总开关，关闭后实时TopSQL不再记录，不会出现在历史TopSQL中。</li><li>resource_track_cost(0):执行代价阈值，对当前会话语句进行资源监控的，最小执行代码。</li><li>resource_track_level(query):资源监控等级，当前会话的资源监控等级，默认为query级别。</li></ul></li><li>历史TopSQL参数：运行完成的语句记录<ul><li>enable_resource_record(on):资源监控记录归档，开启后，执行结束的记录会分布归档到相应INFO视图，CN和DN都需要设置上。</li><li>resource_track_duration(60s):作业运行时间阈值，实时TopSQL中记录的语句执行结束后，进行历史转存的最小执行时间，其判断包含排队时间和运行时间，当排队时间+运行时间&gt;resource_track_duration时，Topsql历史视图会记录作业信息。当执行完成的作业，其执行时间不小于此参数值时，作业信息会从实时视图（statistics为后缀的视图）转存到相应的历史视图。</li><li>topsql_retention_time(30天):历史数据老化周期，历史TopSQL当前CN视图（GS_WLM_SESSION_INFO、GS_WLM_OPERATOR_INFO）中数据保持时间，单位为天。</li></ul></li><li>数据流转过程：作业运行-&gt;运行信息记录实时Topsql-&gt;作业运行结束-&gt;执行信息记录历史Topsql-&gt;结束。</li></ul></li></ol><h3 id="SQL调优"><a href="#SQL调优" class="headerlink" title="SQL调优"></a>SQL调优</h3><ol><li>调优原则：也是唯一原则，资源利用最大化原则.其中资源包括CPU、内存、磁盘IO、网络IO,SQL语句应当尽量高效、节省资源开销，即以最优的执行方式实现功能。SQL语句应当充分利用资源，实现性能极致。</li><li>调优分类和流程：先静态调优，再动态调优<ul><li>静态调优：根据硬件资源和客户的业务特征，确定集群部署方案和表定义。集群部署方案和表定义一旦确定，后续改动的代价会比较大。</li><li>动态调优：根据SQL语句执行的实际情况，采取针对性干预SQL执行计划的方式，提升性能。采取的手段包括：SQL改写，GUC参数干预，Plan Hint</li></ul></li><li>静态调优手段(5种):表定义、存储类型、分布列、局部聚簇、分区表<ul><li>表定义的目的：<ul><li>表数据均匀分布在各个DN，选择合适分布列避免数据分布倾斜，防止单个DN数据过多导致集群有效容量下降。</li><li>表Scan压力均匀分布在各个DN,避免单DN的scan压力过大，形成scan的单节点瓶颈。避免把基表上的等值filter中的列作为分布列。</li></ul></li><li>存储类型：<ul><li>用途：客户业务属性决定表的存储类型；存储类型决定存储格式，进而影响I&#x2F;O操作行为。</li></ul></li><li>分类：<ul><li>行存：适合点查询（返回记录少，基于索引的简单查询），增删改比较多的场景</li><li>列存：统计分析类查询（group, join 多的场景），即席查询（查询条件列不确定，行存无法确定索引）</li></ul></li><li>分布列：<ul><li>分布列选择原则：列值应比较分撒，以便数据能够均匀分布都各个DN上。尽量不要选择存在常量等值过滤条件的列，避免DN剪枝后Scan集中到一个DN上。选择查询中的连接条件为分布列，以便join任务能够下推到DN中执行，而且可以减少DN间的通信数据量，建议选择join-condition或者group by 列为分布列。根据以上原则尽量根据业务特征选择hash分布方式，无法确定时可以选择roundrobin分布：</li></ul></li><li>分布方式：复制（Replication）、哈希（Hash）、轮询（RoundRobin）<ul><li>复制Replication:在集群中的每个DN实例上都有一份全量表数据。存在数据冗余。适用于小表、维表。join操作可减少重分布造成的网络开销。</li><li>哈希（Hash）：数据通过hash方式散列到集群的所有DN实例上。适用于数据量大的表。读写数据可充分利用各个节点IO资源，提升读写速度。</li><li>轮询（RoundRobin）：数据通过轮询方式发放到集群内所有DN实例上。适用于数据量大的表，而且各列都有严重倾斜的表。读写数据可充分利用各个节点IO资源，提升读写速度。</li><li>分布方式分析和调整<ul><li>判断数据是否存在存储倾斜：table_distribution()，不同DN的数据量，相差5%以上即可视为倾斜，相差10%以上，建议调整分布列。</li><li>在线判断数据列是否存在倾斜：table_skewness()，</li><li>调整分布列语法：<code> bash alter table table_name distribution by hash()/replication/roundrobin;</code></li></ul></li></ul></li><li>局部聚簇（Partial Cluster Key，简称PCK）：<ul><li>定义：列存储下的，一种通过min&#x2F;max稀疏索引，实现基表快速扫描的，一种索引技术。</li><li>优化原理：入库时进行局部排序，来换取查询性能</li><li>使用约束:列存表，一个列存表只能创建一个PCK。适用数据类型包括整型、时间类型和字符串类型。对于字符串数据类型，如果当前库的collate不为C，则只对表达式col &#x3D; Const起大加速查询的效果</li><li>使用场景：<br>  -业务特征：大表大批量数据导入，每次导入数据量远大于DN数*6W。<br>  -基表存在大量形如col op Const约束，其中col为列名，const为常量值，op为操作符&#x3D;，&lt;, &gt; ,&lt;&#x3D;, &gt;&#x3D;<br>  -选择选择度比较高的简单表达式的列，建立PCK</li><li>使用方法：在创建表的时候，指定PCK约束。在alter table语法中添加PCK约束（只对后续导入数据生效）</li></ul></li><li>分区表<ul><li>定义：把逻辑上的大表按照某种策略划分为几块物理块进行存储，逻辑上的大表成为分区表，每个物理块为一个分区。</li><li>原理：在查询时，通过分区剪枝技术来尽可能减少底层数据扫描。</li><li>适用场景：数据规模上的大表，业务特征为通过剪枝缩小查询范围。</li><li>优势：改善查询性能、增强可用性、方便维护。</li><li>分区键选择：将数据可以均匀映射到各个分区的列，常见分区键为时间列。</li><li>分类：range分区和list分区</li></ul></li></ul></li><li>执行计划：<ul><li>执行计划三要素：统计信息、优化器、配置参数<ul><li>统计信息（表的数据特征）：包括表的元祖数，字段宽度、null记录比率，distinct值，MCV值（most common value）、hb值（直方图，数据分布概览区间）</li><li>优化器（Cost-Based Optimization,CBO，基于代价的优化）：数据库根据大量的表数据特征，结合代价计算模型，通过代价估算，输出估算后的最优执行计划。其中统计信息是查询优化的核心输入，准确的统计信息可以帮助优化器选择最合适的查询计划。</li><li>配置参数：GUC参数和HINT信息。配置参数直接干预优化器的路径选择。</li><li>统计信息收集：使用analyze语法来收集整个表或者表的若干列统计信息。</li><li>操作：大批量数据导入、更新、删除之后，及时analyze.</li></ul></li><li>执行计划流程：<ul><li>词法&amp;语法解析：按照约定SQL语句规则，输入SQL语句从字符串转化为格式化结构（Stmt）</li><li>语义解析: 格式化结构转化为数据库可识别对象</li><li>查询重写：根据规则，将语义解析的输出，等价转化为执行上更为优化的结构。</li><li>查询优化：根据“查询重写”的输出和数据库内部的统计信息，规划SQL语句的具体执行方式。</li><li>查询执行：根据“查询优化”规划的执行路径，执行SQL查询语句。</li></ul></li></ul></li><li>动态调优：即执行态调优<ul><li>定义：先跑query，判断性能是否满足客户需求，如果不满足，则进一步分析性能瓶颈点，进行针对性优化，重新试跑，一直到满足性能目标为止。</li><li>步骤：<ul><li>判断查询相关表是否已收集统计信息</li><li>判断查询语句是否下推</li><li>收集perfromance信息进行性能分析，并做针对性优化</li><li>SQL改写优化</li></ul></li><li><strong>统计信息收集</strong>：对统计信息前后的执行计划，做对比分析<ul><li>E-rows：在没有收集统计信息的执行计划中，估计值E-row会比实际值小</li><li>执行计划：在没有收集统计信息的执行计划中，出现两个低效的Nest loop算子。</li></ul></li><li><strong>查询语句下推</strong>：<ul><li>执行计划类型：并行计算能力是DWS数据库的性能优势</li><li>优化器在分布式框架下有三种执行规划策略：<ul><li>下推语句计划：CN发送查询语句到DN直接执行，执行结果返回给CN.计划特征：<em>REMOTE_FQS_QUERY</em></li><li>分布式计划：CN先生成计划树，再发送计划树给DN执行，DN执行完成后，执行结果返回给CN.计划特征：Streaming(type: GATHER)</li><li>不下推计划：CN承担大量计算任务，导致性能劣化。优化器先将部分查询（多为基表扫描语句）下推的DN中执行，将中间结果返回给CN,CN再执行执行计划剩下的部分。执行计划特征：REMOTE_XXX + Coordinator quals</li></ul></li></ul></li><li><strong>不下推分析</strong>：<ul><li>常见不下推原因：含有shippable属性且为false的函数语句不下推。</li><li>问题定位手段：<ul><li>Explain performance&#x2F;Explain verbose：对正在执行的SQL，使用explain performance&#x2F;explain verbose,在输出的自诊断信息（SQL Diagostic information）中会提示具体的不下推原因。</li><li>TopSQL：历史执行信息会记录到系统表中，使用postgres库中的查询视图（历史全部DN的perf级）pgxc_wlm_session_info，获取历史SQL的执行信息，此表中的warning字段会记录对应的SQL语句不下推原因。</li></ul></li></ul></li><li><strong>performance分析</strong><ul><li>explain performance优化：收集query执行信息，分析可能的性能问题，针对性优化</li><li>重点关注信息：<ul><li>算子：耗时占整体执行时间高的算子</li><li>执行信息：DataNode information、Memory information、Targetlist information</li></ul></li><li>算子瓶颈和优化策略：<ul><li>Scan性能瓶颈：基表扫描元组数过多场景：增加索引，使用PCK， 使用分区</li></ul></li><li>性能提升策略：<ul><li>减少实际IO：针对点查询场景：增加索引，使用PCK(列存表)；针对范围查询场景：使用分区（优化IO）</li><li>数据在各个DN分布不均衡场景：调整分布列方式。把Scan压力分散到各个DN上：是指数据倾斜，IO压力分布不均衡，performance信息中各DN扫描时间存在明显差异，优化策略是修改分布列</li></ul></li></ul></li></ul></li><li>Join性能瓶颈：<ul><li>join方式选择不当场景：使用plan hint，增加索引</li><li>join内外表选择不当场景：使用plan hint，改写SQL</li><li>join类型：<ul><li>定义：表链接join，根据特定规则从两个其他表（真实表或者生成表）中派生出的结果集。</li><li>类型：在语法层，内连接（inner join）、外连接（outer join）、交叉连接（笛卡尔积， cross join）。inner是缺省的，left、right、full都是外连接，连接条件在on或using子句中指定。在内置实现支持：半连接（Semi join，in约束转化生成， 匹配上即命中）、反半连接（Anti Join, NOT IN约束转化生成，匹配上即排除）</li></ul></li><li>join性能提升策略：<ul><li>选择高效的join方式：通常情况下，hashjoin较为高效。改写SQL实现hashjoin,可以尝试将不等值join条件转化等值join条件。在部分特定场景下，nestloop+indexScan性能更好</li><li>选择合适的内外表：hashjoin：内表小，外表大，执行更高效。或者使用plan hint 调整内外表顺序。</li></ul></li></ul></li></ol><h3 id="plan-hint"><a href="#plan-hint" class="headerlink" title="plan hint"></a>plan hint</h3><ol><li>定义：可直接影响执行计划生成的手段，目的是通过对执行计划的调优，提升查询性能。</li><li>常见hint调优手段：<ul><li>指定scan方法</li><li>指定join方法、join顺序和join时的stream策略</li><li>指定估算行数</li><li>指定重分布过程中的倾斜信息</li><li>配置参数的hint</li></ul></li><li>使用要求：<ul><li>plan hint仅支持在select关键字后面通过如下形式指定。</li><li>可以同时指定多个hint，不同hint信息使用空格分隔。</li><li>配置参数之外的hint 只能hint当前层的计划，对于子查询计划中的hint，需要在子查询的select关键字后面指定hint信息。</li><li>在视图定义时指定hint， 该视图每次被调用时都会使用该hint信息。</li><li>在视图定义时指定hint， 该视图每次被调用时都会使用该hint信息。表只能用单个字符串表示，不能带schema；表如果存在别名，需要优先使用别名来表示该表。</li></ul></li><li>语法格式：<ul><li>在视图定义时指定hint， 该视图每次被调用时都会使用该hint信息。指定scan方法：[no] tablescan|indexscan|indexonlyscan(table [index]): no表示hint的scan方式不使用。table表示hint指定的表，只能指定一个表，如果表存在别名，应该优先使用别名进行hint。index表示使用indexscan或者indexonlyscan的hint时，指定的索引名称，当前只能指定一个。</li><li>在视图定义时指定hint， 该视图每次被调用时都会使用该hint信息。指定join方法：[no] nestloop|hashjoin|mergejoin(table_list):no表示hint的join方式不使用。table_list是hint表集合的字符串，中间不允许出现括号指定join的优先级。<ul><li>仅指定join顺序，不指定内外表顺序：leading(join_table_list)</li><li>同时指定join顺序和内外表顺序，内外表顺序仅在最外层生效：leading((join_table_list))</li></ul></li><li>在视图定义时指定hint， 该视图每次被调用时都会使用该hint信息。指定估算行数：rows(table_list #|+|-|* const):<ul><li>支持#，+，-，*四种操作符，#表示直接使用后面的行数，替换优化器中的估算行数，+、-、*表示对原来估算的行数进行加减乘操作。</li><li>运算后的行数最小值为1行。table_list为hint对应的单表或者多表join结果集，与join的hint中的table_list相同。</li><li>const可以是任意非负数，支持科学计数法</li><li>支持绝对值和相对值的hint，常用于多表的join时，中间结果集估算表不准的场景。</li></ul></li><li>在视图定义时指定hint， 该视图每次被调用时都会使用该hint信息。join顺序和join时的stream策略：[no] broadcast|redistribute(table_list):no表示hint的stream方式不使用。table为进行stream操作的单表或者多表join结果集。<ul><li>stream hint 和join hint配合使用，先hint明确join顺序，然后hint明确中间结果集的数据流动方式。</li></ul></li></ul></li></ol><h3 id="SQL改写"><a href="#SQL改写" class="headerlink" title="SQL改写"></a>SQL改写</h3><ol><li>相关子链接改写：<ul><li>场景：子查询和子链接性能较差。大部分场景，可提升为join进行优化；小部分场景，需要用户改写SQL进行优化。</li><li>改写策略：在语义等价前提下，将子链接和子查询的查询语句，提升到外层查询进行关联查询。</li></ul></li><li>join条件改写：等值join条件的join列增加非空过滤条件<ul><li>场景：等值join，而且join列存在大量的null</li><li>优化原理：null值和任何值比较的结果都是null，而且通过给关联列添加is not null，降低基表扫描输出的数据量，从而减少参与join运算的数据量</li></ul></li><li>not in改写：not in 转为not exists<ul><li>场景：子链接输出列上不存在null值，或者逻辑判断语义上不需要比较null值。</li><li>优化原理：只输出where条件为true的结果。null 和任何值的比较操作都是null。null和bool类型的逻辑运算。</li></ul></li></ol><p>基于公开来源信息, 学习资源来自<a href="https://connect.huaweicloud.com/courses/learn/Learning/sp:cloudEdu_?courseNo=course-v1:HuaweiX+CBUCNXBC006+Self-paced&courseType=1">华为云GaussDB(DWS)数据库官网</a></p>]]></content>
      
      
      <categories>
          
          <category> 数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GaussDB(DWS) </tag>
            
            <tag> 数据库 </tag>
            
            <tag> 数据仓库 </tag>
            
            <tag> 数据 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GaussDB(DWS)数据库-湖仓一体篇</title>
      <link href="/2025/08/04/GaussDB(DWS)%E6%95%B0%E6%8D%AE%E5%BA%93-%E6%B9%96%E4%BB%93%E4%B8%80%E4%BD%93/"/>
      <url>/2025/08/04/GaussDB(DWS)%E6%95%B0%E6%8D%AE%E5%BA%93-%E6%B9%96%E4%BB%93%E4%B8%80%E4%BD%93/</url>
      
        <content type="html"><![CDATA[<hr><h2 id="湖仓一体"><a href="#湖仓一体" class="headerlink" title="湖仓一体"></a>湖仓一体</h2><p>湖仓一体：lake house,其出发点是通过数据仓库和数据湖的打通和融合，让数据流动起来，减少重复建设。<br>lake house架构最重要的一点，是数据仓库和数据湖的数据&#x2F;元数据无缝打通和自由流动。湖里的“显性价值”数据可以流到仓里，甚至可以直接被数据仓库使用；而仓里是“隐性价值”数据，也可以流到湖里，低成本长久保存，供未来的数据挖掘使用。</p><h2 id="湖仓介绍"><a href="#湖仓介绍" class="headerlink" title="湖仓介绍"></a>湖仓介绍</h2><h3 id="数据湖"><a href="#数据湖" class="headerlink" title="数据湖"></a>数据湖</h3><ol><li>数据湖理解<ul><li>传统用户：以hadoop集群为主，满足支持所有结构化、半结构化、无结构化的数据存储即为数据湖</li><li>云厂商：基于对象存储，以S3、OSS、OBS等构建数据底座，进行统一存储。</li><li>大数据互联网：以数据湖三剑客为主（Iceberg、Hudi、Delta   · lake）。它们可以支持比Hive更高层的Upsert、Delete、事务操作等高级特性，能基于Hive进行升级，解决准实时性的问题。</li></ul></li><li>数据湖优势：<ul><li>更好的Table format: 通过支持ACID事务，支持Schema evolution, 能够为用户提供更好的表格式。</li><li>更好的File format: 数据湖在文件格式上支持越来越多的半结构化map、Struct、Json等，并且支持越来越多的索引，进而使文件的查询和存储效率更高，并且在基于列存存储的基础上，支持更多的复杂嵌套结构。</li><li>更低的存储成本、更高的可靠性：使用对象存储，相比于本地磁盘存储、SSD存储或者云盘存储等，可以大幅降低存储成本，并且通过编码的方式能够在降低副本数据量的同时，又能保证高可靠性，可以使用户不用担心底层数据的丢失，从而获得低成本的存储。</li><li>统一的Catalog: 通过统一的catalog，实现统一的元数据管理、权限管理、统计信息管理、入湖管理等。</li></ul></li><li>湖仓融合的价值：<ul><li> 数仓加速：基于数据湖的远程IO成本很高，而且缺少一系列数仓加速手段。早期的数据湖格式多样而且不成熟，索引支持不完善，查询性能有待提升。数据湖主要针对吞吐量的优化，关注低成本和高可靠，不适用于高性能的需求。</li><li> 实时分析：传统的数据湖实时性不够，在Iceberg或者hudi的支持下可能解决分钟级别的时效性，无法解决秒级时效性的问题。</li><li> 高并发查询：对于高并发查询，不管是点查询还是聚合类查询，数仓更加擅长。比如分桶的处理，更精细的裁剪，降低扫描的数据量，提升点查询的效率。另一方面通过物化视图或者cube等相关的预聚合手段，可以提升聚合查询的性能。</li><li> 更完善数据治理：湖仓融合的数据底座，统一主数据和元数据，基于此才有可能做上层统一的数据治理</li><li> 降本增效：简化技术架构、增强整体架构可靠性，降低运维成本。</li></ul></li><li>支持数据格式：<ul><li>文本类型：支持text、CSV，高性能导入导出，支持指定分隔符（delimiter）、换行符（eol）、编码（encoding）,以及多种容错方式处理、错误表等</li><li>列存存储格式：高性能列存存储格式，用于大数据环境中高效存储和查询数据，支持多种压缩算法、编码方式，并且兼容多种引擎。</li><li>Parquet&#x2F;ORC：融合查询，复杂类型查询，支持多种压缩算法，支持多种方式写出</li><li>湖格式：hudi是一个功能丰富的存储管理平台，支持构建具有增量数据管道的流式数据湖，针对处理引擎和常规批处理，进行优化；针对数据探索、BI场景的交互式分析能力，进行优化。支持COW、MOR的导入查询，以及增量同步导入。</li></ul></li></ol><h3 id="湖格式（hudi）"><a href="#湖格式（hudi）" class="headerlink" title="湖格式（hudi）"></a>湖格式（hudi）</h3><p>hudi是一个功能丰富的存储管理平台，支持构建具有增量数据管道的流式数据湖，针对处理引擎和常规批处理，进行优化；针对数据探索、BI场景的交互式分析能力，进行优化。<br>关键能力：变更数据、实时性、数据事务、并发性、多版本能力、存储优化、表结构变更、数据管理、生态兼容。</p><ol><li>存储结构：<ul><li>Metadata:以timeline时间线的形式维护对hudi表的各项操作。</li><li>Data:使用两种存储格式存储数据</li><li>ndex:在数据更新时提供更快的老记录查询性能。</li></ul></li><li>表类型：COW和MOR<ul><li>COW(copy on write)：写入操作时进行复制，每次写入操作都会创建新的cow表，并将原表覆盖。COW表的主要优点是可以减少内存占用和提高写入性能，适合频繁进行写入操作的场景，列如批量更新、数据批量插入等。<ul><li>优点:减少内存占用：每次操作都会创建新的cow表，而不是修改原表，可以减少内存占用，提高性能。提高可扩展性，写优化的行存格式：默认为Avro格式， 空间占用较小。</li><li>缺点：需要内存管理：内存中管理原表和cow表之间的关系，因此需要额外的内存管理能力，需要进行内存管理和回收。数据写入性能较差，写优化的行存格式：默认为Avro格式。</li></ul></li><li>MOR(merge on read):读时合并，数据在写入的时候，为了尽可能保证写入速度，不同步做数据的合并操作（可以看做是异步合并），而是以append的方式，将数据写入到avro格式的日志文件中，在我们读取数据时，再启动合并策略。<ul><li>优点：写入性能高：适用于需要高性能写入的场景，如实时数据分析、流式数据处理等。在写入新数据时会将数据写入临时文件，后通过Compaction过程将临时文件合并到基础数据文件中，更新数据文件并删除旧版本。提高可扩展性，写优化的行存格式：默认为Avro格式。</li><li>缺点：资源消耗：需要定期合并整理compact，否则碎片文件较多，数据写入性能较差。读取性能较差：需要将delta log和老数据文件合并，占用空间相对较大</li></ul></li></ul></li><li>外表查询：<ul><li>hudi外表查询：支持hudi两种表类型：COW(性能优化)、MOR(性能较差)；支持hudi两种查询视图：snapshot、incremental</li><li>增量查询：针对hudi增量查询功能，可以通过设置增量查询参数，实现增量查询。</li><li>增量设置的增量参数：通过查询视图来查看已经设置哪些参数，检查是否设置正确：select * from pg_show_custom_settings();</li><li>查询hudi外表属性：读取OBS上hudi数据的hoodie.properties</li><li>查询hudi外表最大时间线：读取OBS上hudi数据最大时间线，也就是最新的提交记录。</li></ul></li><li>自动同步任务：<ul><li>自动同步：<ul><li>单表同步任务，实现外表到内表的数据合并，记录增量同步进度。（列映射，hudi增量commit time同步点）</li><li>智能调度框架，实现定时调用存储过程任务，并进行资源管控调度，提供任务启停、告警等运维能力。</li></ul></li><li>同步任务流程：创建dws内表-&gt;创建dws外表-&gt;设置同步进度-&gt;提交hudi同步任务。</li><li>设置同步进度：select set_hudi_sync_state()</li><li>提交同步任务：select hudi_sync_task_submit()</li><li>查询同步状态：select * from hudi_show_sync_state()</li></ul></li></ol><h3 id="元数据服务"><a href="#元数据服务" class="headerlink" title="元数据服务"></a>元数据服务</h3><ol><li>元数据打通：</li></ol><ul><li>从湖仓两层架构到湖仓一体，统一元数据共享数据；统一元数据，简化数据共享。<ul><li>湖仓两层架构：存算分离，底层数据文件可对上层服务共享。湖和仓的元数据隔离，共享数据仍需要ETL</li><li>湖仓一体（data lakehouse）:在存算分离的基础上，构建统一的元数据层。上层服务通过统一的元数据层，便捷高效地共享数据。</li></ul></li><li>HiveMetaStore:<ul><li>定义：Apache Hive的一个关键组件，一个元数据存储库，用于管理Hive&#x2F;spark表的元数据信息。HiveMetaStore存储了hive表的结构信息，包括表名、列名、数据类型、分区信息和表的位置信息等。HiveMetaStore的主要作用是提供元数据服务，使得hive&#x2F;spark可以对数据进行查询和分析。它还提供了一些API，可以让开发人员通过编程方式访问表的元数据。</li><li>总之，HiveMetaStore是Hive的一个重要组件，它提供了元数据管理和查询服务。</li></ul></li><li>External schema:<ul><li>定义：External schema即外部模式，dws通过创建extrenal schema来对接hivemetastore服务，每次查询主动获取hive&#x2F;spark表对象的元数据，无需dws内核通过create foreign table获取hive&#x2F;spark表的元数据。</li><li>external schema和schema的区别：<ul><li>external schema主要用于和hivemetastore建立连接，获取表对象元数据，在创建external schema时需要指定连接的所需要的各个属性值。</li><li>普通schema在创建后会将schema的信息记录到pg_namespace中，external schema创建后和普通schema一样也会记录在pg_namespace，可以通过pg_namespace中的nsptype字段区分，是external schema还是普通schema。除了存储在pg_namespace中的相关信息外，external schema连接相关的配置信息，都会记录在pg_external_namespace中。</li><li>external schema下不支持创建表对象。对象的创建是在hive或者spark中创建的。external schema 仅用于执行DML操作。</li></ul></li></ul></li></ul><ol start="2"><li>元数据访问：</li></ol><ul><li>创建Server, external schema, sql query查询</li><li>语法解析：语法解析层主要负责解析。当读取到ex.tbl表以后，连接HMS进行元数据查询。</li><li>元数据查询：从HMS中查询元数据信息，该步骤在步骤1中完成。从HMS中读取数据，主要包括列信息、分区信息、分区键信息、分隔符信息等。</li><li>数据查询（针对select）：从DFS存储中获取统计信息文件个数和文件大小，为plan生成提供依据。</li><li>查询重写、查询优化、查询执行。</li><li>查询下发：将元数据随plan下发到DN，DN收到plan以后，会将元数据进行解码后插入到syscache中</li><li>查询执行：DN访问obs对应文件，执行查询。</li></ul><p>基于公开来源信息, 学习资源来自<a href="https://connect.huaweicloud.com/courses/learn/Learning/sp:cloudEdu_?courseNo=course-v1:HuaweiX+CBUCNXBC006+Self-paced&courseType=1">华为云GaussDB(DWS)数据库官网</a></p>]]></content>
      
      
      <categories>
          
          <category> 数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GaussDB(DWS) </tag>
            
            <tag> 数据库 </tag>
            
            <tag> 数据仓库 </tag>
            
            <tag> 数据 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GaussDB(DWS)数据库-集群管理篇</title>
      <link href="/2025/08/04/GaussDB(DWS)%E6%95%B0%E6%8D%AE%E5%BA%93-%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86%E7%AF%87/"/>
      <url>/2025/08/04/GaussDB(DWS)%E6%95%B0%E6%8D%AE%E5%BA%93-%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h3 id="集群创建与删除"><a href="#集群创建与删除" class="headerlink" title="集群创建与删除"></a>集群创建与删除</h3><ol><li>集群选型<ul><li>产品类型：云数仓、标准数仓、IoT数仓、实时数仓<ul><li>云数仓： 高性价比，支持冷热数据分析，存储、计算弹性伸缩，并按需、按量计价。适用于“库、仓、市、湖”一体化的融合分析业务，适合50节点以内的中小型数据仓库。</li><li>标准数仓：高性能、高扩展、高可用、易运维的企业级数据仓库，支持2048节点、20PB级超大规模数据分析能力。适用于大型企业数仓，上云后体验不变。</li><li>IoT数仓：在标准数仓基础上，提供高效的时序计算和IoT分析能力，支持实时和历史数据关联。适用于物联网、IoT等实时分析场景。</li><li>实时数仓：在大规模数据查询和分析能力基础上，提供高并发、高性能、低时延的事务处理能力。适用于HTAP混合负载场景，“一库两用，生产即分析”，支持单机部署和集群部署两种部署方式。</li></ul></li><li>选择好产品类型后，用户可以根据数据量、业务负载以及性能需求，选择能够支撑业务应用的集群规格和节点数量，CPU数和内存越大，数量越多，存储与计算能力越强。</li><li>刚开始使用DWS服务时，用户也可以先创建一个规格较小的集群，今后随着数据量和业务负载的变化，再自由扩展而不中断业务。</li></ul></li><li>增删CN:<ul><li>当用户的集群创建后，实际需要的CN数量会随着业务需求而发生变化，因此管理CN节点功能的实现，使用户可以根据实际需求动态调整集群的CN数量</li><li>集群创建时默认的CN数量为3，用户可以根据实际发放节点数量，调整CN数量，范围为2~20</li></ul></li><li>删除集群：<ul><li>当用户不再需要使用某个集群时，可以删除该集群。删除的集群无法恢复，同时集群中的用户数据、自动快照也会自动删除，而且无法再访问。删除集群时不会删除手动快照。</li><li>如果集群绑定了弹性IP，建议用户勾选“释放与集群绑定的弹性IP”,将待删除集群的弹性IP资源释放。如果选择不释放，弹性IP将保留，用户可以将该弹性IP绑定到其他DWS集群或者云资源上使用，该弹性IP将仍然按照虚拟私有云（VPC）服务的弹性公网IP计费规则进行计费。</li></ul></li></ol><h3 id="集群监控管理"><a href="#集群监控管理" class="headerlink" title="集群监控管理"></a>集群监控管理</h3><ol><li>节点监控：提供针对当前DWS中所有节点的资源使用情况明细<ul><li>概览：包含节点资源一览图<ul><li>内容：cpu资源使用率，内存资源使用率，数据盘的平均资源使用率，磁盘IO，TCP协议重传率，网络IO</li></ul></li><li>磁盘分页：提供细粒度的磁盘使用情况，说明节点各个磁盘及对应功能（数据盘，日志盘，系统盘）<ul><li>内容：磁盘容量，读速率，写速率，磁盘IO等待时间，磁盘IO服务时间，磁盘IOPS指标。</li></ul></li><li>网络分页：提供节点网卡的详细信息<ul><li>内容：展示名称，网卡状态，接收丢包数，接收速率，发送速率</li></ul></li><li>节点所有页面都可以通过“监控”列查看过去1小时，3小时，12小时，24小时，7天，15天的各个指标变化。</li></ul></li><li>性能监控：提供集群，数据库，节点三个维度相关指标过去一个月的变化趋势，并提供了监控面板供指标集中展示。<ul><li>集群维度的监控指标，包括CPU，内存，磁盘IO，网络IO，集群状态，集群中异常CN数量，SQL堆积数量。</li><li>数据库维度提供了活跃会话数，插入行数，删除行数，修改行数。</li><li>节点维度提供了所有节点相关资源使用情况，并支持对比不同节点的变化趋势。</li><li>集群监控支持查看时间： 过去1小时、3小时、12小时、24小时，7天查看时间，支持自定义时间范围。</li><li>数据库监控支持查看时间：过去1小时，3小时，12小时，24小时。</li></ul></li><li>实时查询：提供了当前系统中的会话和SQL执行情况，并提供了针对会话和查询的查杀功能。<ul><li>实时会话支持查看会话的执行时间，对应的应用名称，接入CN，锁持有状态，锁定对象可以用来排查系统中的长会话或者锁争抢问题。</li><li>实时查询提供了细粒度的查询相关资源使用情况，比如CPU，内存，IO，资源池，查询的排队状态等</li><li>会话和查询都支持根据某一条件查杀问题会话或查询。</li><li>会话：<ul><li>实时会话可以根据多条件过滤查看，当前系统中存在的锁持有的会话，当出现锁争抢是可以根据锁定对象，快速排查相关的执行SQL</li><li>实时会话主要是根据pg_stat_activity和pg_locks信息汇聚上报获取。实时会话默认是启用状态。</li></ul></li><li>查询：<ul><li>实时查询当前仅支持8.1.2以上集群使用，默认不打开，打开需要配置guc参数：enable_resource_track 需要配置为on，resource_track_cost需要根据需求配置，如果配置为0，对所有语句进行监控。</li><li>实时查询主要的是根据pg_session_wlmstat和gs_wlm_session_statistics信息汇聚上报获取。</li></ul></li></ul></li><li>资源池监控：主要反应集群资源池信息，包括CPU使用率，磁盘使用率，内存，语句并发等，可以实时反应集群资源池运行情况。</li><li>SQL诊断：<ul><li>针对已经执行完成的查询中，存在告警信息的SQL进行集中展示。</li><li>SQL诊断对于异常的查询会同步提供SQL语句和执行计划，还有语句的资源使用情况。</li><li>SQL诊断依赖历史查询功能，默认不打开，打开需要配置GUC参数。</li><li>历史查询功能主要是根据pgxc_wlm_session_info信息采集上报获取。</li></ul></li><li>SQL探针<ul><li>SQL探针工具，支持一键执行和定时执行两种探针任务等功能，并可以针对超时的探针SQL提供告警上报信息。</li></ul></li><li>表诊断：提供了集群中数据表，关键运行状态的统计数据和诊断工具。包括：<ul><li>表倾斜率：对于集群中数据表统计信息进行监控分析，展示倾斜率高于5%并且表大小TOP 50的表信息。<ul><li>造成表倾斜率高的原因为不合理的分布列选择，将引发算子计算&#x2F;数据下盘倾斜严重，导致不同的DN的处理压力不同，影响业务性能，并容易造成单DN磁盘使用率过高。</li><li>用户可以通过查询表倾斜率，根据表的大小和倾斜率，对倾斜严重的表重新选择分布列，其中8.1.0及以上集群版本，可以直接通过alter table 语法进行调整表。</li></ul></li><li>表脏页率：对于集群中数据表统计信息进行监控分析，展示脏页率高于50%并且表大小TOP 50的表信息。<ul><li>对于数据表的DML操作将影响数据表数据，导致存在无用的脏数据，过多的脏数据将占据磁盘空间，影响集群可用容量。用户可以通过查询表的脏页率，根据表的大小和脏页率，对较大表和脏页率过高的表进行处理。</li><li>对于脏页率高的表，可以通过手动执行vacuum full操作回收表空间，或者通过“智能运维”操作时执行vacuum full操作。</li></ul></li><li>DDL审核：DDL审核是SQL审核范畴，为了避免不合理的DDL设计影响实际业务运行，该工具会对DDL元数据进行规范性检测，方便用户对潜在的表定义问题提前感知，其结果也可作为性能问题定位的参考数据之一。<ul><li>DDL审核对于审核不通过的数据表，可以通过详情页面查看问题。</li></ul></li></ul></li><li>告警管理<ul><li>包含:查看告警规则，告警规则配置，告警信息订阅功能</li><li>告警规则可以提供过去一周的告警信息统计和告警信息明细，方便用户自行查看租户下的告警。该特性除了以默认值的形式提供一套DWS告警最近实践外，还允许用户根据自己的业务特点，个性化修改告警阈值。</li><li>告警管理通过消息通知服务（Simple Message Notification，SMN）发送DWS告警通知，用户可订阅告警启用通知。</li><li>告警规则配置：<ul><li>当前支持11种告警规则的配置。告警支持启停，只对部分集群生效。当前告警规则是诊断整个租户的告警规则生效。资源类有CPU,磁盘innode,磁盘使用率，磁盘I&#x2F;O时延。业务类有语句堆积告警，语句下盘量过大告警，vacuum full 执行过长告警，资源池队列阻塞告警。工具类有SQL探针执行耗时超阈值。</li></ul></li><li>dws监控系统使用典型场景：<ul><li>磁盘使用率高：配置节点数据盘使用率告警，根据实际需要配置阈值，如果发现该告警上报，则进一步排查系统的磁盘使用情况。</li><li>磁盘只读问题：当发现数据盘使用率高，先查看“工具&#x2F;表诊断&#x2F;表脏页率”，对于脏页率较高的表数据，可以通过vacuum full做清理。如果清理之后可以解决，则可以通过配置智能运维，定位执行vacuum full。如果清理之后磁盘使用率还是较高，则需要重新评估当前系统的规格是否符合业务要求。</li></ul></li></ul></li></ol><h3 id="集群备份恢复管理"><a href="#集群备份恢复管理" class="headerlink" title="集群备份恢复管理"></a>集群备份恢复管理</h3><ol><li>快照：<ul><li>定义：快照是对DWS集群在某一时间点的一次全量数据和增量数据的备份，记录了当前数据库的数据以及集群的相关信息，其中包括节点数量、节点规格和管理员用户名称等。快照创建方式包括手动创建快照和自动创建快照。</li><li>备份：创建快照会将生成的备份文件存储到指定的备份介质中。华为云支持备份介质包括OBS(Object Storage Service)、SFS(Scalable File Service)</li><li>DWS提供免费的快照存储空间，免费容量等于集群储存空间，当快照数据存储空间超过免费空间大小时，超过部分按照OBS的计费规则进行计费。</li><li>手动快照要点：<ul><li>手动快照支持集群级全量快照和schema级快照</li><li>快照级别支持“cluster”和“schema”。schema级别快照依赖版本细粒度备份的特性开关。</li><li>集群名称可选择一个指定集群，只能选择可用状态的集群。</li><li>手动快照创建成功后会一直保存，直到从控制台中将其删除（即使集群删除，手动快照也不会删除。）</li></ul></li><li>自动快照：<ul><li>集群级自动快照采用差异增量备份，第一次创建自动快照为全量备份，以后每间隔一段时间就会做一次全量备份，全量备份作为基础版本。两次全量备份之间都是做增量备份，增量备份记录基于前一次备份所发现的更改。</li><li>集群创建时，自动快照默认处于启用状态。自动增量快照默认为每8小时一次，全量快照每周日执行一次，快照保留期可设置为1-31天，默认为3天。用户也可以根据自身需求设置自动快照策略。</li><li>在策略列表中自动快照开关默认为开启状态，保留天数默认为3天。</li><li>关闭自动快照后，自动删除历史自动快照。</li><li>快照类型设置为全量快照时，快照策略可选一次性和周期性</li><li>快照策略时间需要设置为UTC，同时需要考虑业务所在时区的时差</li></ul></li><li>快照恢复：<ul><li>使用增量快照恢复时，DWS会将最近一次的全量备份到本次快照之间的所有快照，一起用于恢复集群。</li><li>恢复快照到新集群时，恢复时长是由快照备份的数据量所决定的。</li><li>恢复快照时，参数支持重新定义。其他默认参数默认与快照中的备份信息保持一致。</li><li>规格确认无误后，便可执行恢复。待新集群状态变为“可用”，表示快照已恢复成功。</li><li>集群级恢复支持恢复到当前集群。</li><li>细粒度表级恢复：支持恢复单表和多表操作，依赖细粒度备份和细粒度恢复的特性白名单</li></ul></li></ul></li></ol><h3 id="集群容灾管理"><a href="#集群容灾管理" class="headerlink" title="集群容灾管理"></a>集群容灾管理</h3><ol><li>概述：广义上，容灾是一个系统工程，包括所有与业务连续性相关的内容。对于IT而言，容灾是提供一个能防止用户业务系统遭受各种灾难影响破坏的计算机系统。狭义的容灾是指建立两套或多套功能相同的IT系统，互相之间可以进行健康状态监视和功能切换，当主要站点因为意外（如火灾、地震、城市供电中断等）停止工作时，整个应用系统可以利用辅助站点快速恢复，并继续工作。</li><li>容灾目的：容灾的主要目的是，当自然或人为的原因，导致生产系统发生灾难时，能够尽可能地保证业务的连续性。</li><li>容灾和备份的区别 ：<ul><li>容灾主要针对火灾、地震等重大自然灾害，因此生产站点和容灾站点之间，必须保证一定的安全距离；备份主要针对人为误操作、病毒感染、逻辑错误等因素，用于业务系统的数据恢复，数据备份一般是在同一数据中心进行。</li><li>容灾系统不仅保护数据，更重要的目的在于业务的连续性；而数据备份系统只保护不同时间点版本数据的可恢复。一般首次备份为全量备份，所需的备份时间会比较长，而后续增量备份则在较短时间内就可完成。</li><li>容灾的最高等级可实现RPO&#x3D;0;备份可设置一天最多24个不同时间点的自动备份策略，后续可将数据恢复至不同的备份点。</li><li>故障情况下（例如地震、火灾），容灾系统的切换时间可降低至几分钟；而备份系统的恢复时间可能几小时到几十小时。</li></ul></li><li>RPO和RTO:<ul><li>RPO(Recovery Point Objective):即数据恢复点目标，主要指的是业务系统所能容忍的数据丢失量</li><li>RTO(Recovery Time Objective):即恢复时间目标，主要指的是所能容忍的业务停止服务的最长时间，也就是从灾难发生到业务系统恢复服务功能，所需要的最短时间周期。</li></ul></li><li>双集群容灾：<ul><li>容灾原理：<ul><li>生产集群（原集群）周期性做Roach备份，备份文件被同步scp到灾备集群。灾备集群（新集群）周期性做Roach恢复，灾备集群执行恢复操作期间，需要停止集群，其余时间可对外提供查询服务。生产集群故障后，通过switchover&#x2F;failover命令将灾备集群升主，业务由灾备集群接管。</li></ul></li><li>容灾能力：GaussDB内核已具备该能力，DWS管控面已集成该能力</li><li>方案限制：<ul><li>两套集群节点数可以不同，但实例逻辑拓扑要一致，主要指总DN个数需要相同。</li><li>当前支持集群级别的数据同步，也支持细粒度的容灾，但是细粒度容灾受限于白名单使用。</li><li>要求灾备集群具有足够的磁盘空间，存放完整的全量备份集（通常要求是集群数据量的两倍以上）</li></ul></li><li>性能说明：<ul><li>RPO和RTO主要取决于增量数据同步的性能，以及增量同步的周期间隔。对于典型的主、备集群部署场景（物理机、SAS盘、集群内万兆网卡、集群间千兆网卡），在1T&#x2F;天的数据增速下（集群节点数在30个以上时），最小可达RPO 1小时，最小可达RTO 1小时。带宽不是瓶颈的情况下，增量数据同步的性能至少在 30 MB&#x2F;S&#x2F;DN.</li></ul></li></ul></li></ol><h3 id="集群弹性伸缩管理"><a href="#集群弹性伸缩管理" class="headerlink" title="集群弹性伸缩管理"></a>集群弹性伸缩管理</h3><ol><li>节点扩容：<ul><li>集群磁盘容量使用超过70%时进行扩容。</li><li>在扩容配置中，默认配置将不使用在线扩容，并在扩容后自动进行离线重分布。</li><li>如果使用在线扩容，默认将在扩容之后进行在线重分布操作。</li><li>在线扩容以及在线重分布，相比离线模式对业务影响较小。在线重分布期间，用户可以对表执行插入、更新、删除，但重分布过程仍然会短时间阻塞用户的数据更新操作，会影响用户语句的执行性能。</li><li>扩容重分布过程会消耗大量的CPU和IO资源，因此也会对用户作业性能影响较大，用户应该尽可能在停止业务情况下或业务轻载的情况下，执行扩容重分布。</li></ul></li><li>如果用户的集群是EVS盘的云数仓类型，并且只是磁盘空间出现瓶颈，计算资源比较充足，用户可以通过磁盘扩容快速缓解存储资源不足的问题，磁盘扩容过程中无需暂停业务，并且不会造成CPU、内存等资源浪费。</li><li>节点缩容：当用户的计算或者存储资源冗余，超出业务需求时，可在控制台对已有集群进行缩容操作，以便节省成本。集群缩容时只能以安全环为单位减少集群节点的个数，比如用户的集群有9个节点，每3个节点为一个安全环，那么只能选择缩3个节点和6个节点。</li></ol><h3 id="集群资源管理"><a href="#集群资源管理" class="headerlink" title="集群资源管理"></a>集群资源管理</h3><ol><li>为解决用户资源隔离，以实现业务优先级，避免复杂业务阻塞资源，引入了资源池，每个资源池指定可以使用的CPU，内存、磁盘等大小，然后将用户与资源池关联，用户在使用时就只能使用该资源池可以使用的资源，以达到资源限制的目的。</li><li>创建资源池：<br>  用户可以对资源池进行创建、删除、修改（配置，异常规则，关联用户）等操作，队列的参数表示对该资源池的限制，其中CPU有配额和限额两个维度限制。配额：当多个资源池同时在一个CPU上执行时，各自使用时间比例。限额：限定该资源池可以运行的CPU数量。</li><li>创建资源管理计划：<br>  资源管理计划用于自动化的、周期性的对资产池中的资源进行变更，以便实现灵活的资源管理，如果有多个计划，只能生效一个。</li><li>schema空间管控:<br>  模式空间管理，用于对模式空间大小的限额。</li></ol><h3 id="智能运维"><a href="#智能运维" class="headerlink" title="智能运维"></a>智能运维</h3><ol><li>概念:<br>  智能运维是DWS的常驻运维工具，可以帮助用户智能执行运维任务。智能运维会通过集群负载情况，选择合理时间窗、并发度完成用户指定的任务，在运维任务执行过程中，智能运维将时刻关注用户业务的变化，及时调整运维任务执行策略，以减轻对用户业务的影响。智能运维支持周期型和单次型任务的创建，执行时间窗可按照不同用户业务负载定制化。智能运维具备一定的高可用性，在集群异常的情况下，智能运维将重新执行失败的运维任务，若由于集群异常导致运维任务部分步骤无法完成，智能运维将尝试跳过失败的步骤，以节省用户运维时间窗的开销。</li><li>运维计划：<br>  用户可以对运维任务进行创建、删除和修改等操作。调度模式支持自动、指定目标和优先级模式。运维任务可支持单次型和周期型。周期时间窗时间设置为默认为本地时间，请您根据业务所在时区结合时差设置该项。同一天的时间段请不要重叠。</li><li>运维状态：<br>  在运维详情部分切换至运维状态模块。可查看运维任务运行的详细信息，状态包括：Waiting、Running、Finished和Canceled</li></ol><p>基于公开来源信息, 学习资源来自<a href="https://connect.huaweicloud.com/courses/learn/Learning/sp:cloudEdu_?courseNo=course-v1:HuaweiX+CBUCNXBC006+Self-paced&courseType=1">华为云GaussDB(DWS)数据库官网</a></p>]]></content>
      
      
      <categories>
          
          <category> 数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GaussDB(DWS) </tag>
            
            <tag> 数据库 </tag>
            
            <tag> 数据仓库 </tag>
            
            <tag> 数据 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GaussDB(DWS)数据库-导入导出篇</title>
      <link href="/2025/08/03/GaussDB(DWS)%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AF%BC%E5%85%A5%E5%AF%BC%E5%87%BA%E7%AF%87/"/>
      <url>/2025/08/03/GaussDB(DWS)%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AF%BC%E5%85%A5%E5%AF%BC%E5%87%BA%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h2 id="DWS外表"><a href="#DWS外表" class="headerlink" title="DWS外表"></a>DWS外表</h2><p>外表定义(Foregin table)：是对外部数据源的描述，通过使用SQL接口提供访问外部数据的能力。<br>解决问题：实现数据的导入导出，访问其他DWS集群或者其他外部组件等，扩展了DWS对其他组件进行读写的能力。<br>使用原理：利用FDW(foreign data wrapper)机制。首先定义链接信息，之后创建外表，外表的创建是用于定义DWS数据库上对应其他数据源的表结构。<br>外表创建和管理，分为手动创建和自动创建:</p><ol><li>手动创建如下：在普通表基础上，额外添加server和option信息，先建服务，再建外表。<ul><li><p>server是数据库对象，通过create server创建，存储外部数据源访问和认证信息，用于外表如何找到目标数据。</p></li><li><p>语法格式如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">create server server_name </span><br><span class="line">foreign data warpper fdw_name</span><br><span class="line">option();</span><br></pre></td></tr></table></figure></li></ul></li><li>权限控制：默认只有系统管理员有权限，如果其他人使用需要对foreign data wrapper 授权才能创建，授权语法如下： <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grant usage on foreign data wrapper fdw_name to username;</span><br></pre></td></tr></table></figure></li><li>options：数据源定制描述，例如编码、压缩等，不同数据源不同外表差别大。</li><li>系统查询：通过pg_foreign_server和pg_foreign_table系统表查询创建的server和外表。</li><li>自动创建如下：<ul><li>MRS&#x2F;OBS数据源管理+Lakeformation元数据管理；</li><li>MRS&#x2F;OBS数据源管理用于对接HDFS或者OBS,自动建server；</li><li>Lakeformaion元数据管理，自动建外表，使用外部元数据直接进行数据访问。</li></ul></li></ol><h2 id="外表分类"><a href="#外表分类" class="headerlink" title="外表分类"></a>外表分类</h2><p>功能分类：HDFS外表、OBS外表、GDS外表、DLI外表等。<br>server类型分类：</p><ol><li>dfs_fdw&#x2F;hdfs_fdw外表：适应用于外部文件系统（HDFS&#x2F;OBS）上的结构化数据查询。</li><li>dist_fdw外表：适用于文件导入导出。</li><li>gc_fdw外表：适用于协同分析。</li><li>log_fdw外表：适用于dws内部使用，日志查询。</li><li>file_fdw外表：访问服务器文件 。</li></ol><h2 id="GDS-Gauss-Data-Service-工具"><a href="#GDS-Gauss-Data-Service-工具" class="headerlink" title="GDS(Gauss Data Service)工具"></a>GDS(Gauss Data Service)工具</h2><p>定义：DWS提供的数据导入导出工具</p><p>适配场景：</p><ol><li>数据迁移，同构异构集群数据迁移。</li><li>以文本数据作为来源的大数据量表导入</li><li>大数据量表导出<br>  支持导入和导出的文件格式：csv&#x2F;text&#x2F;binary&#x2F;fixed（每行数据等长）</li></ol><p>工具原理：<br>数据导入过程：GDS通过网络和数据库系统相连，CN负责任务规划和下发，GDS负责数据文件切分，然后分发给各个DN，各个DN节点负责数据并行导入，各DN收到数据分片后解析数据，根据表的分布列计算hash值并确定归属哪个DN，如果是自身就缓存到本地，否则就通过网络传给相应的DN.导出过程正好相反。<br>导入要点：</p><ol><li>导入时，GDS数量&lt;&#x3D;DN数量。</li><li>GDS导入时，服务器普通文件系统数据可以导入DWS数据库，HDFS文件系统数据暂时不可以导入DWS数据库。</li></ol><p>导出要点：</p><ol><li>按照导出目的地是否是集群内的主机，分为local模式和remote模式；目的地是集群节点所在主机上为local模式，否则为remote模式。</li><li>导出支持的数据文件格式：csv&#x2F;text&#x2F;fixed，单行数据大小需要&lt;1GB</li><li>在local模式中，数据均匀切割并生成到集群指定文件夹下，需要占用集群磁盘空间。</li><li>在remote模式中，1个GDS同一时刻只为1个集群服务，多个GDS可以并发导出。和集群在同一内网的GDS，导出速度受网络带宽影响。</li></ol><p>GDS导入操作：启动GDS服务&gt;创建外表&gt;执行导入&gt;分析错误表<br>导入操作要点：</p><ol><li>GDS导入数据目录文件过多时，可以使用正则表达式指定外表的location,选择需要的导入文件</li><li>导入过程中的错误，分为数据格式和非数据格式两种错误。数据格式错误是指缺失或多出字段值、数据类型错误或者数据编码错误等。可以通过在创建外表时，设置参数“log into error_table_name”，将导入过程中的数据格式错位信息写入指定的错误信息表中。</li></ol><p>GDS导出操作：启动GDS&gt;创建外表&gt;执行导出<br>导出操作要点：导出的文本命名格式为t1_foreign_output.data.</p><h2 id="实时数仓"><a href="#实时数仓" class="headerlink" title="实时数仓"></a>实时数仓</h2><ol><li>Flink:分布式、流批一体、开源处理引擎。</li><li>用途：在无边界和有边界数据流上进行有状态计算。无边界数据流是指源源不断产生数据，数据流没有结束，类似于kafka消息流。有边界数据流是指有开始和结束的数据流，可以对所有数据做处理。有状态计算是指在流处理过程中，可以将中间状态保留，用于后续数据处理使用。</li><li>Flink内容：组件+任务+API库</li><li>Flink组件：<ul><li>JobManager:负责分配任务、协调执行任务、协助检查点，并处理失败。</li><li>TaskManager:在集群中并行执行任务，管理任务状态和缓冲区。</li><li>Client:提交Flink作业，并与JobManager通讯。</li></ul></li><li>Flink任务：任务由多算子组成，每个算子设置各自的并行度任务<ul><li>source+transformation+sink算子组成。</li><li>source:数据流起点，从外部系统读取数据并转换为Flink可处理的内部数据结构。</li><li>transformation:对数据流进行操作，转换&#x2F;聚合&#x2F;连接&#x2F;分割数据</li><li>sink:数据流终点，将处理后的数据写入外部系统。</li></ul></li><li>Flink的API库：<ul><li>DataStream API:提供转换操作符,用于构建流处理应用核心API。</li><li>Table API&amp;SQL:提供声明式API,用类SQL方式查询流和批处理数据。</li></ul></li></ol><h2 id="dws-flink-connector工具"><a href="#dws-flink-connector工具" class="headerlink" title="dws-flink-connector工具"></a>dws-flink-connector工具</h2><ol><li>用途：可以通过Flink SQL实现从DWS中读写数据（包括增量读）</li><li>功能：<ul><li>批量读：将DWS中表作为数据源供Flink用于批读</li><li>维流join：将DWS中表作为维表供Fink维流join(即用实时流和维表join)</li><li>攒批写：将DWS中表作为结果表供Flink写入数据（一定时间一定量）</li></ul></li><li>Flink catalog：通过Flink catalog，实现Flink和DWS表相互映射。</li><li>语法说明：<ul><li>Flink SQL中的表字段必须和DWS表中有对应字段</li><li>with参数设置中，connector需要指定dws, tableName需要指定为DWS对应表名。lookupAsync表示是否异步读取</li><li>在Flink catalog中，with参数type需要指定为dws,base_url中不用带数据库名称。use catalog dws 表示使用新建catalog。show catalog表示查询所有catalog。可以直接查询数据库中的表信息，不需要在Flink中建映射表。</li></ul></li></ol><h2 id="实时增量读取"><a href="#实时增量读取" class="headerlink" title="实时增量读取"></a>实时增量读取</h2><ol><li>原理：只对变化的数据进行读取，而不重新读取整个数据集。<ul><li>好处：提高处理效率，减少资源消耗</li></ul></li><li>操作：DWS通过Binlog实现增量读取。对表做DML操作时，先进行双写，对应的DML记录到辅助表中，然后通过读取该辅助表来获取增量数据，实现数据同步和增量计算。</li><li>Binlog表语法：用Binlog表作为源表供Flink实时读取。with参数中的binlog属性需要设置为true,binlogSlotName需要设置为自定义的槽位名。</li><li>注意要点：<ul><li>DWS中只有Hstore和Hstore-opt表支持Binlog功能，表需要有主键且设置为enable_binlog &#x3D; on </li><li>如果多任务消费同一个表的Binlog数据，需要保证每个任务的binlogSlotName唯一。</li><li>为达到最高读取速度，建议Flink任务并行度和DWS集群DN数设置一致。</li><li>可以使用dws-flink-connector的sink能力来写入读取到的Binlog数据，需要注意点如下：如果要保证DN内数据写入顺序，需要设置connectionSize &#x3D; 1.如果源端有更新主键操作或者Flink聚合操作，需要将ignoreUpdateBefore设置为False(默认为True).</li></ul></li></ol><h2 id="实时数仓-1"><a href="#实时数仓-1" class="headerlink" title="实时数仓"></a>实时数仓</h2><p>Flink实时处理能力+DWS的Binlog能力。</p><p>基于公开来源信息, 学习资源来自<a href="https://connect.huaweicloud.com/courses/learn/Learning/sp:cloudEdu_?courseNo=course-v1:HuaweiX+CBUCNXBC006+Self-paced&courseType=1">华为云GaussDB(DWS)数据库官网</a></p>]]></content>
      
      
      <categories>
          
          <category> 数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GaussDB(DWS) </tag>
            
            <tag> 数据库 </tag>
            
            <tag> 数据仓库 </tag>
            
            <tag> 数据 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GaussDB(DWS)数据库-开发应用篇</title>
      <link href="/2025/08/03/GaussDB(DWS)%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%BC%80%E5%8F%91%E5%BA%94%E7%94%A8%E7%AF%87/"/>
      <url>/2025/08/03/GaussDB(DWS)%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%BC%80%E5%8F%91%E5%BA%94%E7%94%A8%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h3 id="DWS驱动及ODBC-JDBC开发"><a href="#DWS驱动及ODBC-JDBC开发" class="headerlink" title="DWS驱动及ODBC&#x2F;JDBC开发"></a>DWS驱动及ODBC&#x2F;JDBC开发</h3><ol><li>DWS驱动：<ul><li>驱动概念：数据库驱动是应用程序和数据库存储之间的一种接口，数据库厂商为某一开发语言环境，能够实现数据库调用而开发的类似“翻译员”功能的程序，将复杂的数据库操作和通讯抽象成为当前开发语言的访问接口。</li></ul></li><li>支持驱动类型：JDBC、ODBC等，如果同时拥有不同版本的集群或者当前没有集群，单击“下载”时下载的依然是与现有集群版本相匹配的。</li><li>驱动支持平台：JDBC没有平台限制，ODBC有支持平台限制（平台：X86、鲲鹏）</li><li>JDBC应用程序开发：<ul><li>JDBC整体架构（4层）：应用程序-&gt;JDBC Driver Interface-&gt;JDBC驱动-&gt;数据库。JAVA本身具有良好的平台移植性，这也直接导致JDBC的平台移植性比ODBC强很多。</li><li>JDBC安全配置：配置JDBC包-&gt;加载驱动-&gt;连接数据库<ul><li>gsjdbc4.jar:与PostgreSQL保持兼容的驱动包，其中类名、类结构与PostgreSQL驱动完全一致，曾经运行于PostgreSQL的应用程序可以直接移植到当前系统使用。主类名为“org.postgresql.Driver”，数据库连接的URL前缀为“jdbc:postgresql”</li><li>gsjdbc200.jar:如果同一JVM进程内需要同时访问PostgreSQL及GaussDB(DWS)，请使用此驱动包。主类名为“com.huawei.gauss200.jdbc.Driver”,数据库连接的URL前缀为“jdbc:guassdb”，其余与gsjdbc4.jar相同。</li></ul></li><li>配置注意事项：<ul><li>连接参数：第三方工具通过JDBC连接DWS时，jdbc向DWS发起连接请求，会默认添加以下配置参数，详见jdbc代码ConnectionFactoryImpl类的实现。这些参数可能会导致JDBC客户端的行为与gsql客户端的行为不一致。如果实际期望和这些配置不符，建议在Java连接设置代码中显示设定这些参数。</li><li>fetchsize:在应用程序中，如果需要使用fetchsize，必须关闭autocommit。开启autocommit，会令fetchsize失效。</li><li>autocommit:在jdbc向DWS申请连接的代码中，建议显示打开autocommit开关。如果基于性能或者其他方面考虑，需要关闭autocommit时，需要应用程序自己来保证事务的提交。例如，在指定的业务SQL执行完之后做显式提交，特别是客户端退出之前务必保证所有的事务已经提交。</li><li>CopyManager:在不使用ETL工具，数据入库实时性要求又比较高的情况下，建议在开发应用程序时，使用dws的jdbc驱动的CopyManager接口进行微批导入。</li><li>释放连接：推荐使用连接池来限制应用程序的连接数。每执行一条SQL就连接一次数据库，是一种不好的SQL编写习惯。在应用程序完成作业任务之后，应当及时断开和dws的连接，释放资源。建议在任务中设置session超时时间参数。</li><li>使用jdbc连接池，在将连接释放给连接池前，需要执行以下操作，重置会话环境。否则，可能会因为历史会话信息导致的对象冲突。如果在连接中设置了GUC参数，那么在将连接归还连接池之前，必须使用“SET SESSION AUTHORIZATION DEFAULT;RESET ALL;”将连接的状态清空。如果使用了临时表，那么在将连接归还连接池之前，必须将临时表删除。（常见报错：relation “xxx_tmp” already exists）</li></ul></li></ul></li><li>ODBC应用程序开发：<ul><li>ODBC整体架构（5层）：应用程序-&gt;标准接口（ODBC API）-&gt; 驱动程序管理器（ODBC Driver Manager）-&gt;ODBC驱动-&gt;数据库</li><li>数据源配置：Linux、windows</li><li>Linux-ODBC数据源配置：<ul><li>步骤（4个）：<ul><li>安装ODBC驱动管理器：获取UnixODBC源码包，编译安装驱动管理器。备注：驱动默认安装在“&#x2F;usr&#x2F;local”目录下，生成数据源文件到“&#x2F;usr&#x2F;local&#x2F;etc”目录下，库文件生成在“&#x2F;usr&#x2F;local&#x2F;lib”目录下</li><li>配置驱动</li><li>安全组配置</li><li>测试连接：使用isql -v GaussODBC(数据源名称)</li></ul></li></ul></li><li>windows-ODBC数据源配置：<ul><li>步骤<ul><li>配置服务器：与Linux相同</li><li>配置数据源（Windows自带驱动管理器，无需额外安装）</li></ul></li></ul></li><li>应用程序调试：<ul><li>ODBC应用程序调试需要在前面配置正确可用的环境下进行。</li></ul></li></ul></li><li>客户端连接管理<ul><li>功能描述：JDBC和ODBC驱动在连接成功后会设置session级GUC参数connection_info.该参数包含连接数据库的驱动类型、驱动版本号、当前驱动的部署路径和进程属主用户，使用json格式记录。默认只显示driver_name和driver_version，driver_path和os_user的显示由用户控制，控制连接参数为ConnectionExtraInfo.connetion_info可以在pg_stat_activity和pgxc_stat_activity中查看。</li><li>连接配置示例：<ul><li>JDBC连接：在连接URL中增加connectionExtraInfo参数。</li><li>ODBC连接：在“&#x2F;usr&#x2F;local&#x2F;etc&#x2F;odbc.ini”文件中追加ConnectionExtraInfo设置。</li></ul></li></ul></li></ol><h3 id="SQL编辑器"><a href="#SQL编辑器" class="headerlink" title="SQL编辑器"></a>SQL编辑器</h3><ol><li>SQL登录：IAM用户一键登录、自定义数据源登录<ul><li>IAM用户一键登录：<ul><li>用户可以直接使用IAM账号登录集群数据库，免除填写账号密码操作</li><li>提供集群列表树展示当前用户拥有的所有集群，可以选择某一个集群直接双击打开集群。</li><li>当前连接是用户的IAM账号直接登录，一键登录集群后，用户可以对账号进行赋权。</li><li>登录后，就可以对集群数据库做一些开发运维等操作。</li><li>限制要求：<ul><li>使用IAM账号登录，首先需要有DWS Database Access角色权限，以及当前用户必须是子账号。</li><li>对于集群版本有一些要求，集群版本要高于8.3.1.330.</li></ul></li></ul></li><li>自定义数据源登录：<ul><li>自定义数据源和传递连接方式类似，用户选择集群后，需要填用户名密码登录</li><li>在创建连接之前会先提示测试连接，测试正常后可以正常保存。</li><li>限制要求：<ul><li>自定义连接对集群版本没有限制，有的只是一些语法上的兼容性，需要用户自定义根据集群版本来编写SQL</li><li>数据源名称如果不填写，会根据集群名（用户名）来创建，注意一个用户下名称不可以重复</li></ul></li></ul></li></ul></li><li>元数据管理：<ul><li>提供了库、模式、表等图形化界面管理，而且以树形方式来层级查看。</li><li>提供树形结构来展现库、模式、表、索引等元数据列表，可以层级打开查看。</li><li>每层节点也提供右键菜单做新增、修改、删除等操作。</li><li>表提供批量操作列，索引，分区，约束信息。</li><li>表，分区，视图支持直接打开数据操作，支持根据SQL条件过滤，表和分区还支持单条数据插入，修改和新增。</li></ul></li><li>SQL分析执行：<ul><li>SQL诊断军规，SQL拦截规则：<ul><li>NULL校验：NULL值的比较只能使用IS [NOT] NULL方式，其他任何形式的逻辑判断都返回NULL。例如NULL &lt;&gt; NULL,NULL &#x3D; NULL和NULL &lt;&gt; 1 返回结果都是NULL.</li><li>COUNT(col)：count(1) 会统计NULL值（真实行数），而count（col）不会统计。</li><li>LIMIT和ORDER BY : DWS的分布式操作会导致数据跨节点流动，不带ORDER BY 的LIMIT 操作的，会导致输出结果随机。因此除非不关注结果集的稳定性，否则禁止不带ORDER BY 的LIMIT操作。</li><li>不稳定函数：子查询中不能出现uuid_generate_v1(), sys_guid(),nextval等不稳定函数，会造成结果集的不稳定。</li><li>NOT IN 校验：not in 子查询逻辑执行计划，走的是nestloop嵌套循环，在数据量大的情况下，非常容易出现性能问题；通过改写not exists之后，执行计划可以使用hashjoin哈希关联，性能能够得到极大的提升。</li><li>join 代替exists: join 相比exists和in 具有更好的代码阅读性，SQL优化器相对更容易找到更精确的执行计划。</li><li>select * : 不建议使用“select * ”这种写法，请明确指定列。</li><li>ORDER BY: 子查询中禁止使用order by </li><li>RETURNING: returning会导致语句不下推</li><li>DISTINCT ON: distinct on 会导致语句不下推</li><li>表关联数量：优化器是基于代价的优化器，表数据量越多，估算的偏差就越大，产生性能差的执行计划的风险就越大，所以每条SQL语句中关联的表个数不超过16个</li><li>schema: 访问表需要加schema。</li></ul></li></ul></li><li>脚本管理<ul><li>目录：支持使用目录来管理SQL脚本，最多支持二级目录，每级目录支持创建10个文件夹，用户只能看到自己创建的目录和脚本。</li><li>脚本：脚本保存到OBS桶中，可以在基础设置中设置默认桶地址，可选择目录进行管理，目录地址也会默认带到桶地址中。</li></ul></li></ol><h3 id="数据集成工具"><a href="#数据集成工具" class="headerlink" title="数据集成工具"></a>数据集成工具</h3><ol><li>实时同步服务：<ul><li>定义： 实时同步服务是DWS 团队根据特性孵化出的，一个简便易用、高性能的、从Kafka同步数据到DWS的服务化工具。其入库时可采用DWS内部协议，以减少对DWS集群的资源消耗，同步提升入库性能。</li></ul></li><li>创建实时同步服务实例：<ul><li>创建时填写对应参数，然后点击立即购买，等待创建完成后，就会按需在后台创建好一个资源池，后续运行的作业将会从资源池中分配一定资源供作业运行；资源池统一使用CU分配资源，规格中的CPU数量即为CU数量，内存不可直接控制，他们会按照CPU内存比按照比例分配，例如创建的实例规格为4U16GB,创建时选择3节点，那么总资源 &#x3D; 4 CU * 3 &#x3D; 12CU,每个CU内存大学4GB.</li></ul></li><li>实时同步服务连接配置：<ul><li>连接配置用于配置Kafka、DWS的连接信息，一份配置所有作业均可使用，以便于后面在提交作业时可以直接使用，不用每个作业都配置一份。</li><li>当前支持Kafka和dws两种类型的连接，连接名称只用于业务区分，无实际含义，Kafka的服务地址需要是带端口的连接串，并且保证是VPC能访问的IP;dws的连接地址是一个包含协议的完整JDBC连接串，同时也需要是能在VPC内能访问的IP.</li></ul></li></ol><h3 id="数据调度工具"><a href="#数据调度工具" class="headerlink" title="数据调度工具"></a>数据调度工具</h3><ol><li>Airflow基础：<ul><li>定义：Apache Airflow是一个开源平台，用于开发、调度和监控，面向批处理的工作流程。</li><li>是可扩展Python框架，主要特点是所有工作流都用Python代码定义，有如下优点：<ul><li>动态：Airflow管道配置为Python代码，允许动态管道生成。</li><li>可扩展：Airflow框架包含可连接多种技术的运算符。所有Airflow组件都可扩展，可轻松适应运行环境。</li><li>灵活：利用Jinja模板引擎，内置工作流参数化。</li></ul></li><li>架构组件（5）：分布式架构<ul><li>Worker: 执行分配的任务</li><li>Scheduler: 负责将必要的任务添加到队列中</li><li>Web Server: HTTP服务器，提供对DAG&#x2F;任务状态等信息的访问</li><li>Database: 存储有关任务、DAG、变量、连接等状态的信息</li><li>Celery: Broker、Result backend</li><li>Broker: 存储要执行的任务。</li><li>Result backend : 存储已完成任务的状态。</li></ul></li><li>Airflow删除：当用户不需要使用某个集群时，可以删除该集群。删除的集群无法恢复，同时集群中的用户数据，已执行任务的历史记录，也会自动删除，而且无法再访问。删除集群时，不会删除集群使用的DAG文件。</li></ul></li><li>注意要点：<ul><li>访问DWS时，建议在Airflow中新建连接，通过base_hook获取连接ID，获取对应数仓的连接信息。避免在DAG文件中，硬编码数仓密码。</li></ul></li></ol><p>基于公开来源信息, 学习资源来自<a href="https://connect.huaweicloud.com/courses/learn/Learning/sp:cloudEdu_?courseNo=course-v1:HuaweiX+CBUCNXBC006+Self-paced&courseType=1">华为云GaussDB(DWS)数据库官网</a></p>]]></content>
      
      
      <categories>
          
          <category> 数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GaussDB(DWS) </tag>
            
            <tag> 数据库 </tag>
            
            <tag> 数据仓库 </tag>
            
            <tag> 数据 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GaussDB(DWS)数据库-数据库管理篇</title>
      <link href="/2025/08/03/GaussDB(DWS)%E6%95%B0%E6%8D%AE%E5%BA%93-%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%AF%87/"/>
      <url>/2025/08/03/GaussDB(DWS)%E6%95%B0%E6%8D%AE%E5%BA%93-%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h3 id="DWS数据仓库服务："><a href="#DWS数据仓库服务：" class="headerlink" title="DWS数据仓库服务："></a>DWS数据仓库服务：</h3><ol><li>特点：在线数据分析处理、即开即用、可扩展、完全托管、分析型数据库</li><li>兼容性：兼容SQL92、SQL99、SQL2003语法、兼容PostgreSQL&#x2F;Oracle&#x2F;Teradata&#x2F;Mysql等数据库生态。</li></ol><h3 id="存储引擎"><a href="#存储引擎" class="headerlink" title="存储引擎"></a>存储引擎</h3><ol><li>用途：组织和管理数据存储</li><li>使用：SQL语句从客户端发到SQL引擎，先进行语法分析，经过SQL引擎的优化器生成执行计划，SQL引擎的执行器和存储引擎交互，存储引擎支持行存和列存（普通列存表和HStore&#x2F;HStore OPT表），并分别提供存储访问接口。</li></ol><h3 id="行存储引擎"><a href="#行存储引擎" class="headerlink" title="行存储引擎"></a>行存储引擎</h3><ol><li>行存表page页面组件：页头（page header）+空闲时间（free space）+数据（heap tuple）</li><li>页头（page header）各字段存储信息如下：<ul><li>tuple:一行数据为一个tuple</li><li>free space:行指针的末尾与最新元祖起始位置之间的空余空间。</li><li>heap tuple:存储实际数据的地方。</li></ul></li></ol><h3 id="列存储引擎"><a href="#列存储引擎" class="headerlink" title="列存储引擎"></a>列存储引擎</h3><ol><li>列存储引擎的最小存储单元是CU(Compression Unit,压缩单元)，一个CU是由表中某一列的一部分数据组成的压缩数据块，可以通过CU_ID,COL_ID标识一个CU.</li><li>列存储引擎架构：辅助表+场景</li><li>辅助表用途：行存表，辅助列存储的实现，用于记录存储的元信息，或者用于提升存储效率。</li><li>辅助表分类：<ul><li>CUDesc表：用于记录CU的事务时间戳、CU大小、存储位置、magic校验码、min&#x2F;max等信息。</li><li>delta表：由表级参数enable_delta来控制是否开启。表主要用于缓存列存表的入库数据，等攒批后再刷到CU中。</li><li>查询：可以通过查询pg_class得到列存储引擎辅助表信息。</li></ul></li><li>适用场景：<ul><li>实时场景（性能）：<ul><li>Hstore表：可用于增强实时场景下的小批量DML性能，兼容列存2.0.可以切换。</li><li>Hstore opt表：与Hstore相比，增强入库和查询能力，但是不兼容老数据。</li></ul></li><li>海量数据场景（数据）：<ul><li>冷热表：自动将冷数据放到OBS服务中存储，从而降低数据存储成本，保障热数据性能。</li><li>存算分离表：将数据完全存放到OBS存储中，并根据性能和存储可以选择相应的规格。</li></ul></li><li>兼容性：Hstore表、Hstore opt表、列存2.0都可以兼容冷热表和存算分离表。</li></ul></li></ol><h3 id="表类型（6种）"><a href="#表类型（6种）" class="headerlink" title="表类型（6种）"></a>表类型（6种）</h3><p>行存表+列存表+Hstore+Hstore opt+冷热表+存算分离表</p><ol><li>行存表：<ul><li>直接使用create table建立的表，默认为行存表，使用B-Tree索引。</li><li>B-Tree索引特点：<ul><li>索引结构：B-Tree（平衡树）是一种有序树，每个节点包含多个键，并且子节点的键值范围是确定的。</li><li>索引优势：高效支持范围查询、等值查询、排序操作。</li></ul></li></ul></li><li>列存表：<ul><li>建表时指定参数orientation &#x3D; column，建立列存表。</li><li>列存参数：<ul><li>compress_level: 指定压缩级别（low, middle, high）</li><li>max_batchrows:CU内最大行数，默认6w行。</li><li>column_version:1.0&#x2F;2.0&#x2F;3.0(存算分离)</li></ul></li><li>列索引：<ul><li>Gin索引：基于B-tree树结构的倒排索引，用于存储被索引字段的value或者value的元素，适应于数组过滤、全文检索的场景。</li><li>Gist索引：通用索引接口，用于不同类型支持不同索引方式，适用于位置索引。</li><li>PSort索引：用于对该列进行聚簇排序，目的是提升查询过滤性能。</li><li>CBTree索引：列存表的B-Tree索引，原理和行存相同。</li></ul></li></ul></li><li>Hstore表：<ul><li>建表：with参数enable_hstore指定为开启，则开启Hstore表，或者通过alter修改为普遍列存表。是实时数仓中设计的表类型，用于将insert&#x2F;update&#x2F;upsert等操作实时快速入库。</li><li>功能：<ul><li>支持异步排序：当指定Psort后，对存量未排序数据在后台排序，风险是在压力大时会造成Delta膨胀。</li><li>支持小CU合并：将小CU在后台合并为一个新CU，提升实时能力。</li><li>单条或者小批量IUD(insert&#x2F;update&#x2F;delete)操作高并发实时入库，支持大批量定期入库。</li><li>支持冷热数据管理。</li></ul></li><li>适配场景：实时入库和实时查询强诉求场景，同时拥有处理传统TP场景事务能力。版本：8.2.0.100及以上集群版本支持</li><li>与普通列存表的差异：主要的辅助表Delta表的差异：<ul><li>Hstoreb表的delta表：表结构上和主表定义不一致，功能上用于持久化存储update&#x2F;delete&#x2F;insert信息。缺点是依赖后台常驻autovacuum来执行merge操作。</li><li>列存表的delta表：表结构上和列存主表定义一致，功能上用于暂存小批量insert数据，达到阈值后统一merge到主表，避免直接insert到主表产生大量CU,缺点是如果来不及merge，会导致delta表膨胀，进而影响查询性能，同时无法解决并发update的锁冲突问题。</li></ul></li><li>注意要点：<ul><li>表级参数enable_dalta和enable_hstore不能同时开启，原因是enable_delta用于普通列存表的delta表开启，与enable_hstore冲突。</li><li>Hstore表只支持col_version 2.0版本。</li></ul></li></ul></li><li>Hstroe opt表：<ul><li>建表:前提是列存表，在with参数enable_hstore_opt指定为开启。或者和enable_hstore同时指定。当hstore_opt开启时，不能通过alter关闭。opt会默认打开turbo属性，除非手动关闭。</li><li>功能：<ul><li>支持异步排序和小CU合并：和Hstore表相同，但是没有delta表膨胀的问题。</li><li>支持Bitmap columns索引：使用bitmap_columns &#x3D; “列名1，列名2”指定。此索引只针对varchar&#x2F;text类型，并且字符长度不能大于127，只正对基数特征比较明显的列。在低基数时，内部会使用bitmap配合字典压缩存储数据，在高基数时，通过bloomfilter生成hash的bit列加速过滤。</li></ul></li><li>Hstore OPT表和Hstore表在辅助表方面的差异：<ul><li>CUDesc辅助表:</li><li>Hstore OPT表结构的cudesc和hstore不一致，支持二级分区等特性。</li><li>Hstore表结构的cudesc和列存一致，因此可以切换。</li><li>Delta辅助表：</li><li>Hstore opt表结构的delta表定义和hstore一致，cudesc和hstore不一致。</li><li>而Hstore的表结构的delta表定义和列存不一致。cudesc和列存一致。</li></ul></li></ul></li><li>冷热表：<ul><li>定义：在时间场景，数据按照时间可划分为：热数据和冷数据。</li><li>热数据Hot:被频繁查询或更新，对访问的响应时间要求高的数据。</li><li>冷数据Cold:不允许更新，偶尔查询，对访问的响应时间要求不高的数据。</li><li>通过定义冷热表，将符合规则的冷数据切换到OBS上存储，按照分区自动进行冷热数据判断和迁移。</li><li>切换策略模式：<ul><li>LMT(Last Modify Time，最后修改时间)：根据分区的最后更新时间进行切换，切换数据为切换[day]天前的分区数据为冷数据，迁移到OBS表空间，其中[day]范围：0到36500天。</li><li>HPN(Hot Partition Number，热分区数量)：保留指定数量的热分区。分区顺序根据分区的序列ID（Sequence id）确定，ID由分区边界值大小生成，切换时，将数据迁移到OBS表空间。HPN范围；0到1600。</li></ul></li><li>建表：<ul><li>创建OBS tablespace，指定OBS路径</li><li>建表，选项中指定orientation &#x3D; column, cold_tablespace &#x3D;’’, storage_policy。cold_tablespace表空间为必选项，storage_policy冷热数据切换规则，必选项。</li></ul></li></ul></li><li>存算分离表：<ul><li>背景：用于实现计算层和存储层独立增加节点并行互不打扰。</li><li>建表：<ul><li>先建立OBS tablespace 表空间，和冷热表一致。</li><li>创建逻辑集群</li><li>创建表，指定col_version &#x3D; 3.0，即可创建存算分离表。</li></ul></li></ul></li><li>不同表的优劣：<ul><li>行存表：适合少量数据，对实时性要求较高的场景。</li><li>列存表1.0&#x2F;2.0: 并发能力弱于Hstore表,不推荐</li><li>Hstore:维批copy，无更新入库场景，性能要求要的情况下使用。</li><li>Hstore opt:对列式存储有实时场景需求，当前推荐版本。</li><li>冷热表：在业务系统中，用户对不同时期数据，有不同使用需求时使用，可以和Hstore OPT同时使用。</li><li>存算分离表：云原生环境下，需要分别增减计算和储存节点的场景下使用。</li></ul></li></ol><h3 id="事务"><a href="#事务" class="headerlink" title="事务"></a>事务</h3><ol><li>定义：是数据库系统执行过程中的一个逻辑单位，由一组有限数据库操作序列构成，包括对数据库的读写操作。</li><li>要点：<ul><li>若事务被提交后，数据库需要确保事务中的所有操作成功完成，并且结构永久保持。</li><li>若事务中有操作没成功完成，则所有操作全部回滚，恢复事务执行前的状态。</li><li>每个事务对其他事务无影响。</li><li>并非任意对数据库操作的序列都是数据库事务，事务要满足ACID特性。</li><li>在进行多个相关更改时，一个事务内执行多个语句有助于保证一致性；在所有相关更改完成前，其他会话不能看见中间状态。</li></ul></li><li>事务分类：显性事务+自动提交事务<ul><li>显性事务：begin开始事务块，所有begin命令后的所有语句都在一个事务中执行，直到给出一个显示的commit或者rollback.</li><li>自动提交事务：默认情况下，数据自动提交模式中执行事务（没有begin块）。每个语句都在自己的事务中执行并且在语句结束时隐式执行一次提交，若失败会完成一次回滚。</li></ul></li><li>事务性质ACID:<ul><li>原子性（Atomicity）:一个事务要么全做，要么全不做。</li><li>一致性（Consistency）:事务执行前后，数据都是正确的，不存在矛盾。</li><li>隔离性（Isolation）:不同事务之间不会相互影响。</li><li>持久性（Durability）:事务提交后，对数据库的改变不会消失。</li></ul></li><li>ACID实现原理：</li><li>原则性（Atomicity）:<ul><li>通过分配的唯一标识事务号（XID）区分不同事务，是单调递增数据，用于事务提交和回滚。事务在修改（insert&#x2F;update&#x2F;delete）前先获取事务号，采用64位bit的XID；通过txid_current（）查询事务号。</li><li>事务提交日志（Commit Log,CLog）:用于记录事务执行结果的状态。事务提交，CLog中记录commit;事务回滚，CLog中记录abort.</li><li>用途：查询事务修改是否有效，只需要查询事务对应的CLog状态。</li></ul></li><li>查询函数:<ul><li>pgxc_is_committed():查询事务状态</li><li>pgxc_xacts_iscommitted():查询各节点事务状态。</li><li>注意要点：CLog日志记录了事务的提交和回滚的状态，不得随意删除或者移动。</li></ul></li><li>一致性Consistency和隔离性Isolation：任何事务都感受不到有其他事务在系统中并发地执行。<ul><li>典型读取问题<ul><li>脏读：一个事务读取另一个未提交事务写入的数据。</li><li>不可重复读：一个事务重新读取之前读取过的数据，发现该数据已经被其他已提交事务修改。</li><li>幻读：一个事务重新执行一个查询，返回符合查询结果的行，发现这些行由于其他最近提交的事务而发生改变（行的增加、减少或者更新）</li></ul></li><li>隔离级别和容易出现的问题：<ul><li>未提交读（read uncommitted）:脏读+不可重复读+幻读</li><li>已提交读（read committed）：不可重复读+幻读</li><li>可重复读（rapeatable read）:幻读</li></ul></li><li>快照：<ul><li>定义：当前时刻所有活跃事务号的集合。</li><li>特点：扫描数据时，每个事务看到的只是获取快照那一刻的数据，而不是数据的当前最新状态，从而避免一个事务看到其他并发事务的更新而导致的不一致数据。</li><li>可见性：针对某条数据对当前查询中是否可见。</li><li>可见情况如下：<ul><li>快照中活跃事务的修改不可见</li><li>事务启动前提交的事务，其修改可见</li><li>事务启动后提交的事务，其修改不可见。</li></ul></li></ul></li><li>注意要点：DWS默认隔离级别为已提交读（read committed).保证隔离级别下的数据一致性。</li></ul></li><li>持久性（Durability）:<ul><li>方法：重做日志（Redo Log）,Redo Log&#x2F;WAL日志&#x2F;xlog，记录数据修改操作，用于数据恢复和持久化。</li><li>产生方法：数据修改过程中会产生redo log，默认单文件大小16M,保存在pg_xlog目录中。主要用途：归档、节点重启恢复、备份恢复、容灾。</li><li>注意要点：xlog记录数据库发生的各种事务信息，不得随意删除或者移动日志文件，否则数据库会有无法恢复的风险。</li><li>FPW(full page writes)会产生大量xlog.带索引导入会产生大量xlog，主要因为带索引导入会使数据使用xlog进行主备复制，而不带索引导入时会使用页复制进行同步。带索引导入推荐做饭：导入前先删除索引，导入完成后重建索引。</li><li>典型函数：<ul><li>pg_current_xlog_location:获取当前重做日志写入位置。</li><li>pg_xlogfile_name:获取当前写入重做日志文件名：</li><li>pg_xlogfile_name_offset:获取当前写入重做日志文件名并返回其在文件中的字节偏移量。</li></ul></li></ul></li><li>分布式事务：<ul><li>特点：分布式事务中，事务由不同服务器不同节点共同完成，所有节点事务要么全部成功，要么全部失败。</li><li>DWS通过GTM(全局事务控制器)实现分布式事务强一致性。</li><li>DWS实施方式：两阶段提交法（2PC）<ul><li>事务协调者要求所有涉及事务的节点预提交操作，并反馈是否可以提交。</li><li>事务协调者要求每个数据库提交数据，或者回滚数据。</li></ul></li><li>2PC流程分类：按照涉及节点不同<ul><li>单节点DML：根据数据分布实际情况，在一个事务中，DML操作只涉及某一个DN节点，只需要该DN以及发起事务的CN参与事务，其他节点不参与本次事务。</li><li>跨节点DML：根据数据分布实际情况，在一个事务中，DML操作涉及多个DN节点。需要多个DN以及发起事务的CN都参与事务，其他CN节点不参与事务。</li><li>DDL:分布式DDL,更新所有实例上的元数据信息。由于元数据信息是存储在所有实例上的，所以更新元数据信息需要在所有CN和DN上都更新一遍，即所有CN和DN都参与事务。</li></ul></li></ul></li></ol><h3 id="表"><a href="#表" class="headerlink" title="表"></a>表</h3><ol><li>表要素：字段类型+存储方式+分布方式+分区方式+约束+表操作</li><li>定义：关系型数据库中二维数组集合，代码存储对象之间的关系</li><li>记录：每一行为一个记录，也称元祖，由若干字段组成。</li><li>字段：域或属性，每一列为一个字段，包含两个属性：列名和数据类型。</li><li>字段数据类型：基本数据类型：数值类型+字符类型+日期时间类型<ul><li>用户自定义类型：create type</li></ul></li><li>存储方式：<ul><li>行存：orientation &#x3D; row</li><li>列存：orientatioin &#x3D; column</li></ul></li><li>分布方式：复制+哈希+轮询<ul><li>复制（replication）：每个节点拥有完整表数据</li><li>哈希（hash）:对表中的列进行哈希，根据哈希值映射到指定数据节点</li><li>轮询（roundrobin）:默认创建方式，修改默认使用参数default_distribution_mode，轮番选择数据节点保存数据。</li></ul></li><li>分区方式：range+list<ul><li>设置：partition by range&#x2F;list(字段)</li><li>其他类型分类：非日志表+临时表</li><li>非日志表（unlogged）:不记录redo 日志，通过日志量的减少提高数据写性能，没有redo日志后，出现故障数据库重启无法恢复，适合于可靠性要求不高的非核心数据。</li><li>临时表：分为会话级和事务级临时表，用来保存会话或者事务中的、临时性的、过程性的数据。表定义和数据仅当前会话可见。</li></ul></li><li>约束：<ul><li>定义方法：列约束&#x2F;表约束</li><li>约束类型：主键约束+唯一约束+非空约束+default约束</li><li>检查约束（仅支持行存表）</li><li>partial cluster key(仅支持列存表)</li></ul></li><li>表操作：建表+修改+删除+查询<ul><li>建表方式：create table as :分区表不能采用此方式创建，可根据查询结果创建，表字段和select查询字段和数据类型相关，指定with no data时，不填充数据。create table like：自动从like指定来源表中，继承所有字段、数据类型和非空约束。使用includeing&#x2F;excluding继承或不继承来源表的某些属性。</li><li>修改方式：alter table</li><li>增加字段：add column column_name data_type</li><li>修改字段类型：modify column_name data_type</li><li>修改分布方式：distribute by repliction&#x2F;roundrobin&#x2F;hash(column_name)</li><li>删除列：drop column column_name</li><li>删除方式：drop table</li><li>可以添加cascade 进行级联删除</li><li>查询命令：<ul><li>元命令：\d</li><li>系统视图：pg_tables&#x2F;xxx_part_tables&#x2F;xxx_tab_partitions</li><li>系统表：pg_class&#x2F;pg_partition</li><li>系统函数：pg_get_tabledef</li></ul></li></ul></li></ol><h3 id="分区设计"><a href="#分区设计" class="headerlink" title="分区设计"></a>分区设计</h3><ol><li>分区策略：<ul><li>表数据量比较大：小表扫描耗时不大，分区表性能收益不明显，只建议对大表采取分区方式。在列存储下，每个列都是单独文件存储，最小存储单元CU可储存6w行数据，对列分区表，建议每个分区数据不小于DN数*6w。</li><li>数据有明显区间特征字段：需要根据有明显区间性字段做表分区，时间字段最常见。</li><li>业务查询有明显区间范围特征：查询数据可落在区间范围指定分区内，才能通过分区剪枝扫描查询需要的分区，提升数据扫描效率，降低数据扫描IO开销。</li></ul></li><li>分区类型：<ul><li>range:基于数值型范围划分数据，数据范围由建表时指定的分区键决定。间隔分区表在发现记录映射不到任何分区时，会根据间隔条件（interval）自动创建分区。 </li><li>list：基于值列表划分数据，仅8.1.3及以上版本支持。</li></ul></li><li>分区优势：改善查询性能+增强可用性+方便维护+均衡I&#x2F;O</li><li>分区要点：<ul><li>针对已存在的表进行分区，最好将数据迁移完后再建索引。</li><li>若数据表已存在，建议先建立分区表，然后使用非堵塞式迁移接口。</li><li>要充分发挥分区表查询优势，必须使用分区时的字段作为过滤字段。分区键条件查询，效率高</li><li>分区后没有全局唯一性，各个分区之间有重复uuid</li><li>分区字段必须是非空的，类似于案件的立案日期和结案日期就不能作为分区字段</li><li>vacuum 或者 analyze 只对主表有作用，要分析分区表，需要分析每个分区。</li><li>分区备份可单独备份各个分区，若要备份所有分区只能备份整个schema。</li><li>数据迁移到分区表后建议禁用主键，因为若主表没有执行vaccuum操作，则执行计划会全表扫描主表，耗时长。</li></ul></li></ol><h3 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h3><ol><li>索引分类：<ul><li>按照数据组织方式：Gin索引+Gist索引+Post索引+Btree索引<ul><li>GIN索引：倒排索引，可处理包含多个键的</li><li>Gist索引：适用于几何和地理等多维数据类型和集合数据</li><li>Psort索引：列存表的局部排序索引。</li><li>Btree索引：适用类似B+树结构来存储数据的键值。</li></ul></li><li>支持情况：<ul><li>行存表：btree(默认)+gin+gist</li><li>列存表：psort(默认)+bree+gin</li></ul></li><li>按照索引方式：唯一+多字段+部分+表达式<ul><li>唯一索引：唯一索引会在每次添加数据时检查表中是否有重复值，行存表Btree索引和列存表btree索引支持唯一索引，主键约束和唯一约束都在自动创建唯一索引</li><li>多字段索引：索引键值多于一个字段的索引，最多声明32个字段</li><li>部分索引：只包含表一部分数据的索引，常用于分布不一致的表，只索引其中频率高的key</li><li>表达式索引：基于表中一个或多个字段的表达式索引。</li></ul></li><li>按照基表类型分类：<ul><li>全局索引：非分区表创建的索引</li><li>索引分区：分区表创建的索引，不支持创建部分索引。</li></ul></li></ul></li><li>重建索引：<ul><li>重建索引条件：<ul><li>索引崩溃，并且不在包含有效数据；</li><li>索引臃肿，包含大量空页或者接近空页</li><li>为索引更改存储参数，并且要求更改生效。</li></ul></li><li>重建方式：reindex+alter index name rebuild</li><li>索引使用：哪些列可以创建索引<ul><li>经常需要搜索查询的列</li><li>经常需要根据范围进行搜索的列</li><li>经常需要排序的列</li><li>经常使用where子句的列</li><li>经常出现在关键字order by&#x2F;group by &#x2F;distinct后面的字段。</li></ul></li><li>索引优缺点：<ul><li>优点如下：点查询提速显著，直接定位到需要的位置，减少无效IO。多条件组合查询，过滤大量数据，缩小扫描范围。利用倒排索引加速全文检索。利用等值条件索引查询速度快的优势，结合nestloop提高多表join效率。提供主键和唯一性索引，满足业务需要。利用btree索引天然有序的特点，优化查询计划。</li><li>缺点如下：索引页面占用额外空间，导致磁盘膨胀。每次导入数据同时更新索引，影响导入性能。索引需要记录xlog,增加日志量。索引页面没有可见性，存在垃圾数据，需要定期清理。每个索引至少一个文件，增加备份恢复、扩容等操作代价。索引索引扫描的性能不一定比顺序扫描性能好，特别是优化器判断错误，导致查询性能劣化的情况下。</li></ul></li></ul></li></ol><h3 id="视图"><a href="#视图" class="headerlink" title="视图"></a>视图</h3><ol><li>虚表：只存放视图定义，不存放视图数据，数据仍然在基本表中，若基表数据发生变化，视图数据也变化。</li><li>视图管理：创建+修改+删除<ul><li>create view：创建视图</li><li>create or replace view: 替换同名视图或创建新视图</li><li>create temp view ：创建会话级临时视图</li><li>alter view：修改视图</li><li>alter view rebuild：在视图解耦下，可使用已保留的原始语句重建视图，恢复依赖关系。</li><li>drop view :删除视图。</li></ul></li><li>可更新视图：<ul><li>背景：在使用视图过程中，为确保权限问题，需要在表上都封装一层视图，对表中数据的IUDS(insert&#x2F;update&#x2F;delete&#x2F;select)通过对应视图操作完成。</li><li>更新机制：视图相当于子查询，子查询中的实际表作为需要更新的表，对该表做merge into&#x2F;insert&#x2F;update&#x2F;delete操作。</li><li>注意要点：可更新视图定义中包含where，则该条件会限制update和delete语句修改基础表上的行。</li><li>使用限制：<ul><li>视图定义的From语句中只能有一个普通表，不能是系统表、外表、DFS表、delta表、toast表、错误表。</li><li>视图中包含可更新的列，这些列是对基础表可更新列的简单引用。</li><li>视图定义中不能包含with、distinct、group by、order by、FOR update 、FOR share 、HAVING、tablesample、limit、offset子句。</li><li>视图定义中不能包含union、intersect、excep集合操作。</li><li>视图定义的选择列表不能包含聚集函数、窗口函数、返回集合的函数。</li><li>视图上不能有触发时机为instead of的触发器。</li><li>视图定义不能包含子链接。</li><li>视图定义不能包含属性为volatile的函数（函数值可以在一次表扫描内改变的函数）</li><li>视图定义不能对表的分布键所在列起别名，或将普通列起别名为分布键列名。</li><li>视图更新操作中包含returning子句时，视图定义中的列只能来自于基础表。</li><li>视图定义包含where条件，则该条件将会限制update和delete语句修改基础表上的行。如果update语句更新行后不再满足where条件，更新后通过视图将无法查询到。</li><li>在视图上执行插入、更新或删除的用户必须在视图和表上具有相应的插入。更新或删除权限。</li></ul></li></ul></li><li>视图解耦：<ul><li>背景：一般情况下，删除视图依赖对象，使用cascade级联将删除视图。重建视图依赖对象后，如果视图量大，使用create view重建视图工作量大。</li><li>解耦机制：删除对象时，不删除依赖对象的视图和其定义，仅删除依赖信息，实现不指定cascade也可删除对象的效果（临时表和临时视图除外）。视图依赖对象重建后，根据依赖对象重建前后差异，可分为自动重建、通过alter view rebuild重建或者drop view + create view重建</li><li>视图解耦设置：<ul><li>参数view_independent &#x3D; on，支持视图解耦和视图重建。</li></ul></li><li>视图重建：<ul><li>自动重建：视图引用的列名在重建后依赖对象中存在，则视图在查询时自动重建。</li><li>alter view rebuild：视图引用的列名在重建后依赖对象中存在，可执行命令重建。</li><li>drop view+create view:视图依赖表中的列被删除或重命名，通过该方式重建。</li></ul></li><li>视图建立原则：<ul><li>业务逻辑：经常使用的数据定义为视图，简化SQL编写，在逻辑上屏蔽真实表结构的变化带来的影响。</li><li>安全逻辑：视图封装只希望业务看到的数据。通过复杂视图，用户不能通过视图修改基表数据。</li></ul></li></ul></li></ol><h3 id="序列（sequence）"><a href="#序列（sequence）" class="headerlink" title="序列（sequence）"></a>序列（sequence）</h3><ol><li>自增整数序列：按照一定规则自增的整数，取值不重复，具有唯一标识性，常被用作主键。<br>  序列管理：</li><li>创建：create sequence</li><li>创建唯一标识自动方法：<ul><li>声明字段类型为序列整型来定义唯一标识符字段：在建表时，将唯一标识符字段类型定义为serial,数据库后台会自动创建一个对应的序列sequence。</li><li>创建自定义sequence并指定字段默认值：使用create sequence自定义序列，使用nextval(‘sequence_name’)函数读取的序列值，指定为某一字段默认值。好处是更灵活，可以为序列定义cache,一次预申请多个序列值，减少和GTM交互次数，来提高性能。</li></ul></li><li>创建注意事项：<ul><li>不建议同时定义cache和maxvalue或者minvalue.因为定义cache后不能保证sequence的连续性，可能产生空洞，造成sequence号段浪费。</li><li>建议cache值不要设置过大，否则会出现缓存序列号时耗时过长的问题；建议cache的值小于10000 0000（1亿）。实际使用时应根据业务设置合理的cache值，既保证快速访问，又不会浪费序列号。</li><li>通过owner by 创建的sequence不建议用于其他表，若希望多表共享sequence，则该sequence不应该从属于特定表。</li><li>为序列sequence指定关联列后u，该列删除时，对应的sequence也会被删除。虽然数据库并不限制序列只能为一列产生默认值，但是最好不要多列共用同一个序列。</li><li>当前版本只支持在定义表的时候指定自增列，或者指定某列的默认值为nextval(“sequence_name”),不支持在已有表中增加自增列或者增加默认值为nextval(“sequence_name”)的列。</li></ul></li><li>修改：alter sequence，alter sequence 语句能够更改现有的sequence的属性，包括修改拥有者、归属列和最大值。将序列和表的指定字段进行关联，在删除字段或者器所在表的的红时候会自动删除已关联的序列。</li><li>修改注意事项：<ul><li>使用alter sequence的用户必须是该序列的所有者</li><li>当前版本仅支持修改拥有者、归属列和最大值。若要修改其他参数，可以删除重建，并用setval函数恢复当前值。</li><li>alter sequence maxvalue 不支持在事务、函数和存储过程中使用。</li><li>修改序列的最大值后，会清空该序列在所有会话中的cache</li><li>alter sequence 会阻塞nextval、currval、lastval、setval的调用。</li></ul></li><li>删除：drop sequence<ul><li>用于从当前数据库里删除序列，只有序列的所有者或者系统管理员才能删除。</li></ul></li><li>序列函数：nextval+currval+lastval+setval<ul><li>nextval():用于递增序列并返回新值，返回类型为bight</li><li>注意事项：<ul><li>为避免从同一序列获取值的并发事务被阻塞，nextval操作不回滚。这种情况将在指定值的顺序中留下未使用的“空洞”。因此,Gauss(DWS)序列对象不能用于获得“无间隙”序列。</li><li>当nextval被下推到DN上时，各DN会自动连接GTM，请求next values值，由于GTM上有最大连接数为8192的限制，这类下推会消耗过多的GTM连接数，因此对于这类语句的并发数目限制为7000&#x2F;集群DN数目。</li></ul></li><li>currval():用于返回当前会话里，最近一次nextval返回的，指定的sequence数值。</li><li>注意事项：<ul><li>如果当前会话没调用过指定sequence的nextval,调用currval报错。</li><li>currval函数默认不支持，如果要使用，需要修改参数enable_beta_featuresw&#x3D; true,并且设置后，nextval函数将不支持下推，返回类型为bight.</li></ul></li><li>lastval():用于放回当前会话里，最近一次nextval返回的值。等效于currval.</li><li>注意事项：<ul><li>如果当前会话没有调度过nextval,调用lastval会报错。</li><li>astval函数默认不支持，如果要使用，需要修改参数enable_beta_features或者lastval_supported &#x3D; true.并且设置后，nextval函数不支持下推，返回类型为bight.</li></ul></li><li>setval():用于设置序列当前值和is_called标识，放回类型为bight</li><li>注意事项：<ul><li>setval使用后，会在当前会话和GTM上立刻生效。但是如果其他会话有缓存的序列值，需要等缓存值耗尽才能感知setval的作用。由于序列的非事务的，setval造成的改变不会由于事务的回滚而撤销。</li></ul></li></ul></li><li>序列注意事项：<ul><li>新sequence序列值产生依靠GTM维护，默认情况下，每申请一个sequence值都要向GTM发送一次申请，GTM在当前值基础上加上步长值，作为新值放回给调度者。</li><li>GTM是全局唯一节点，是性能瓶颈，对大量频繁产生序列号操作，不推荐产生默认序列值。</li><li>序列函数nextval、setval等不支持回滚。setval设置新值，会被当前会话nextval立即生效，但对于其他会话，若定义cache不会立即生效，必须在用尽所有缓存值后，其变动才能被其他会话感知。为避免产生重复值，使用setval设置的新值不能是已经产生的值或者在缓存中的值。</li><li>不要在bulkload的场景中产生默认序列值。如果必须要在bulkload场景下产生默认序列值，则一定要为newSeq1定义足够大的cache，并且不要定义maxvalue或者minvalue。</li><li>sequence创建后，在每个节点都维护一张单行表，存储序列定义和当前值，但此当前值非GTM上当前值，只是保存本节点和GTM交互后的状态。如果其他节点向GTM申请新值，或调用setval修改序列状态，则不刷新本节点的单行表，由于每次申请序列值都是向GTM申请，所以对序列正确性无影响。</li></ul></li><li>序列典型问题：<ul><li>如何确定sequence和哪个表有关联：先在pg_class查找目标sequence的oid, 然后在pg_depend根据oid查依赖该sequence的对象。</li><li>如何查询sequence的当前最新值：通过currval函数可以查询sequence的当前最新值。</li><li>如何解决sequence取值超出范围的问题：可以通过创建sequence时设置cycle字段，从而使得序列达到maxvalue或者minvalue后可循环并继续下去。但是需要注意，如果定义序列为cycle，则不能保证序列的唯一性。或者通过调用setval(regclass, bight)函数对序列取值进行重置。</li></ul></li></ol><h3 id="数据脱敏-对敏感数据进行屏蔽"><a href="#数据脱敏-对敏感数据进行屏蔽" class="headerlink" title="数据脱敏(对敏感数据进行屏蔽)"></a>数据脱敏(对敏感数据进行屏蔽)</h3><ol><li>敏感数据定义：指任何泄露后可能会给社会或个人带来严重危害的数据都属于常见的敏感数据</li><li>敏感数据举例：个人身份信息、企业不适合公开信息、设备信息、银行卡号、受保护的健康信息、知识产权等属于敏感信息。</li><li>数据脱敏原因：对敏感信息通过脱敏规则进行数据变形，实现敏感数据可靠保护。常见脱敏规则为：替换、重排、加密、截断、掩码等。用户可以根据期望的脱敏算法自定义脱敏规则。</li><li>数据脱敏原则：<ul><li>尽可能为脱敏后的应用，保留脱敏前有意义信息</li><li>最大程度防止黑客破解</li></ul></li><li>数据脱敏分类：动态脱敏+静态脱敏<ul><li>动态脱敏：在访问敏感数据时，实时脱敏。优势如下：<ul><li>策略可配置。可结合自身业务场景识别敏感数据，并对业务表的指定列灵活预置脱敏策略</li><li>策略可扩展。内置脱敏函数，可涵盖大部分常见脱敏效果，支持自定义脱敏函数</li><li>敏感数据可算不可见。原始敏感数据参与运算，仅在出库时刻，返回结果时才做脱敏处理</li><li>数据访问受控。脱敏策略生效条件的用户，均对原始敏感数据不可见。</li><li>全场景数据不泄露。底座交互，可减少敏感数据传输链路潜在的泄露风险，安全可靠，而且充分识别各种恶意套取潜在场景，有效防护。</li></ul></li><li>静态脱敏：数据的“搬迁仿真替换”，是将数据抽取并进行脱敏后，下发到下游环节，可随意取用和读写，脱敏后数据和生产数据隔离，从而满足业务需求的同时保障生产数据库安全。</li></ul></li><li>数据脱敏管理：<ul><li>创建：create redaction policy </li><li>修改：alter redaction policy</li><li>删除：drop redaction policy<ul><li>查看脱敏信息：系统视图redaction_policies和redaction_columns.</li></ul></li><li>函数：内置型+扩展型<ul><li>内置型脱敏函数：优先推荐<ul><li>mask_full():全脱敏成固定值，用于实现替代。是可覆盖任何数据类型的全脱敏函数，只关注表达式返回值类型，可保证脱敏数据不泄露，但会导致脱敏结</li><li>mask_partial():针对数值类型&#x2F;字符类型&#x2F;日期或时间类型的部分脱敏，用于实现数值变换&#x2F;截断&#x2F;遮挡</li></ul></li><li>扩展型脱敏函数：可使用pl&#x2F;pgsql语言自定义脱敏函数，遵从要求如下：<ul><li>返回值和脱敏列，类型一致</li><li>函数必须定位为可下推的</li><li>参数列表除脱敏格式外，只能包含一个脱敏列</li><li>函数仅实现针对特定数据类型的格式化改写。</li></ul></li></ul></li></ul></li><li>可算不可见：<ul><li>背景：在使用数据脱敏功能时，存在先对敏感数据加工计算，再输出的情况。</li><li>功能：在数据库内使用原始敏感数据参与加工计算，只在出库时对敏感数据进行脱敏。</li><li>使用条件：需要设置参数enable_redactcol_computable &#x3D; on</li><li>脱敏策略继承：<ul><li>对insert&#x2F;update&#x2F;merge into&#x2F;create table as 语句，当子查询对某个敏感字段投影时，会触发脱敏继承，从而实现包含脱敏信息的新表和源表使用相同的脱敏策略，进而避免敏感数据在新表中数据泄露的问题。</li><li>内置创建的脱敏策略，统一命名为“inherted_rp”</li><li>脱敏策略冲突处理原则：保护用户任何敏感数据不致泄露，优先于数据脱敏效果不具有原始特征，当遇到脱敏效果冲突，都提升为通用脱敏效果mask_full.</li></ul></li></ul></li><li>防护恶意套取：<ul><li>恶意套取定义：通过已知常量值和等值&#x2F;类等值判断表达式来进行套取用户隐私数据的行为。借助等值判断形式表达式的过滤条件或投影操作试探性匹配敏感信息。</li><li>恶意套取定义：通过已知常量值和等值&#x2F;类等值判断表达式来进行套取用户隐私数据的行为。借助等值判断形式表达式的过滤条件或投影操作试探性匹配敏感信息。应对策略：数据脱敏功能采用“悲观主义”模式，任何常量等值判断都有可能存在恶意套取的风险，都应当禁止。</li><li>恶意套取定义：通过已知常量值和等值&#x2F;类等值判断表达式来进行套取用户隐私数据的行为。借助等值判断形式表达式的过滤条件或投影操作试探性匹配敏感信息。禁止使用常量恶意套取场景总结：<ul><li>脱敏字段的常量等值判断表达式、复合表达式、等价表达式</li></ul></li></ul></li></ol><h3 id="审计日志"><a href="#审计日志" class="headerlink" title="审计日志"></a>审计日志</h3><ol><li>功能：用于监视并记录在数据库系统中用户的操作行为，将操作行为的结果记录到审计日志中。</li><li>作用：提高数据库安全级别，识别安全威胁。对用户访问数据库的行为进行记录和分析。可以对事故进行追溯，防止抵赖。支持对数据库操作细粒度的筛选</li><li>管理：支持语句类型和操作类型两种方式设置审计粒度。</li><li>审计日志前提条件：<ul><li>需要审计的审计项开关已开启</li><li>数据库正常运行，并且对数据库执行增删改查操作，保证在查询时段内有审计结果产生。</li><li>数据库各个节点审计日志单独记录，如果使用LVS负载管理机制，需要根据LVS日志追溯到具体的执行节点，并且直接连接该节点查询相关审计日志。</li><li>只有拥有auditadmin属性的用户才可以查看审计记录。</li></ul></li><li>审计日志使用：<ul><li>审计查询命令：使用数据库提供的SQL函数 pg_query_audit</li><li>查询所有CN节点审计日志：pgxc_query_audit()</li><li>查询单个CN节点，也可查询所有CN节点审计日志：pg_query_audit_details()</li></ul></li></ol><p>基于公开来源信息, 学习资源来自<a href="https://connect.huaweicloud.com/courses/learn/Learning/sp:cloudEdu_?courseNo=course-v1:HuaweiX+CBUCNXBC006+Self-paced&courseType=1">华为云GaussDB(DWS)数据库官网</a></p>]]></content>
      
      
      <categories>
          
          <category> 数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GaussDB(DWS) </tag>
            
            <tag> 数据库 </tag>
            
            <tag> 数据仓库 </tag>
            
            <tag> 数据 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GaussDB(DWS)数据库-基础篇</title>
      <link href="/2025/08/03/GaussDB(DWS)%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%9F%BA%E7%A1%80%E7%AF%87/"/>
      <url>/2025/08/03/GaussDB(DWS)%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%9F%BA%E7%A1%80%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h2 id="软件架构"><a href="#软件架构" class="headerlink" title="软件架构"></a>软件架构</h2><p>软件架构：分为存算分离型和存算一体型，二者的区别在于DN和存储（Local-Disk）是否绑定在一起，绑定在一起为存算一体，否则为存算分离。</p><h2 id="数据组件-7种类型"><a href="#数据组件-7种类型" class="headerlink" title="数据组件(7种类型)"></a>数据组件(7种类型)</h2><ol><li>OM(运维管理模块、主备)：提供日常运维和配置管理功能，每个节点都部署</li><li>CM(集群管理模块、主备)：自动化集群管理和监控各单元的物理资源使用情况，每个节点都部署。</li><li>GTM(全局事务控制器，主备)：主要负责生成并维护全局事务ID、事务快照、时间戳等需要全局唯一的信息。DWS集群部署2个，一主一备，分布在不同节点上。</li><li>WLM(工作负载管理)：控制资源分配，防止过载对系统冲击导致的业务拥塞和系统崩溃。内置在CN和DN实例中。</li><li>CN(Coordinator Node,协调节点，多主)：主要处理请求分解、调度、结果返回。负责SQL解析和优化。决定对外业务访问能力，默认部署2个，最大支持5个。</li><li>DN(Data Node,数据节点，主备从)：负责存储业务数据（支持行存、列存、混合存储）、执行数据查询任务以及向CN返回执行结果。决定对外业务处理能力。根据节点规格，每个节点部署2-4个，最大支持1024个。</li><li>GDS Loader(多实例)：负责批量数据加载和并行加速。</li></ol><p>物理集群由3到多个节点组成，最大支持1024个，节点为ECS&#x2F;BMS.<br>数据组件分布：以最简的ECS+EVS结构为例。ECS负责计算资源（CPU+内存+DWS实例（CN+DN等））。EVS负责存储资源，每个DN挂载EVS盘。</p><h2 id="DWS资源分配"><a href="#DWS资源分配" class="headerlink" title="DWS资源分配"></a>DWS资源分配</h2><ol><li>CPU资源：20%+60%+20%：20%CPU资源用于系统运维，20%CPU资源用于数据接入入库业务，60%CPU资源用于数据分析业务,入库业务和分析业务资源隔离，互不影响。</li><li>内存资源：默认GaussDB(DWS)使用内存占主机Linux系统可用内存的80%。GaussDB 200 默认关闭操作系统的虚拟内存。</li><li>内存参数：<ul><li>max_process_memory ：一个数据库节点（DN&#x2F;CN）可用的最大物理内存</li><li>视图pv_total_memory_detail：查看一个数据库节点总的内存分配情况。</li></ul></li></ol><h2 id="业务架构"><a href="#业务架构" class="headerlink" title="业务架构"></a>业务架构</h2><p>业务下发的SQL信息是如何在GaussDB(DWS)中的各个组件运行的。CN-&gt;DN-&gt;CN</p><ol><li>业务的查询信息先下发SQL到CN节点，其中的SQL信息可以是对数据的增删改查。</li><li>CN通过优化器生成执行计划，DN按照执行计划处理数据</li><li>在分布式存储中，数据处理的DN和数据存储DN不同，数据处理过程中需要从其他DN获取数据，通过stream流（广播流、聚合流和重分布流）降低数据在DN节点间的流动。</li><li>DN返回数据处理结果给CN，CN汇总结果。</li><li>CN将汇总结果返回给业务。</li></ol><h2 id="分布式架构优点"><a href="#分布式架构优点" class="headerlink" title="分布式架构优点"></a>分布式架构优点</h2><ol><li>支持按需扩展，支持2048节点的超大规模，100PB级的数据容量.</li><li>通过多层级的并行计算引擎，基于鲲鹏CPU的优化，软硬协同，性能相比于X86提升30%。</li><li>通过支持事务ACID,实现全场景数据的一致性数据保障，支持双集群容灾，全组件HA设计，来实现高可用。</li><li>兼容标准SQL 2003、JDBC和ODBC接口，全图形化的运维管理和开发工具，来实现高兼容性。</li></ol><h2 id="高性能"><a href="#高性能" class="headerlink" title="高性能"></a>高性能</h2><p>高斯数据库实现高性能的技术：全并行架构+分布式优化+行列混合引擎</p><ol><li>全并行架构：目的是解决如何利用x86的多核计算资源，如何解决鲲鹏众核的资源利用问题。解决方法：从大到小<ul><li>MPP(节点并行)：在集群内并行，通过使用分布式执行框架，支持1000以上的服务器，万级CPU的并行计算 </li><li>SMP(算子并行)：在查询内并行，通过多线程并行算法，从而实现在核心算子内的并行执行。众核支持，和NUMA架构优化。</li><li>SIMD(指令级并行)：操作数归并，通过SIMD和向量化引擎    ，实现一个指令执行一批数据的操作，指令可以是x86或者鲲鹏指令.</li><li>LLVM(动态编译)：指令数减少，通过将热点函数预编译成机器码，实现减少SQL执行指令数，从而提升性能。</li></ul></li><li>分布式优化：用于分布式架构下最优执行计划的生成。<ul><li>分布式查询重写:<ul><li>解决问题：单机SQL逻辑无法实现分布式执行的问题</li><li>解决方法：利用分布式查询重写技术，在分布式下消除NestLoop和子查询等查询瓶颈。</li></ul></li><li>分布式计划生成：<ul><li>解决问题：单机统计信息不能全面反应分布式数据特征</li><li>解决方案：基于Poisson估算模型，单点和全局cost估算模型，local和Global结合数据处理估算模型，实现单机+全局的自动统计信息收集。</li></ul></li><li>分布式倾斜处理：<ul><li>解决问题：数据倾斜导致的分布式执行出现木桶效应.</li><li>解决方案：针对静态模型，使用分布式倾斜估算模型；针对动态模型，使用动态倾斜处理方案RLBT（Runtime Load Balance Technology）</li></ul></li></ul></li><li>行列混合引擎：分为查询引擎和存储引擎。查询引擎分为行存+列存<ul><li>行存：适合高并发+短事务场景，例如点查询、数据更新、实时数据接入、并发增删改。</li><li>列存：适合分析AP场景，例如分析统计分组聚合、统计分析、批量加载、访问大量行少数列。</li><li>Delta列存：适合实时分析场景，例如实时分析同时进行实时插入和更新、实时插入更新进入行存Delta、实时分析基于列存+行存Delta<br>注意：Delta表是列存表附带的行存表，若创建列存表同时开启Delta表,插入列存表的数据也会以行存的形式保存。</li></ul></li></ol><h2 id="高扩展"><a href="#高扩展" class="headerlink" title="高扩展"></a>高扩展</h2><p>逻辑集群+异构扩展</p><ol><li>逻辑集群实现如下功能：通过使用CN+DN<ul><li>逻辑统一+业务隔离：用逻辑集群实现DN层的计算存储资源隔离，从而实现业务隔离。</li><li>数据共享：将不同逻辑集群的公共数据放到同一个逻辑集群中</li><li>计算弹性：可以利用空闲集群的计算资源用于其他业务的作业。</li></ul></li><li>异构扩展：用于冷数据的低成本管理<ul><li>功能：用本地盘做性能加速，用OBS做冷数据区，实现数据存储异构，自动冷热数据迁移（2年以上冷数据）。<br>结果：分层存储+成本最优：按需选择存储和计算引擎，实现冷热数据的动态切换。</li></ul></li></ol><h2 id="高可用"><a href="#高可用" class="headerlink" title="高可用"></a>高可用</h2><p>主备从HA技术+多租户资源管理+可信安全+多层次多类型备份</p><h3 id="主备从HA技术"><a href="#主备从HA技术" class="headerlink" title="主备从HA技术"></a>主备从HA技术</h3><p>功能：数据三重保护（主+备+从备）+容忍单副本故障+两副本提供HA保障。<br>主备流程：<br>正常情况：主机和备机通过日志+数据页流复制实现强同步，主机和从备仅保持连接，从备不占用额外存储资源<br>主机故障：集群管理器感知并仲裁备机升为主机，升级后的备机和从备进行主从强同步。<br>备机故障：主机自动感知，主机的未同步日志和数据发送给从备，实现主从强同步，主备同步利用内核底层实现主从同步切换，事务层不感知</p><h3 id="多租户资源管理"><a href="#多租户资源管理" class="headerlink" title="多租户资源管理"></a>多租户资源管理</h3><p>和企业组织结构匹配+管理租户资源（计算+存储）+租户资源隔离（容器技术）+租户资源监控</p><h3 id="多层级多类型备份"><a href="#多层级多类型备份" class="headerlink" title="多层级多类型备份"></a>多层级多类型备份</h3><p>多种介质：云备份推荐使用OBS备份，支持OBS&#x2F;NBU&#x2F;华为数据一体机<br>全量+增量备份：全量物理备份、差异增量、累积增量等类型备份<br>完全在线：备份期间不加锁，业务SQL无影响<br>全局一致性：备份全局一致性快照<br>细颗粒备份恢复：支持集群+schema级+表级备份恢复，支持就地集群恢复。<br>安全：加密传输。</p><h2 id="融合分析"><a href="#融合分析" class="headerlink" title="融合分析"></a>融合分析</h2><p>支持直接读写HDFS&#x2F;OBS&#x2F;PostGIS数据：<br>数据源互通：可以读取Oracle&#x2F;Spark&#x2F;Hive数据库<br>外表机制：支持HDFS&#x2F;OBS&#x2F;MPP外表，读取HDFS&#x2F;OBS数据。<br>兼容性：兼容SQL2003、JDBC&#x2F;ODBC、SQL2003访问HDFS和OBS</p><h2 id="智能运维"><a href="#智能运维" class="headerlink" title="智能运维"></a>智能运维</h2><p>适应场景：支持扩容加节点+扩容重分布+空间回收Vacuum full<br>快照策略：手动快照+自动快照<br>存储介质选择：OBS + NBU</p><p>学习资源来自<a href="https://connect.huaweicloud.com/courses/learn/Learning/sp:cloudEdu_?courseNo=course-v1:HuaweiX+CBUCNXBC006+Self-paced&courseType=1">华为云GaussDB(DWS)数据库官网</a></p>]]></content>
      
      
      <categories>
          
          <category> 数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GaussDB(DWS) </tag>
            
            <tag> 数据库 </tag>
            
            <tag> 数据仓库 </tag>
            
            <tag> 数据 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2025/04/25/hello-world/"/>
      <url>/2025/04/25/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      <categories>
          
          <category> 测试 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Typora </tag>
            
            <tag> Markdown </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
